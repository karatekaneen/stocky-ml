{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "### Operations\n",
    "- Figure out the least amount of data that needs to be provided\n",
    "- Save + load model and compare results\n",
    "- Build API around it\n",
    "\n",
    "### Features\n",
    "- ~~Add feature for days since last signal~~\n",
    "- ~~Add feature for number of signals last year~~\n",
    "- ~~Kaufmanns efficiency ratio~~\n",
    "- Insider buys/sells\n",
    "- VIX\n",
    "- Interest rates\n",
    "- Squeeze DIX/GEX\n",
    "- Add feature based on output on news model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:\\dev\\stocky-ml\\credentials.json\"\n",
    "\n",
    "# Data:\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dateutil.parser\n",
    "import dateutil.tz as tz\n",
    "from datetime import datetime, timedelta\n",
    "import talib   \n",
    "from talib import MA_Type\n",
    "\n",
    "\n",
    "# Visualization:\n",
    "import seaborn as sns\n",
    "\n",
    "# Database:\n",
    "from google.cloud import firestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create db instance: \n",
    "db = firestore.Client()\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price_data(id):\n",
    "    doc = db.collection('prices').document(id).get().to_dict()\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(doc['priceData'])\n",
    "    df = add_calculated_columns(df)\n",
    "    df = convert_dates(df)\n",
    "\n",
    "    # Read the file to lazily make sure that the dates are strings etc.\n",
    "    # FIXME: Should probably be done some other way.\n",
    "    # output.drop(columns=['Unnamed: 0'], inplace = True)\n",
    "    return df\n",
    "\n",
    "def get_trade_data(doc):\n",
    "    doc = doc.to_dict()\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(doc['trades'])\n",
    "    df['result'] = (df['exitPrice'] / df['entryPrice']) -1\n",
    "    df['trades_last_year'] =  count_trades_last_year(df, df['entryDate'])\n",
    "    shifted = df['exitDate'].shift()\n",
    "    df['days_since_last_signal'] = (df['entryDate'] - shifted).dt.days\n",
    "    # df['days_since_last_signal'] =df['days_since_last_signal'].days\n",
    "\n",
    "    df = convert_trade_dates(df)\n",
    "\n",
    "    return df[['date', 'result', 'trades_last_year',  'days_since_last_signal']]\n",
    "\n",
    "def convert_date(date, fmt = \"%Y-%m-%d\", target_tz = tz.gettz('CET')):\n",
    "    return date.replace(tzinfo=tz.gettz(\"UTC\")).astimezone(target_tz).strftime(fmt)\n",
    "\n",
    "def count_trades_last_year(df, d):\n",
    "    # print(d)\n",
    "    out = []\n",
    "\n",
    "    for x in d:\n",
    "        trades_last_year = df[(df['entryDate'] < x - timedelta(days=2)) & (df['entryDate'] >= x - timedelta(days=365))]\n",
    "        out.append(len(trades_last_year))\n",
    "    return out\n",
    "\n",
    "    \n",
    "def convert_dates(df):\n",
    "    cet = tz.gettz('CET')\n",
    "    for i, row in df.iterrows():\n",
    "        d = convert_date(dateutil.parser.isoparse(row['date']))\n",
    "        df.at[i,'date'] = d\n",
    "    return df\n",
    "\n",
    "def kaufmanns_efficiency_ratio(prices, n):\n",
    "    \"\"\"\n",
    "    Calculates Kaufmann's Efficiency Ratio over a lookback of n.\n",
    "    :param prices: list of prices\n",
    "    :param n: lookback period\n",
    "    :return: Kaufmann's Efficiency Ratio\n",
    "    \"\"\"\n",
    "    change = abs(prices[-n] - prices[0])\n",
    "    volatility = sum(abs(prices[i] - prices[i-1]) for i in range(1, n))\n",
    "    return change / volatility if volatility != 0 else 0\n",
    "\n",
    "\n",
    "def convert_trade_dates(df):\n",
    "    cet = tz.gettz('CET')\n",
    "    for i, row in df.iterrows():\n",
    "        d = convert_date(row['entryDate'] - timedelta(days=1)) # Want one day earlier so that we don't have look ahead\n",
    "        df.at[i,'date'] = d\n",
    "    return df\n",
    "\n",
    "def add_calculated_columns(price):\n",
    "    lookbacks = [20, 50, 100, 200]\n",
    "    values = ['close', 'volume']\n",
    "        \n",
    "    for value in values:\n",
    "        for lookback in lookbacks:\n",
    "            # Get the rolling average and std:\n",
    "            price['average'] = price[value].rolling(lookback).mean()\n",
    "            price['std'] = price[value].rolling(lookback).std()\n",
    "            high = price['high'].rolling(lookback).max()\n",
    "            low = price['low'].rolling(lookback).min()\n",
    "            \n",
    "\n",
    "            # Normalize distance to mean. This could be done with the data above but dont know how.\n",
    "            price[f'zs-{lookback}-{value}'] = (price[value] - price['average']) / price['std']\n",
    "\n",
    "            # Get slope of rolling average and std\n",
    "            price[f'ma-slope-{lookback}-{value}'] = price['average'] / price['average'].shift(1)\n",
    "            price[f'std-slope-{lookback}-{value}'] = price['std'] / price['std'].shift(1)\n",
    "\n",
    "            # Get range\n",
    "            price[f'rng-{lookback}'] = high / low\n",
    "            price[f'percent-rng-{lookback}-{value}'] = (high / low) / price[value]\n",
    "            price[f'percent-std-{lookback}-{value}'] = price['std'] / price[value]\n",
    "\n",
    "            if value == 'volume':\n",
    "                price['temp_volume'] = round(price['volume'] * (price['close'] * 2 + price['open'] * 2 + price['low'] + price['high'])/6)\n",
    "                price[f'avg-log-volume-{lookback}'] = np.log10(price['temp_volume'])  \n",
    "                price.drop(columns=['temp_volume'], inplace = True)\n",
    "            else:\n",
    "                # Hehe, so bad code\n",
    "                # apply the kaufmanns_efficiency_ratio function to a rolling window of the close column\n",
    "                price[f'kaufmanns_efficiency_ratio-{lookback}'] = price['close'].rolling(window=lookback).apply(lambda x: kaufmanns_efficiency_ratio(x.tolist(), lookback))\n",
    "\n",
    "\n",
    "            # Drop the actual values since they carry no interest:\n",
    "            price.drop(columns=['average', 'std'], inplace = True)\n",
    "            \n",
    "        # TODO: Add calculations for volume\n",
    "        # TODO: Add calculations for owners\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_index_stock_df(omxdf, stockdf):\n",
    "  df = pd.merge(stockdf[['date', 'close']], omxdf[['date', 'close']], on='date', suffixes=('_stock', '_omx'))\n",
    "  # df['date'] = df['date_stock']\n",
    "  df['stock_quota'] = df['close_stock']/df['close_omx']\n",
    "\n",
    "\n",
    "  df['stock_hist_relative_perf20'] = df['stock_quota'].shift(20) / df['stock_quota']\n",
    "  df['stock_hist_relative_perf50'] = df['stock_quota'].shift(50) / df['stock_quota']\n",
    "  df['stock_hist_relative_perf100'] = df['stock_quota'].shift(100) / df['stock_quota']\n",
    "\n",
    "  df['stock_hist_perf20'] = df['close_stock'].shift(20) / df['close_stock']\n",
    "  df['stock_hist_perf50'] = df['close_stock'].shift(50) / df['close_stock']\n",
    "  df['stock_hist_perf100'] = df['close_stock'].shift(100) / df['close_stock']\n",
    "\n",
    "  for p in [3, 10, 34, 100]:\n",
    "    df[f'stock_relative_rsi_{p}'] = talib.RSI(df['stock_quota'], timeperiod=p) /100\n",
    "    df[f'stock_rsi_{p}'] = talib.RSI(df['close_stock'], timeperiod=p) / 100\n",
    "\n",
    "  df.drop(columns=['close_stock', 'close_omx'], inplace = True)\n",
    "\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the omx price data\n",
    "omxdf = get_price_data('19002')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have 20390\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>volume_stock</th>\n",
       "      <th>close_stock</th>\n",
       "      <th>zs-20-close_stock</th>\n",
       "      <th>ma-slope-20-close_stock</th>\n",
       "      <th>std-slope-20-close_stock</th>\n",
       "      <th>rng-20_stock</th>\n",
       "      <th>percent-rng-20-close_stock</th>\n",
       "      <th>percent-std-20-close_stock</th>\n",
       "      <th>kaufmanns_efficiency_ratio-20_stock</th>\n",
       "      <th>zs-50-close_stock</th>\n",
       "      <th>ma-slope-50-close_stock</th>\n",
       "      <th>std-slope-50-close_stock</th>\n",
       "      <th>rng-50_stock</th>\n",
       "      <th>percent-rng-50-close_stock</th>\n",
       "      <th>percent-std-50-close_stock</th>\n",
       "      <th>kaufmanns_efficiency_ratio-50_stock</th>\n",
       "      <th>zs-100-close_stock</th>\n",
       "      <th>ma-slope-100-close_stock</th>\n",
       "      <th>std-slope-100-close_stock</th>\n",
       "      <th>rng-100_stock</th>\n",
       "      <th>percent-rng-100-close_stock</th>\n",
       "      <th>percent-std-100-close_stock</th>\n",
       "      <th>kaufmanns_efficiency_ratio-100_stock</th>\n",
       "      <th>zs-200-close_stock</th>\n",
       "      <th>...</th>\n",
       "      <th>zs-200-close_omx</th>\n",
       "      <th>ma-slope-200-close_omx</th>\n",
       "      <th>std-slope-200-close_omx</th>\n",
       "      <th>rng-200_omx</th>\n",
       "      <th>percent-rng-200-close_omx</th>\n",
       "      <th>percent-std-200-close_omx</th>\n",
       "      <th>kaufmanns_efficiency_ratio-200_omx</th>\n",
       "      <th>stock_quota</th>\n",
       "      <th>stock_hist_relative_perf20</th>\n",
       "      <th>stock_hist_relative_perf50</th>\n",
       "      <th>stock_hist_relative_perf100</th>\n",
       "      <th>stock_hist_perf20</th>\n",
       "      <th>stock_hist_perf50</th>\n",
       "      <th>stock_hist_perf100</th>\n",
       "      <th>stock_relative_rsi_3</th>\n",
       "      <th>stock_rsi_3</th>\n",
       "      <th>stock_relative_rsi_10</th>\n",
       "      <th>stock_rsi_10</th>\n",
       "      <th>stock_relative_rsi_34</th>\n",
       "      <th>stock_rsi_34</th>\n",
       "      <th>stock_relative_rsi_100</th>\n",
       "      <th>stock_rsi_100</th>\n",
       "      <th>result</th>\n",
       "      <th>trades_last_year</th>\n",
       "      <th>days_since_last_signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>937763.0</td>\n",
       "      <td>104.80</td>\n",
       "      <td>2.406278</td>\n",
       "      <td>1.008411</td>\n",
       "      <td>1.206170</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>0.011768</td>\n",
       "      <td>0.049977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.550730</td>\n",
       "      <td>1.000816</td>\n",
       "      <td>1.008874</td>\n",
       "      <td>1.205161</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.034478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059427</td>\n",
       "      <td>0.910515</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.853244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.972445</td>\n",
       "      <td>0.982223</td>\n",
       "      <td>0.811284</td>\n",
       "      <td>0.867440</td>\n",
       "      <td>0.574894</td>\n",
       "      <td>0.654295</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.334595</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2167584.0</td>\n",
       "      <td>169.90</td>\n",
       "      <td>2.570386</td>\n",
       "      <td>1.012324</td>\n",
       "      <td>1.023911</td>\n",
       "      <td>1.277528</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.041544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.138433</td>\n",
       "      <td>1.010411</td>\n",
       "      <td>1.007476</td>\n",
       "      <td>1.849783</td>\n",
       "      <td>0.010887</td>\n",
       "      <td>0.097403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.087838</td>\n",
       "      <td>1.003921</td>\n",
       "      <td>1.019671</td>\n",
       "      <td>2.210915</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.105117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316113</td>\n",
       "      <td>1.000618</td>\n",
       "      <td>0.996687</td>\n",
       "      <td>1.510986</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.079502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099913</td>\n",
       "      <td>0.874627</td>\n",
       "      <td>0.742786</td>\n",
       "      <td>0.662212</td>\n",
       "      <td>0.782519</td>\n",
       "      <td>0.592113</td>\n",
       "      <td>0.695115</td>\n",
       "      <td>0.844962</td>\n",
       "      <td>0.938508</td>\n",
       "      <td>0.630043</td>\n",
       "      <td>0.713423</td>\n",
       "      <td>0.582931</td>\n",
       "      <td>0.604268</td>\n",
       "      <td>0.576904</td>\n",
       "      <td>0.579589</td>\n",
       "      <td>-0.134604</td>\n",
       "      <td>1</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>908268.0</td>\n",
       "      <td>184.45</td>\n",
       "      <td>2.339982</td>\n",
       "      <td>1.000686</td>\n",
       "      <td>1.031583</td>\n",
       "      <td>1.262564</td>\n",
       "      <td>0.006845</td>\n",
       "      <td>0.047219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.121880</td>\n",
       "      <td>1.007725</td>\n",
       "      <td>1.012728</td>\n",
       "      <td>1.526664</td>\n",
       "      <td>0.008277</td>\n",
       "      <td>0.079306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.999881</td>\n",
       "      <td>1.003812</td>\n",
       "      <td>1.019996</td>\n",
       "      <td>2.393700</td>\n",
       "      <td>0.012977</td>\n",
       "      <td>0.117752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475301</td>\n",
       "      <td>1.000196</td>\n",
       "      <td>1.000570</td>\n",
       "      <td>1.510986</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.076064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106564</td>\n",
       "      <td>0.988013</td>\n",
       "      <td>0.771849</td>\n",
       "      <td>0.661123</td>\n",
       "      <td>0.987802</td>\n",
       "      <td>0.681214</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.908826</td>\n",
       "      <td>0.958653</td>\n",
       "      <td>0.704871</td>\n",
       "      <td>0.750717</td>\n",
       "      <td>0.596260</td>\n",
       "      <td>0.613973</td>\n",
       "      <td>0.579304</td>\n",
       "      <td>0.582695</td>\n",
       "      <td>-0.067935</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1479449.0</td>\n",
       "      <td>188.90</td>\n",
       "      <td>1.817743</td>\n",
       "      <td>1.009085</td>\n",
       "      <td>1.050660</td>\n",
       "      <td>1.272425</td>\n",
       "      <td>0.006736</td>\n",
       "      <td>0.050375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.618434</td>\n",
       "      <td>0.997120</td>\n",
       "      <td>0.968908</td>\n",
       "      <td>1.505648</td>\n",
       "      <td>0.007971</td>\n",
       "      <td>0.105323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.539262</td>\n",
       "      <td>1.002220</td>\n",
       "      <td>0.992524</td>\n",
       "      <td>1.586279</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>0.110237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.146368</td>\n",
       "      <td>...</td>\n",
       "      <td>1.045681</td>\n",
       "      <td>1.000126</td>\n",
       "      <td>1.001366</td>\n",
       "      <td>1.510986</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.076733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102616</td>\n",
       "      <td>0.837600</td>\n",
       "      <td>1.196999</td>\n",
       "      <td>0.907093</td>\n",
       "      <td>0.836421</td>\n",
       "      <td>1.134992</td>\n",
       "      <td>0.791689</td>\n",
       "      <td>0.833995</td>\n",
       "      <td>0.862504</td>\n",
       "      <td>0.681871</td>\n",
       "      <td>0.702627</td>\n",
       "      <td>0.535038</td>\n",
       "      <td>0.554836</td>\n",
       "      <td>0.532713</td>\n",
       "      <td>0.545055</td>\n",
       "      <td>0.282463</td>\n",
       "      <td>3</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>659684.0</td>\n",
       "      <td>283.70</td>\n",
       "      <td>2.180677</td>\n",
       "      <td>1.006883</td>\n",
       "      <td>1.156869</td>\n",
       "      <td>1.243475</td>\n",
       "      <td>0.004383</td>\n",
       "      <td>0.044750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.856338</td>\n",
       "      <td>1.005233</td>\n",
       "      <td>0.983717</td>\n",
       "      <td>1.338377</td>\n",
       "      <td>0.004718</td>\n",
       "      <td>0.055730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.829468</td>\n",
       "      <td>1.004957</td>\n",
       "      <td>1.003570</td>\n",
       "      <td>1.709517</td>\n",
       "      <td>0.006026</td>\n",
       "      <td>0.112228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.328634</td>\n",
       "      <td>...</td>\n",
       "      <td>2.419453</td>\n",
       "      <td>1.001572</td>\n",
       "      <td>1.005738</td>\n",
       "      <td>1.356484</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.061583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128638</td>\n",
       "      <td>0.935286</td>\n",
       "      <td>0.863594</td>\n",
       "      <td>0.756640</td>\n",
       "      <td>0.876630</td>\n",
       "      <td>0.766655</td>\n",
       "      <td>0.608037</td>\n",
       "      <td>0.824917</td>\n",
       "      <td>0.921578</td>\n",
       "      <td>0.683500</td>\n",
       "      <td>0.772665</td>\n",
       "      <td>0.577465</td>\n",
       "      <td>0.641126</td>\n",
       "      <td>0.554928</td>\n",
       "      <td>0.594481</td>\n",
       "      <td>0.297843</td>\n",
       "      <td>3</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  volume_stock  close_stock  zs-20-close_stock  ma-slope-20-close_stock  std-slope-20-close_stock  rng-20_stock  percent-rng-20-close_stock  percent-std-20-close_stock  kaufmanns_efficiency_ratio-20_stock  zs-50-close_stock  ma-slope-50-close_stock  std-slope-50-close_stock  rng-50_stock  percent-rng-50-close_stock  percent-std-50-close_stock  kaufmanns_efficiency_ratio-50_stock  zs-100-close_stock  ma-slope-100-close_stock  std-slope-100-close_stock  rng-100_stock  percent-rng-100-close_stock  percent-std-100-close_stock  kaufmanns_efficiency_ratio-100_stock  zs-200-close_stock  ...  zs-200-close_omx  ma-slope-200-close_omx  std-slope-200-close_omx  rng-200_omx  percent-rng-200-close_omx  percent-std-200-close_omx  kaufmanns_efficiency_ratio-200_omx  stock_quota  stock_hist_relative_perf20  stock_hist_relative_perf50  stock_hist_relative_perf100  stock_hist_perf20  stock_hist_perf50  stock_hist_perf100  stock_relative_rsi_3  stock_rsi_3  stock_relative_rsi_10  \\\n",
       "0           0      937763.0       104.80           2.406278                 1.008411                  1.206170      1.233333                    0.011768                    0.049977                                  0.0                NaN                      NaN                       NaN           NaN                         NaN                         NaN                                  NaN                 NaN                       NaN                        NaN            NaN                          NaN                          NaN                                   NaN                 NaN  ...          2.550730                1.000816                 1.008874     1.205161                   0.000683                   0.034478                                 0.0     0.059427                    0.910515                         NaN                          NaN           0.853244                NaN                 NaN              0.972445     0.982223               0.811284   \n",
       "1           1     2167584.0       169.90           2.570386                 1.012324                  1.023911      1.277528                    0.007519                    0.041544                                  0.0           2.138433                 1.010411                  1.007476      1.849783                    0.010887                    0.097403                                  0.0            2.087838                  1.003921                   1.019671       2.210915                     0.013013                     0.105117                                   0.0                 NaN  ...          0.316113                1.000618                 0.996687     1.510986                   0.000889                   0.079502                                 0.0     0.099913                    0.874627                    0.742786                     0.662212           0.782519           0.592113            0.695115              0.844962     0.938508               0.630043   \n",
       "2           2      908268.0       184.45           2.339982                 1.000686                  1.031583      1.262564                    0.006845                    0.047219                                  0.0           2.121880                 1.007725                  1.012728      1.526664                    0.008277                    0.079306                                  0.0            1.999881                  1.003812                   1.019996       2.393700                     0.012977                     0.117752                                   0.0                 NaN  ...          0.475301                1.000196                 1.000570     1.510986                   0.000873                   0.076064                                 0.0     0.106564                    0.988013                    0.771849                     0.661123           0.987802           0.681214            0.709677              0.908826     0.958653               0.704871   \n",
       "3           3     1479449.0       188.90           1.817743                 1.009085                  1.050660      1.272425                    0.006736                    0.050375                                  0.0           0.618434                 0.997120                  0.968908      1.505648                    0.007971                    0.105323                                  0.0            0.539262                  1.002220                   0.992524       1.586279                     0.008397                     0.110237                                   0.0            1.146368  ...          1.045681                1.000126                 1.001366     1.510986                   0.000821                   0.076733                                 0.0     0.102616                    0.837600                    1.196999                     0.907093           0.836421           1.134992            0.791689              0.833995     0.862504               0.681871   \n",
       "4           4      659684.0       283.70           2.180677                 1.006883                  1.156869      1.243475                    0.004383                    0.044750                                  0.0           1.856338                 1.005233                  0.983717      1.338377                    0.004718                    0.055730                                  0.0            1.829468                  1.004957                   1.003570       1.709517                     0.006026                     0.112228                                   0.0            2.328634  ...          2.419453                1.001572                 1.005738     1.356484                   0.000615                   0.061583                                 0.0     0.128638                    0.935286                    0.863594                     0.756640           0.876630           0.766655            0.608037              0.824917     0.921578               0.683500   \n",
       "\n",
       "   stock_rsi_10  stock_relative_rsi_34  stock_rsi_34  stock_relative_rsi_100  stock_rsi_100    result  trades_last_year  days_since_last_signal  \n",
       "0      0.867440               0.574894      0.654295                     NaN            NaN  0.334595                 0                     NaN  \n",
       "1      0.713423               0.582931      0.604268                0.576904       0.579589 -0.134604                 1                    94.0  \n",
       "2      0.750717               0.596260      0.613973                0.579304       0.582695 -0.067935                 2                    25.0  \n",
       "3      0.702627               0.535038      0.554836                0.532713       0.545055  0.282463                 3                    55.0  \n",
       "4      0.772665               0.577465      0.641126                0.554928       0.594481  0.297843                 3                    33.0  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "tradedf = pd.DataFrame()\n",
    "stockdf = pd.DataFrame()\n",
    "\n",
    "try: \n",
    "  df = pd.read_csv('stockytrades.csv')\n",
    "except:\n",
    "  print('Failed to read file')\n",
    "  docs = db.collection('trades').stream()\n",
    "\n",
    "  for doc in docs:\n",
    "    print('starting', doc.id)\n",
    "    stockdf = get_price_data(doc.id)\n",
    "    tradedf =  get_trade_data(doc)\n",
    "    merged_df = pd.merge(stockdf, omxdf, on='date', suffixes=('_stock', '_omx'))\n",
    "    merged_df = pd.merge(merged_df, merge_index_stock_df(omxdf, stockdf), on='date')\n",
    "\n",
    "    \n",
    "    # merged_df = pd.merge(merged_df, tradedf, on='date', suffixes=('_1', '_2'))\n",
    "    merged_df = pd.merge(merged_df, tradedf, on='date')\n",
    "    merged_df.drop(columns=['owners'], inplace = True, errors='ignore')\n",
    "    \n",
    "    df = pd.concat([df, merged_df], ignore_index=True)\n",
    "  \n",
    "  # Save the file so we dont have to next time\n",
    "  # Drop problematic columns.\n",
    "  df.to_csv('stockytrades.csv')\n",
    "\n",
    "df.drop(columns=[\n",
    "                # These values does not carry much importance\n",
    "                'volume_omx',\n",
    "                'owners_omx',\n",
    "                'date',\n",
    "                'high_stock',\n",
    "                'low_stock',\n",
    "                'owners_stock',\n",
    "                # 'close_stock',\n",
    "                'open_stock',\n",
    "                'high_omx',\n",
    "                'low_omx',\n",
    "                'owners_omx',\n",
    "                'close_omx',\n",
    "                'open_omx',\n",
    "                # Theses values are missing a lot of the time and would result in a lot of rows being dropped.\n",
    "                # TODO: See if you can improve the data quality to be able to use more of these\n",
    "                'owners_stock', \n",
    "                'zs-20-volume_omx',\n",
    "                'ma-slope-20-volume_omx',\n",
    "                'std-slope-20-volume_omx',\n",
    "                'percent-rng-20-volume_omx',\n",
    "                'percent-std-20-volume_omx',\n",
    "                'avg-log-volume-20_omx',\n",
    "                'zs-50-volume_omx',\n",
    "                'ma-slope-50-volume_omx',\n",
    "                'std-slope-50-volume_omx',\n",
    "                'percent-rng-50-volume_omx',\n",
    "                'percent-std-50-volume_omx',\n",
    "                'avg-log-volume-50_omx',\n",
    "                'zs-100-volume_omx',\n",
    "                'ma-slope-100-volume_omx',\n",
    "                'std-slope-100-volume_omx',\n",
    "                'percent-rng-100-volume_omx',\n",
    "                'percent-std-100-volume_omx',\n",
    "                'avg-log-volume-100_omx',\n",
    "                'zs-200-volume_omx',\n",
    "                'ma-slope-200-volume_omx',\n",
    "                'std-slope-200-volume_omx',\n",
    "                'percent-rng-200-volume_omx',\n",
    "                'percent-std-200-volume_omx',\n",
    "                'avg-log-volume-200_omx',\n",
    "                'zs-200-volume_stock',            \n",
    "                'ma-slope-200-volume_stock',      \n",
    "                'std-slope-200-volume_stock',     \n",
    "                'percent-rng-200-volume_stock',  \n",
    "                'percent-std-200-volume_stock',   \n",
    "                'zs-100-volume_stock',            \n",
    "                'ma-slope-100-volume_stock',      \n",
    "                'std-slope-100-volume_stock',     \n",
    "                'percent-rng-100-volume_stock',   \n",
    "                'percent-std-100-volume_stock',   \n",
    "                ], inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "print(\"have\", len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "std-slope-200-close_stock               2333\n",
       "ma-slope-200-close_stock                2332\n",
       "percent-std-200-close_stock             2322\n",
       "percent-rng-200-close_stock             2322\n",
       "zs-200-close_stock                      2322\n",
       "rng-200_stock                           2322\n",
       "kaufmanns_efficiency_ratio-200_stock    2322\n",
       "std-slope-50-volume_stock               1367\n",
       "ma-slope-50-volume_stock                1367\n",
       "percent-std-50-volume_stock             1344\n",
       "zs-50-volume_stock                      1344\n",
       "days_since_last_signal                  1284\n",
       "stock_rsi_100                           1274\n",
       "stock_relative_rsi_100                  1274\n",
       "stock_hist_perf100                      1274\n",
       "stock_hist_relative_perf100             1274\n",
       "ma-slope-100-close_stock                1272\n",
       "std-slope-100-close_stock               1272\n",
       "kaufmanns_efficiency_ratio-100_stock    1260\n",
       "zs-100-close_stock                      1260\n",
       "rng-100_stock                           1260\n",
       "percent-rng-100-close_stock             1260\n",
       "percent-std-100-close_stock             1260\n",
       "std-slope-20-volume_stock                750\n",
       "ma-slope-20-volume_stock                 749\n",
       "percent-rng-50-volume_stock              746\n",
       "stock_hist_perf50                        738\n",
       "stock_hist_relative_perf50               738\n",
       "std-slope-50-close_stock                 737\n",
       "ma-slope-50-close_stock                  737\n",
       "zs-20-volume_stock                       734\n",
       "percent-std-20-volume_stock              733\n",
       "percent-std-50-close_stock               725\n",
       "kaufmanns_efficiency_ratio-50_stock      725\n",
       "zs-50-close_stock                        725\n",
       "rng-50_stock                             725\n",
       "percent-rng-50-close_stock               725\n",
       "stock_relative_rsi_34                    543\n",
       "stock_rsi_34                             543\n",
       "percent-rng-20-volume_stock              372\n",
       "std-slope-20-close_stock                 370\n",
       "stock_hist_relative_perf20               362\n",
       "stock_hist_perf20                        362\n",
       "ma-slope-20-close_stock                  361\n",
       "percent-std-20-close_stock               351\n",
       "zs-20-close_stock                        351\n",
       "rng-20_stock                             351\n",
       "percent-rng-20-close_stock               351\n",
       "kaufmanns_efficiency_ratio-20_stock      351\n",
       "stock_rsi_10                             212\n",
       "stock_relative_rsi_10                    212\n",
       "stock_relative_rsi_3                      80\n",
       "stock_rsi_3                               80\n",
       "volume_stock                              21\n",
       "avg-log-volume-200_stock                  21\n",
       "avg-log-volume-100_stock                  21\n",
       "avg-log-volume-50_stock                   21\n",
       "avg-log-volume-20_stock                   21\n",
       "result                                     0\n",
       "trades_last_year                           0\n",
       "ma-slope-200-close_omx                     0\n",
       "stock_quota                                0\n",
       "kaufmanns_efficiency_ratio-200_omx         0\n",
       "percent-std-200-close_omx                  0\n",
       "percent-rng-200-close_omx                  0\n",
       "rng-200_omx                                0\n",
       "std-slope-200-close_omx                    0\n",
       "Unnamed: 0                                 0\n",
       "zs-200-close_omx                           0\n",
       "std-slope-50-close_omx                     0\n",
       "close_stock                                0\n",
       "ma-slope-20-close_omx                      0\n",
       "std-slope-20-close_omx                     0\n",
       "rng-20_omx                                 0\n",
       "percent-rng-20-close_omx                   0\n",
       "percent-std-20-close_omx                   0\n",
       "kaufmanns_efficiency_ratio-20_omx          0\n",
       "zs-50-close_omx                            0\n",
       "ma-slope-50-close_omx                      0\n",
       "rng-50_omx                                 0\n",
       "kaufmanns_efficiency_ratio-100_omx         0\n",
       "percent-rng-50-close_omx                   0\n",
       "percent-std-50-close_omx                   0\n",
       "kaufmanns_efficiency_ratio-50_omx          0\n",
       "zs-100-close_omx                           0\n",
       "ma-slope-100-close_omx                     0\n",
       "std-slope-100-close_omx                    0\n",
       "rng-100_omx                                0\n",
       "percent-rng-100-close_omx                  0\n",
       "percent-std-100-close_omx                  0\n",
       "zs-20-close_omx                            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trades: 17125\n"
     ]
    }
   ],
   "source": [
    "# Clean the data that cannot be used for training\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "print(\"Total number of trades:\", len(df))\n",
    "\n",
    "\n",
    "# Add the label we want to predict\n",
    "df['label'] = np.where(df['result'] > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0344315899193478 89.56603773584905\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 3.4431589919347796%\n",
      "Best 8956.603773584906%\n",
      "Worst -98.48192771084338%\n",
      "std 78.74111515871954%\n"
     ]
    }
   ],
   "source": [
    "#plot distribution of points by team \n",
    "avg = df['result'].mean()\n",
    "best = df['result'].max()\n",
    "worst = df['result'].min()\n",
    "std = df['result'].std()\n",
    "\n",
    "print(avg, best)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "stats = ['Average', 'Best', 'Worst', 'std']\n",
    "columns = [avg, best, worst, std]\n",
    "ax.bar(stats, columns)\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(stats)):\n",
    "  print(stats[i], f'{columns[i]*100}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning part\n",
    "\n",
    "- Drop all `NaN` values first. \n",
    "- Then split the dataset for test and training (using K-fold?). \n",
    "- Train the model\n",
    "- Create a confusion matrix on the validation data. Compare results with reality\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "tf.autograph.set_verbosity(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "train_data, validation_data =  train_test_split(df, test_size=0.15, random_state=3456) \n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = train_data.drop(columns=['result', 'label'], axis=1).values\n",
    "y = train_data['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'volume_stock', 'close_stock', 'zs-20-close_stock', 'ma-slope-20-close_stock', 'std-slope-20-close_stock', 'rng-20_stock', 'percent-rng-20-close_stock', 'percent-std-20-close_stock', 'kaufmanns_efficiency_ratio-20_stock', 'zs-50-close_stock', 'ma-slope-50-close_stock', 'std-slope-50-close_stock', 'rng-50_stock', 'percent-rng-50-close_stock', 'percent-std-50-close_stock', 'kaufmanns_efficiency_ratio-50_stock', 'zs-100-close_stock', 'ma-slope-100-close_stock', 'std-slope-100-close_stock', 'rng-100_stock', 'percent-rng-100-close_stock', 'percent-std-100-close_stock', 'kaufmanns_efficiency_ratio-100_stock', 'zs-200-close_stock', 'ma-slope-200-close_stock', 'std-slope-200-close_stock', 'rng-200_stock', 'percent-rng-200-close_stock', 'percent-std-200-close_stock', 'kaufmanns_efficiency_ratio-200_stock', 'zs-20-volume_stock', 'ma-slope-20-volume_stock', 'std-slope-20-volume_stock', 'percent-rng-20-volume_stock', 'percent-std-20-volume_stock', 'avg-log-volume-20_stock', 'zs-50-volume_stock', 'ma-slope-50-volume_stock', 'std-slope-50-volume_stock', 'percent-rng-50-volume_stock', 'percent-std-50-volume_stock', 'avg-log-volume-50_stock', 'avg-log-volume-100_stock', 'avg-log-volume-200_stock', 'zs-20-close_omx', 'ma-slope-20-close_omx', 'std-slope-20-close_omx', 'rng-20_omx', 'percent-rng-20-close_omx', 'percent-std-20-close_omx', 'kaufmanns_efficiency_ratio-20_omx', 'zs-50-close_omx', 'ma-slope-50-close_omx', 'std-slope-50-close_omx', 'rng-50_omx', 'percent-rng-50-close_omx', 'percent-std-50-close_omx', 'kaufmanns_efficiency_ratio-50_omx', 'zs-100-close_omx', 'ma-slope-100-close_omx', 'std-slope-100-close_omx', 'rng-100_omx', 'percent-rng-100-close_omx', 'percent-std-100-close_omx', 'kaufmanns_efficiency_ratio-100_omx', 'zs-200-close_omx', 'ma-slope-200-close_omx', 'std-slope-200-close_omx', 'rng-200_omx', 'percent-rng-200-close_omx', 'percent-std-200-close_omx', 'kaufmanns_efficiency_ratio-200_omx', 'stock_quota', 'stock_hist_relative_perf20', 'stock_hist_relative_perf50', 'stock_hist_relative_perf100', 'stock_hist_perf20', 'stock_hist_perf50', 'stock_hist_perf100', 'stock_relative_rsi_3', 'stock_rsi_3', 'stock_relative_rsi_10', 'stock_rsi_10', 'stock_relative_rsi_34', 'stock_rsi_34', 'stock_relative_rsi_100', 'stock_rsi_100', 'result', 'trades_last_year', 'days_since_last_signal', 'label']\n"
     ]
    }
   ],
   "source": [
    "# K-fold\n",
    "num_folds = 7\n",
    "# Define the k-fold cross-validator\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "\n",
    "print(df.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize input data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define TensorFlow model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X.shape[1],)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    # tf.keras.layers.BatchNormalization(),\n",
    "    # tf.keras.layers.Dropout(0.5),\n",
    "    # tf.keras.layers.Dense(32, activation='relu'),\n",
    "    # tf.keras.layers.BatchNormalization(),\n",
    "    # tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bauhn\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0 1], y=[0 1 1 ... 0 0 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15)\n",
    "\n",
    "# Define learning rate scheduler callback\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.25)\n",
    "\n",
    "# Calculate class weights based on occurrence since the base strategy loses more often we want to equalize the weights\n",
    "class_weights = compute_class_weight('balanced', np.unique(y), y)\n",
    "\n",
    "# Convert class weights to a dictionary\n",
    "class_weights_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "780/780 [==============================] - 2s 1ms/step - loss: 0.7152 - accuracy: 0.5445 - val_loss: 0.6724 - val_accuracy: 0.5909 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6833 - accuracy: 0.5810 - val_loss: 0.6692 - val_accuracy: 0.5928 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6761 - accuracy: 0.5931 - val_loss: 0.6570 - val_accuracy: 0.6327 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6669 - accuracy: 0.6115 - val_loss: 0.6752 - val_accuracy: 0.5923 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6662 - accuracy: 0.6023 - val_loss: 0.6551 - val_accuracy: 0.6245 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6621 - accuracy: 0.6179 - val_loss: 0.6603 - val_accuracy: 0.6197 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6581 - accuracy: 0.6177 - val_loss: 0.6512 - val_accuracy: 0.6471 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6615 - accuracy: 0.6185 - val_loss: 0.6513 - val_accuracy: 0.6279 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6561 - accuracy: 0.6233 - val_loss: 0.6600 - val_accuracy: 0.6279 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6550 - accuracy: 0.6322 - val_loss: 0.6559 - val_accuracy: 0.6394 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6534 - accuracy: 0.6401 - val_loss: 0.6504 - val_accuracy: 0.6404 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6535 - accuracy: 0.6331 - val_loss: 0.6403 - val_accuracy: 0.6529 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6512 - accuracy: 0.6333 - val_loss: 0.6397 - val_accuracy: 0.6558 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6507 - accuracy: 0.6393 - val_loss: 0.6409 - val_accuracy: 0.6534 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6491 - accuracy: 0.6323 - val_loss: 0.6311 - val_accuracy: 0.6654 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6465 - accuracy: 0.6479 - val_loss: 0.6491 - val_accuracy: 0.6385 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6468 - accuracy: 0.6416 - val_loss: 0.6457 - val_accuracy: 0.6514 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6463 - accuracy: 0.6474 - val_loss: 0.6482 - val_accuracy: 0.6486 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6446 - accuracy: 0.6430 - val_loss: 0.6330 - val_accuracy: 0.6683 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6426 - accuracy: 0.6454 - val_loss: 0.6368 - val_accuracy: 0.6630 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6380 - accuracy: 0.6528 - val_loss: 0.6341 - val_accuracy: 0.6587 - lr: 2.5000e-04\n",
      "Epoch 22/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6361 - accuracy: 0.6532 - val_loss: 0.6343 - val_accuracy: 0.6606 - lr: 2.5000e-04\n",
      "Epoch 23/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6323 - accuracy: 0.6538 - val_loss: 0.6331 - val_accuracy: 0.6635 - lr: 2.5000e-04\n",
      "Epoch 24/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6306 - accuracy: 0.6534 - val_loss: 0.6290 - val_accuracy: 0.6678 - lr: 2.5000e-04\n",
      "Epoch 25/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6307 - accuracy: 0.6576 - val_loss: 0.6320 - val_accuracy: 0.6649 - lr: 2.5000e-04\n",
      "Epoch 26/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6283 - accuracy: 0.6555 - val_loss: 0.6284 - val_accuracy: 0.6654 - lr: 2.5000e-04\n",
      "Epoch 27/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6258 - accuracy: 0.6570 - val_loss: 0.6251 - val_accuracy: 0.6731 - lr: 2.5000e-04\n",
      "Epoch 28/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6254 - accuracy: 0.6615 - val_loss: 0.6312 - val_accuracy: 0.6582 - lr: 2.5000e-04\n",
      "Epoch 29/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6267 - accuracy: 0.6595 - val_loss: 0.6355 - val_accuracy: 0.6668 - lr: 2.5000e-04\n",
      "Epoch 30/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6228 - accuracy: 0.6634 - val_loss: 0.6246 - val_accuracy: 0.6755 - lr: 2.5000e-04\n",
      "Epoch 31/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6248 - accuracy: 0.6587 - val_loss: 0.6321 - val_accuracy: 0.6596 - lr: 2.5000e-04\n",
      "Epoch 32/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6244 - accuracy: 0.6638 - val_loss: 0.6346 - val_accuracy: 0.6587 - lr: 2.5000e-04\n",
      "Epoch 33/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6238 - accuracy: 0.6613 - val_loss: 0.6283 - val_accuracy: 0.6687 - lr: 2.5000e-04\n",
      "Epoch 34/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6236 - accuracy: 0.6644 - val_loss: 0.6446 - val_accuracy: 0.6452 - lr: 2.5000e-04\n",
      "Epoch 35/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6194 - accuracy: 0.6626 - val_loss: 0.6321 - val_accuracy: 0.6659 - lr: 2.5000e-04\n",
      "Epoch 36/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6204 - accuracy: 0.6666 - val_loss: 0.6275 - val_accuracy: 0.6712 - lr: 6.2500e-05\n",
      "Epoch 37/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6171 - accuracy: 0.6607 - val_loss: 0.6355 - val_accuracy: 0.6548 - lr: 6.2500e-05\n",
      "Epoch 38/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6169 - accuracy: 0.6652 - val_loss: 0.6360 - val_accuracy: 0.6577 - lr: 6.2500e-05\n",
      "Epoch 39/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6169 - accuracy: 0.6634 - val_loss: 0.6322 - val_accuracy: 0.6558 - lr: 6.2500e-05\n",
      "Epoch 40/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6160 - accuracy: 0.6694 - val_loss: 0.6290 - val_accuracy: 0.6615 - lr: 6.2500e-05\n",
      "Epoch 41/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6133 - accuracy: 0.6675 - val_loss: 0.6311 - val_accuracy: 0.6567 - lr: 1.5625e-05\n",
      "Epoch 42/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6154 - accuracy: 0.6691 - val_loss: 0.6368 - val_accuracy: 0.6558 - lr: 1.5625e-05\n",
      "Epoch 43/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6192 - accuracy: 0.6667 - val_loss: 0.6280 - val_accuracy: 0.6649 - lr: 1.5625e-05\n",
      "Epoch 44/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6151 - accuracy: 0.6660 - val_loss: 0.6305 - val_accuracy: 0.6601 - lr: 1.5625e-05\n",
      "Epoch 45/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6158 - accuracy: 0.6679 - val_loss: 0.6330 - val_accuracy: 0.6601 - lr: 1.5625e-05\n",
      "hello\n",
      "Epoch 1/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6244 - accuracy: 0.6605 - val_loss: 0.6020 - val_accuracy: 0.6986 - lr: 3.9063e-06\n",
      "Epoch 2/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6232 - accuracy: 0.6554 - val_loss: 0.5997 - val_accuracy: 0.6990 - lr: 3.9063e-06\n",
      "Epoch 3/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6214 - accuracy: 0.6639 - val_loss: 0.5968 - val_accuracy: 0.6962 - lr: 3.9063e-06\n",
      "Epoch 4/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6227 - accuracy: 0.6601 - val_loss: 0.5976 - val_accuracy: 0.7019 - lr: 3.9063e-06\n",
      "Epoch 5/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6220 - accuracy: 0.6607 - val_loss: 0.6007 - val_accuracy: 0.6971 - lr: 3.9063e-06\n",
      "Epoch 6/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6237 - accuracy: 0.6574 - val_loss: 0.5990 - val_accuracy: 0.6995 - lr: 3.9063e-06\n",
      "Epoch 7/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6239 - accuracy: 0.6609 - val_loss: 0.6002 - val_accuracy: 0.7014 - lr: 3.9063e-06\n",
      "Epoch 8/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6216 - accuracy: 0.6607 - val_loss: 0.6010 - val_accuracy: 0.7019 - lr: 3.9063e-06\n",
      "Epoch 9/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6229 - accuracy: 0.6602 - val_loss: 0.6012 - val_accuracy: 0.7024 - lr: 9.7656e-07\n",
      "Epoch 10/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6237 - accuracy: 0.6569 - val_loss: 0.5983 - val_accuracy: 0.7024 - lr: 9.7656e-07\n",
      "Epoch 11/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6219 - accuracy: 0.6634 - val_loss: 0.6011 - val_accuracy: 0.6962 - lr: 9.7656e-07\n",
      "Epoch 12/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6233 - accuracy: 0.6596 - val_loss: 0.5993 - val_accuracy: 0.7005 - lr: 9.7656e-07\n",
      "Epoch 13/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6224 - accuracy: 0.6606 - val_loss: 0.6005 - val_accuracy: 0.6990 - lr: 9.7656e-07\n",
      "Epoch 14/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6236 - accuracy: 0.6653 - val_loss: 0.5985 - val_accuracy: 0.6942 - lr: 2.4414e-07\n",
      "Epoch 15/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6227 - accuracy: 0.6595 - val_loss: 0.5962 - val_accuracy: 0.7000 - lr: 2.4414e-07\n",
      "Epoch 16/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6217 - accuracy: 0.6663 - val_loss: 0.6014 - val_accuracy: 0.6952 - lr: 2.4414e-07\n",
      "Epoch 17/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6242 - accuracy: 0.6602 - val_loss: 0.6027 - val_accuracy: 0.6976 - lr: 2.4414e-07\n",
      "Epoch 18/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6224 - accuracy: 0.6598 - val_loss: 0.5979 - val_accuracy: 0.6981 - lr: 2.4414e-07\n",
      "Epoch 19/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6232 - accuracy: 0.6589 - val_loss: 0.6002 - val_accuracy: 0.7019 - lr: 2.4414e-07\n",
      "Epoch 20/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6228 - accuracy: 0.6601 - val_loss: 0.5998 - val_accuracy: 0.6981 - lr: 2.4414e-07\n",
      "Epoch 21/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6217 - accuracy: 0.6601 - val_loss: 0.5991 - val_accuracy: 0.6952 - lr: 6.1035e-08\n",
      "Epoch 22/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6225 - accuracy: 0.6564 - val_loss: 0.6005 - val_accuracy: 0.6904 - lr: 6.1035e-08\n",
      "Epoch 23/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6227 - accuracy: 0.6630 - val_loss: 0.5989 - val_accuracy: 0.7019 - lr: 6.1035e-08\n",
      "Epoch 24/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6249 - accuracy: 0.6533 - val_loss: 0.6030 - val_accuracy: 0.6938 - lr: 6.1035e-08\n",
      "Epoch 25/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6258 - accuracy: 0.6525 - val_loss: 0.5992 - val_accuracy: 0.6971 - lr: 6.1035e-08\n",
      "Epoch 26/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6254 - accuracy: 0.6603 - val_loss: 0.5997 - val_accuracy: 0.7024 - lr: 1.5259e-08\n",
      "Epoch 27/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6217 - accuracy: 0.6620 - val_loss: 0.5989 - val_accuracy: 0.6928 - lr: 1.5259e-08\n",
      "Epoch 28/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6234 - accuracy: 0.6589 - val_loss: 0.5997 - val_accuracy: 0.6962 - lr: 1.5259e-08\n",
      "Epoch 29/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6249 - accuracy: 0.6602 - val_loss: 0.5995 - val_accuracy: 0.7014 - lr: 1.5259e-08\n",
      "Epoch 30/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6267 - accuracy: 0.6597 - val_loss: 0.5972 - val_accuracy: 0.7034 - lr: 1.5259e-08\n",
      "hello\n",
      "Epoch 1/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6302 - accuracy: 0.6545 - val_loss: 0.5885 - val_accuracy: 0.7058 - lr: 3.8147e-09\n",
      "Epoch 2/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6245 - accuracy: 0.6630 - val_loss: 0.5933 - val_accuracy: 0.7024 - lr: 3.8147e-09\n",
      "Epoch 3/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6292 - accuracy: 0.6555 - val_loss: 0.5923 - val_accuracy: 0.7053 - lr: 3.8147e-09\n",
      "Epoch 4/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6262 - accuracy: 0.6608 - val_loss: 0.5899 - val_accuracy: 0.7067 - lr: 3.8147e-09\n",
      "Epoch 5/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6274 - accuracy: 0.6560 - val_loss: 0.5934 - val_accuracy: 0.6986 - lr: 3.8147e-09\n",
      "Epoch 6/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6295 - accuracy: 0.6588 - val_loss: 0.5955 - val_accuracy: 0.7067 - lr: 3.8147e-09\n",
      "Epoch 7/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6311 - accuracy: 0.6515 - val_loss: 0.5975 - val_accuracy: 0.6966 - lr: 9.5367e-10\n",
      "Epoch 8/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6249 - accuracy: 0.6597 - val_loss: 0.5886 - val_accuracy: 0.7005 - lr: 9.5367e-10\n",
      "Epoch 9/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6309 - accuracy: 0.6544 - val_loss: 0.5899 - val_accuracy: 0.7010 - lr: 9.5367e-10\n",
      "Epoch 10/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6291 - accuracy: 0.6512 - val_loss: 0.5951 - val_accuracy: 0.6986 - lr: 9.5367e-10\n",
      "Epoch 11/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6251 - accuracy: 0.6567 - val_loss: 0.5888 - val_accuracy: 0.7063 - lr: 9.5367e-10\n",
      "Epoch 12/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6264 - accuracy: 0.6557 - val_loss: 0.5899 - val_accuracy: 0.7067 - lr: 2.3842e-10\n",
      "Epoch 13/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6333 - accuracy: 0.6492 - val_loss: 0.5860 - val_accuracy: 0.7038 - lr: 2.3842e-10\n",
      "Epoch 14/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6316 - accuracy: 0.6537 - val_loss: 0.5959 - val_accuracy: 0.6976 - lr: 2.3842e-10\n",
      "Epoch 15/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6272 - accuracy: 0.6552 - val_loss: 0.5877 - val_accuracy: 0.7043 - lr: 2.3842e-10\n",
      "Epoch 16/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6276 - accuracy: 0.6551 - val_loss: 0.5959 - val_accuracy: 0.6966 - lr: 2.3842e-10\n",
      "Epoch 17/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6237 - accuracy: 0.6613 - val_loss: 0.5980 - val_accuracy: 0.6995 - lr: 2.3842e-10\n",
      "Epoch 18/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6307 - accuracy: 0.6527 - val_loss: 0.5911 - val_accuracy: 0.7043 - lr: 2.3842e-10\n",
      "Epoch 19/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6251 - accuracy: 0.6601 - val_loss: 0.5902 - val_accuracy: 0.6995 - lr: 5.9605e-11\n",
      "Epoch 20/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6275 - accuracy: 0.6580 - val_loss: 0.5885 - val_accuracy: 0.7000 - lr: 5.9605e-11\n",
      "Epoch 21/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6260 - accuracy: 0.6574 - val_loss: 0.5906 - val_accuracy: 0.7034 - lr: 5.9605e-11\n",
      "Epoch 22/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6298 - accuracy: 0.6530 - val_loss: 0.5948 - val_accuracy: 0.6962 - lr: 5.9605e-11\n",
      "Epoch 23/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6258 - accuracy: 0.6554 - val_loss: 0.5924 - val_accuracy: 0.7024 - lr: 5.9605e-11\n",
      "Epoch 24/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6267 - accuracy: 0.6564 - val_loss: 0.5959 - val_accuracy: 0.6986 - lr: 1.4901e-11\n",
      "Epoch 25/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6301 - accuracy: 0.6539 - val_loss: 0.5871 - val_accuracy: 0.7048 - lr: 1.4901e-11\n",
      "Epoch 26/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6287 - accuracy: 0.6565 - val_loss: 0.5934 - val_accuracy: 0.6986 - lr: 1.4901e-11\n",
      "Epoch 27/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6293 - accuracy: 0.6585 - val_loss: 0.5910 - val_accuracy: 0.7024 - lr: 1.4901e-11\n",
      "Epoch 28/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6322 - accuracy: 0.6534 - val_loss: 0.5876 - val_accuracy: 0.7053 - lr: 1.4901e-11\n",
      "hello\n",
      "Epoch 1/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6233 - accuracy: 0.6609 - val_loss: 0.6014 - val_accuracy: 0.6950 - lr: 3.7253e-12\n",
      "Epoch 2/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6231 - accuracy: 0.6607 - val_loss: 0.6012 - val_accuracy: 0.6936 - lr: 3.7253e-12\n",
      "Epoch 3/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6241 - accuracy: 0.6621 - val_loss: 0.5953 - val_accuracy: 0.7003 - lr: 3.7253e-12\n",
      "Epoch 4/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6245 - accuracy: 0.6629 - val_loss: 0.5989 - val_accuracy: 0.6984 - lr: 3.7253e-12\n",
      "Epoch 5/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6220 - accuracy: 0.6599 - val_loss: 0.5945 - val_accuracy: 0.7047 - lr: 3.7253e-12\n",
      "Epoch 6/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6216 - accuracy: 0.6619 - val_loss: 0.6044 - val_accuracy: 0.6950 - lr: 3.7253e-12\n",
      "Epoch 7/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6222 - accuracy: 0.6658 - val_loss: 0.5957 - val_accuracy: 0.6979 - lr: 3.7253e-12\n",
      "Epoch 8/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6191 - accuracy: 0.6639 - val_loss: 0.5998 - val_accuracy: 0.6965 - lr: 3.7253e-12\n",
      "Epoch 9/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6246 - accuracy: 0.6586 - val_loss: 0.5947 - val_accuracy: 0.6975 - lr: 3.7253e-12\n",
      "Epoch 10/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6249 - accuracy: 0.6557 - val_loss: 0.5977 - val_accuracy: 0.6984 - lr: 3.7253e-12\n",
      "Epoch 11/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6224 - accuracy: 0.6617 - val_loss: 0.5989 - val_accuracy: 0.6994 - lr: 9.3132e-13\n",
      "Epoch 12/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6219 - accuracy: 0.6641 - val_loss: 0.5980 - val_accuracy: 0.6999 - lr: 9.3132e-13\n",
      "Epoch 13/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6224 - accuracy: 0.6611 - val_loss: 0.5964 - val_accuracy: 0.6989 - lr: 9.3132e-13\n",
      "Epoch 14/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6259 - accuracy: 0.6550 - val_loss: 0.5920 - val_accuracy: 0.7085 - lr: 9.3132e-13\n",
      "Epoch 15/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6247 - accuracy: 0.6610 - val_loss: 0.6030 - val_accuracy: 0.6975 - lr: 9.3132e-13\n",
      "Epoch 16/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6233 - accuracy: 0.6658 - val_loss: 0.6068 - val_accuracy: 0.6955 - lr: 9.3132e-13\n",
      "Epoch 17/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6194 - accuracy: 0.6639 - val_loss: 0.5977 - val_accuracy: 0.7018 - lr: 9.3132e-13\n",
      "Epoch 18/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6256 - accuracy: 0.6556 - val_loss: 0.6018 - val_accuracy: 0.7032 - lr: 9.3132e-13\n",
      "Epoch 19/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6238 - accuracy: 0.6602 - val_loss: 0.5982 - val_accuracy: 0.7003 - lr: 9.3132e-13\n",
      "Epoch 20/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6249 - accuracy: 0.6587 - val_loss: 0.6041 - val_accuracy: 0.6926 - lr: 2.3283e-13\n",
      "Epoch 21/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6207 - accuracy: 0.6595 - val_loss: 0.5988 - val_accuracy: 0.6975 - lr: 2.3283e-13\n",
      "Epoch 22/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6209 - accuracy: 0.6635 - val_loss: 0.5972 - val_accuracy: 0.7023 - lr: 2.3283e-13\n",
      "Epoch 23/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6208 - accuracy: 0.6630 - val_loss: 0.5992 - val_accuracy: 0.6965 - lr: 2.3283e-13\n",
      "Epoch 24/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6212 - accuracy: 0.6614 - val_loss: 0.5988 - val_accuracy: 0.6926 - lr: 2.3283e-13\n",
      "Epoch 25/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6265 - accuracy: 0.6618 - val_loss: 0.5984 - val_accuracy: 0.7023 - lr: 5.8208e-14\n",
      "Epoch 26/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6243 - accuracy: 0.6611 - val_loss: 0.5970 - val_accuracy: 0.7013 - lr: 5.8208e-14\n",
      "Epoch 27/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6230 - accuracy: 0.6530 - val_loss: 0.5980 - val_accuracy: 0.7023 - lr: 5.8208e-14\n",
      "Epoch 28/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6241 - accuracy: 0.6626 - val_loss: 0.5976 - val_accuracy: 0.7032 - lr: 5.8208e-14\n",
      "Epoch 29/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6199 - accuracy: 0.6640 - val_loss: 0.5999 - val_accuracy: 0.7008 - lr: 5.8208e-14\n",
      "hello\n",
      "Epoch 1/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6246 - accuracy: 0.6620 - val_loss: 0.5909 - val_accuracy: 0.7056 - lr: 1.4552e-14\n",
      "Epoch 2/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6227 - accuracy: 0.6586 - val_loss: 0.5891 - val_accuracy: 0.7071 - lr: 1.4552e-14\n",
      "Epoch 3/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6230 - accuracy: 0.6626 - val_loss: 0.5909 - val_accuracy: 0.6999 - lr: 1.4552e-14\n",
      "Epoch 4/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6261 - accuracy: 0.6557 - val_loss: 0.5918 - val_accuracy: 0.7061 - lr: 1.4552e-14\n",
      "Epoch 5/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6240 - accuracy: 0.6518 - val_loss: 0.5919 - val_accuracy: 0.7042 - lr: 1.4552e-14\n",
      "Epoch 6/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6265 - accuracy: 0.6566 - val_loss: 0.5923 - val_accuracy: 0.7071 - lr: 1.4552e-14\n",
      "Epoch 7/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6244 - accuracy: 0.6602 - val_loss: 0.5925 - val_accuracy: 0.7100 - lr: 1.4552e-14\n",
      "Epoch 8/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6252 - accuracy: 0.6626 - val_loss: 0.5908 - val_accuracy: 0.7071 - lr: 3.6380e-15\n",
      "Epoch 9/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6241 - accuracy: 0.6629 - val_loss: 0.5938 - val_accuracy: 0.7056 - lr: 3.6380e-15\n",
      "Epoch 10/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6235 - accuracy: 0.6609 - val_loss: 0.5924 - val_accuracy: 0.7032 - lr: 3.6380e-15\n",
      "Epoch 11/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6258 - accuracy: 0.6591 - val_loss: 0.5906 - val_accuracy: 0.7061 - lr: 3.6380e-15\n",
      "Epoch 12/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6222 - accuracy: 0.6604 - val_loss: 0.5915 - val_accuracy: 0.7104 - lr: 3.6380e-15\n",
      "Epoch 13/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6224 - accuracy: 0.6591 - val_loss: 0.5924 - val_accuracy: 0.7051 - lr: 9.0949e-16\n",
      "Epoch 14/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6212 - accuracy: 0.6615 - val_loss: 0.5922 - val_accuracy: 0.7071 - lr: 9.0949e-16\n",
      "Epoch 15/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6248 - accuracy: 0.6603 - val_loss: 0.5947 - val_accuracy: 0.7066 - lr: 9.0949e-16\n",
      "Epoch 16/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6264 - accuracy: 0.6533 - val_loss: 0.5897 - val_accuracy: 0.7056 - lr: 9.0949e-16\n",
      "Epoch 17/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6250 - accuracy: 0.6585 - val_loss: 0.5909 - val_accuracy: 0.7090 - lr: 9.0949e-16\n",
      "hello\n",
      "Epoch 1/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6244 - accuracy: 0.6574 - val_loss: 0.5901 - val_accuracy: 0.7071 - lr: 2.2737e-16\n",
      "Epoch 2/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6259 - accuracy: 0.6540 - val_loss: 0.5951 - val_accuracy: 0.7013 - lr: 2.2737e-16\n",
      "Epoch 3/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6219 - accuracy: 0.6557 - val_loss: 0.5907 - val_accuracy: 0.7027 - lr: 2.2737e-16\n",
      "Epoch 4/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6183 - accuracy: 0.6651 - val_loss: 0.5904 - val_accuracy: 0.7076 - lr: 2.2737e-16\n",
      "Epoch 5/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6227 - accuracy: 0.6595 - val_loss: 0.5967 - val_accuracy: 0.6975 - lr: 2.2737e-16\n",
      "Epoch 6/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6246 - accuracy: 0.6569 - val_loss: 0.5955 - val_accuracy: 0.7013 - lr: 2.2737e-16\n",
      "Epoch 7/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6214 - accuracy: 0.6604 - val_loss: 0.5921 - val_accuracy: 0.7051 - lr: 5.6843e-17\n",
      "Epoch 8/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6270 - accuracy: 0.6530 - val_loss: 0.5958 - val_accuracy: 0.7066 - lr: 5.6843e-17\n",
      "Epoch 9/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6226 - accuracy: 0.6568 - val_loss: 0.5909 - val_accuracy: 0.7056 - lr: 5.6843e-17\n",
      "Epoch 10/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6239 - accuracy: 0.6551 - val_loss: 0.5933 - val_accuracy: 0.7018 - lr: 5.6843e-17\n",
      "Epoch 11/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6242 - accuracy: 0.6600 - val_loss: 0.5912 - val_accuracy: 0.7090 - lr: 5.6843e-17\n",
      "Epoch 12/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6225 - accuracy: 0.6610 - val_loss: 0.5894 - val_accuracy: 0.7095 - lr: 1.4211e-17\n",
      "Epoch 13/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6229 - accuracy: 0.6588 - val_loss: 0.5948 - val_accuracy: 0.7008 - lr: 1.4211e-17\n",
      "Epoch 14/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6221 - accuracy: 0.6581 - val_loss: 0.5937 - val_accuracy: 0.7061 - lr: 1.4211e-17\n",
      "Epoch 15/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6233 - accuracy: 0.6609 - val_loss: 0.5932 - val_accuracy: 0.7018 - lr: 1.4211e-17\n",
      "Epoch 16/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6250 - accuracy: 0.6554 - val_loss: 0.5906 - val_accuracy: 0.7066 - lr: 1.4211e-17\n",
      "Epoch 17/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6267 - accuracy: 0.6566 - val_loss: 0.5936 - val_accuracy: 0.7042 - lr: 1.4211e-17\n",
      "Epoch 18/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6250 - accuracy: 0.6599 - val_loss: 0.5910 - val_accuracy: 0.7066 - lr: 3.5527e-18\n",
      "Epoch 19/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6261 - accuracy: 0.6502 - val_loss: 0.5931 - val_accuracy: 0.7027 - lr: 3.5527e-18\n",
      "Epoch 20/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6243 - accuracy: 0.6580 - val_loss: 0.5903 - val_accuracy: 0.7047 - lr: 3.5527e-18\n",
      "Epoch 21/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6229 - accuracy: 0.6619 - val_loss: 0.5930 - val_accuracy: 0.7080 - lr: 3.5527e-18\n",
      "Epoch 22/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6240 - accuracy: 0.6601 - val_loss: 0.5977 - val_accuracy: 0.7008 - lr: 3.5527e-18\n",
      "Epoch 23/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6229 - accuracy: 0.6626 - val_loss: 0.5958 - val_accuracy: 0.7008 - lr: 8.8818e-19\n",
      "Epoch 24/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6250 - accuracy: 0.6634 - val_loss: 0.5891 - val_accuracy: 0.7085 - lr: 8.8818e-19\n",
      "Epoch 25/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6211 - accuracy: 0.6607 - val_loss: 0.5907 - val_accuracy: 0.7018 - lr: 8.8818e-19\n",
      "Epoch 26/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6264 - accuracy: 0.6584 - val_loss: 0.5920 - val_accuracy: 0.7066 - lr: 8.8818e-19\n",
      "Epoch 27/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6226 - accuracy: 0.6597 - val_loss: 0.5897 - val_accuracy: 0.7066 - lr: 8.8818e-19\n",
      "Epoch 28/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6215 - accuracy: 0.6591 - val_loss: 0.5956 - val_accuracy: 0.7018 - lr: 8.8818e-19\n",
      "Epoch 29/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6223 - accuracy: 0.6610 - val_loss: 0.5970 - val_accuracy: 0.6970 - lr: 8.8818e-19\n",
      "Epoch 30/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6235 - accuracy: 0.6583 - val_loss: 0.5975 - val_accuracy: 0.7047 - lr: 2.2204e-19\n",
      "Epoch 31/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6243 - accuracy: 0.6602 - val_loss: 0.5978 - val_accuracy: 0.7085 - lr: 2.2204e-19\n",
      "Epoch 32/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6256 - accuracy: 0.6533 - val_loss: 0.5909 - val_accuracy: 0.7013 - lr: 2.2204e-19\n",
      "Epoch 33/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6258 - accuracy: 0.6573 - val_loss: 0.5981 - val_accuracy: 0.7008 - lr: 2.2204e-19\n",
      "Epoch 34/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6256 - accuracy: 0.6548 - val_loss: 0.5924 - val_accuracy: 0.7042 - lr: 2.2204e-19\n",
      "Epoch 35/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6240 - accuracy: 0.6596 - val_loss: 0.5932 - val_accuracy: 0.7076 - lr: 5.5511e-20\n",
      "Epoch 36/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6248 - accuracy: 0.6599 - val_loss: 0.5913 - val_accuracy: 0.7061 - lr: 5.5511e-20\n",
      "Epoch 37/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6217 - accuracy: 0.6635 - val_loss: 0.5903 - val_accuracy: 0.7042 - lr: 5.5511e-20\n",
      "Epoch 38/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6194 - accuracy: 0.6629 - val_loss: 0.5959 - val_accuracy: 0.7051 - lr: 5.5511e-20\n",
      "Epoch 39/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6228 - accuracy: 0.6593 - val_loss: 0.5907 - val_accuracy: 0.7066 - lr: 5.5511e-20\n",
      "hello\n",
      "Epoch 1/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6225 - accuracy: 0.6625 - val_loss: 0.5895 - val_accuracy: 0.6975 - lr: 1.3878e-20\n",
      "Epoch 2/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6210 - accuracy: 0.6648 - val_loss: 0.5932 - val_accuracy: 0.6989 - lr: 1.3878e-20\n",
      "Epoch 3/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6239 - accuracy: 0.6602 - val_loss: 0.5992 - val_accuracy: 0.7003 - lr: 1.3878e-20\n",
      "Epoch 4/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6200 - accuracy: 0.6641 - val_loss: 0.5933 - val_accuracy: 0.6946 - lr: 1.3878e-20\n",
      "Epoch 5/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6208 - accuracy: 0.6622 - val_loss: 0.5954 - val_accuracy: 0.7027 - lr: 1.3878e-20\n",
      "Epoch 6/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6251 - accuracy: 0.6574 - val_loss: 0.5920 - val_accuracy: 0.6965 - lr: 1.3878e-20\n",
      "Epoch 7/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6212 - accuracy: 0.6623 - val_loss: 0.5983 - val_accuracy: 0.6999 - lr: 3.4694e-21\n",
      "Epoch 8/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6229 - accuracy: 0.6648 - val_loss: 0.5919 - val_accuracy: 0.6902 - lr: 3.4694e-21\n",
      "Epoch 9/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6223 - accuracy: 0.6618 - val_loss: 0.5907 - val_accuracy: 0.6950 - lr: 3.4694e-21\n",
      "Epoch 10/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6198 - accuracy: 0.6614 - val_loss: 0.5910 - val_accuracy: 0.7042 - lr: 3.4694e-21\n",
      "Epoch 11/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6214 - accuracy: 0.6592 - val_loss: 0.5965 - val_accuracy: 0.7008 - lr: 3.4694e-21\n",
      "Epoch 12/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6217 - accuracy: 0.6637 - val_loss: 0.5905 - val_accuracy: 0.6946 - lr: 8.6736e-22\n",
      "Epoch 13/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6228 - accuracy: 0.6665 - val_loss: 0.5892 - val_accuracy: 0.6926 - lr: 8.6736e-22\n",
      "Epoch 14/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6236 - accuracy: 0.6627 - val_loss: 0.5923 - val_accuracy: 0.6994 - lr: 8.6736e-22\n",
      "Epoch 15/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6204 - accuracy: 0.6591 - val_loss: 0.5891 - val_accuracy: 0.6946 - lr: 8.6736e-22\n",
      "Epoch 16/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6265 - accuracy: 0.6575 - val_loss: 0.5905 - val_accuracy: 0.6965 - lr: 8.6736e-22\n",
      "Epoch 17/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6226 - accuracy: 0.6611 - val_loss: 0.5924 - val_accuracy: 0.6917 - lr: 8.6736e-22\n",
      "Epoch 18/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6249 - accuracy: 0.6565 - val_loss: 0.5919 - val_accuracy: 0.6950 - lr: 8.6736e-22\n",
      "Epoch 19/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6225 - accuracy: 0.6588 - val_loss: 0.5903 - val_accuracy: 0.6955 - lr: 2.1684e-22\n",
      "Epoch 20/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6246 - accuracy: 0.6600 - val_loss: 0.5911 - val_accuracy: 0.6970 - lr: 2.1684e-22\n",
      "Epoch 21/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6235 - accuracy: 0.6585 - val_loss: 0.5881 - val_accuracy: 0.6960 - lr: 2.1684e-22\n",
      "Epoch 22/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6217 - accuracy: 0.6576 - val_loss: 0.5946 - val_accuracy: 0.7018 - lr: 2.1684e-22\n",
      "Epoch 23/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6229 - accuracy: 0.6619 - val_loss: 0.5909 - val_accuracy: 0.7008 - lr: 2.1684e-22\n",
      "Epoch 24/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6234 - accuracy: 0.6604 - val_loss: 0.5963 - val_accuracy: 0.7018 - lr: 2.1684e-22\n",
      "Epoch 25/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6263 - accuracy: 0.6595 - val_loss: 0.5928 - val_accuracy: 0.6975 - lr: 2.1684e-22\n",
      "Epoch 26/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6223 - accuracy: 0.6644 - val_loss: 0.5922 - val_accuracy: 0.6926 - lr: 2.1684e-22\n",
      "Epoch 27/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6246 - accuracy: 0.6599 - val_loss: 0.5974 - val_accuracy: 0.7027 - lr: 5.4210e-23\n",
      "Epoch 28/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6241 - accuracy: 0.6613 - val_loss: 0.5945 - val_accuracy: 0.7008 - lr: 5.4210e-23\n",
      "Epoch 29/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6214 - accuracy: 0.6632 - val_loss: 0.5897 - val_accuracy: 0.7008 - lr: 5.4210e-23\n",
      "Epoch 30/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6232 - accuracy: 0.6562 - val_loss: 0.5986 - val_accuracy: 0.7008 - lr: 5.4210e-23\n",
      "Epoch 31/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6241 - accuracy: 0.6590 - val_loss: 0.5910 - val_accuracy: 0.6989 - lr: 5.4210e-23\n",
      "Epoch 32/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6196 - accuracy: 0.6625 - val_loss: 0.5988 - val_accuracy: 0.7003 - lr: 1.3553e-23\n",
      "Epoch 33/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6223 - accuracy: 0.6573 - val_loss: 0.5931 - val_accuracy: 0.7042 - lr: 1.3553e-23\n",
      "Epoch 34/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6229 - accuracy: 0.6605 - val_loss: 0.5888 - val_accuracy: 0.6970 - lr: 1.3553e-23\n",
      "Epoch 35/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6240 - accuracy: 0.6574 - val_loss: 0.5966 - val_accuracy: 0.7023 - lr: 1.3553e-23\n",
      "Epoch 36/1000\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.6259 - accuracy: 0.6595 - val_loss: 0.5936 - val_accuracy: 0.6979 - lr: 1.3553e-23\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# Train the model using k-fold cross-validation\n",
    "for train_index, val_index in kfold.split(X):\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    # Scale the data\n",
    "    X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n",
    "    X_val_fold_scaled = scaler.transform(X_val_fold)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_fold_scaled, \n",
    "              y_train_fold, \n",
    "              epochs=1000, \n",
    "              batch_size=16, \n",
    "              validation_data=(X_val_fold_scaled, y_val_fold), \n",
    "              callbacks=[lr_scheduler, early_stopping],  \n",
    "              class_weight=class_weights_dict \n",
    "            )\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scale the validation data with the same scaler used for the training data\n",
    "validation_x = scaler.fit_transform(validation_data.drop(columns=['result', 'label'], axis=1).values)\n",
    "\n",
    "# Run predictions on the validation dataset\n",
    "y_pred = model.predict(validation_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6348773841961853 - precision: 0.5061855670103093 - recall: 0.5168421052631579 - f1 0.5114583333333333\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(validation_data['label'], np.where(y_pred > 0.5, 1, 0).flatten()).ravel()\n",
    "\n",
    "accuracy = (tp+tn) / (tp+tn+fn+fp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2 * (precision*recall)/(precision+recall)\n",
    "\n",
    "print(f'accuracy: {accuracy} - precision: {precision} - recall: {recall} - f1 {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(df, name):\n",
    "    winners = df[df['result'] >= 0]\n",
    "    losers = df[df['result'] < 0]\n",
    "    \n",
    "    return pd.DataFrame.from_dict({\n",
    "        'count': len(df),\n",
    "        'avg': df['result'].mean(),\n",
    "        'winrate': len(winners) / len(df),\n",
    "        'avg_winner': winners['result'].mean(),\n",
    "        'avg_loser': losers['result'].mean(),\n",
    "        'total_win': winners['result'].sum(),\n",
    "        'total_loss':  losers['result'].sum(),\n",
    "        'profit_factor': winners['result'].sum() / -losers['result'].sum()\n",
    "    }, orient='index', columns=[name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>predicted winners</th>\n",
       "      <th>predicted losers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2569.000000</td>\n",
       "      <td>970.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>0.033677</td>\n",
       "      <td>0.120749</td>\n",
       "      <td>-0.019144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winrate</th>\n",
       "      <td>0.384196</td>\n",
       "      <td>0.512371</td>\n",
       "      <td>0.306442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_winner</th>\n",
       "      <td>0.295127</td>\n",
       "      <td>0.342731</td>\n",
       "      <td>0.246843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_loser</th>\n",
       "      <td>-0.129441</td>\n",
       "      <td>-0.112497</td>\n",
       "      <td>-0.136667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_win</th>\n",
       "      <td>291.290120</td>\n",
       "      <td>170.337085</td>\n",
       "      <td>120.953035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_loss</th>\n",
       "      <td>-204.774914</td>\n",
       "      <td>-53.210891</td>\n",
       "      <td>-151.564022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>profit_factor</th>\n",
       "      <td>1.422489</td>\n",
       "      <td>3.201170</td>\n",
       "      <td>0.798033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       all  predicted winners  predicted losers\n",
       "count          2569.000000         970.000000       1599.000000\n",
       "avg               0.033677           0.120749         -0.019144\n",
       "winrate           0.384196           0.512371          0.306442\n",
       "avg_winner        0.295127           0.342731          0.246843\n",
       "avg_loser        -0.129441          -0.112497         -0.136667\n",
       "total_win       291.290120         170.337085        120.953035\n",
       "total_loss     -204.774914         -53.210891       -151.564022\n",
       "profit_factor     1.422489           3.201170          0.798033"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = validation_data[['result', 'label']].copy()\n",
    "res['pred'] = y_pred\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "all_trades = summarize(res, \"all\")\n",
    "pred_winners = summarize(res[res['pred'] > threshold], \"predicted winners\")\n",
    "pred_losers = summarize(res[res['pred'] <= threshold], \"predicted losers\")\n",
    "\n",
    "\n",
    "pd.concat([all_trades, pred_winners, pred_losers,], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3456 - Benchmark innan Kaufman + trade count features\n",
    "```csv\n",
    "stat,all,predicted winners,predicted losers\n",
    "count,2621.000000,1036.000000,1585.000000\n",
    "avg,0.029182,0.112754,-0.025443\n",
    "winrate,0.375048,0.519305,0.280757\n",
    "avg_winner,0.296503,0.317832,0.270716\n",
    "avg_loser,-0.131243,-0.108796,-0.141048\n",
    "total_win,291.461980,170.993371,120.468609\n",
    "total_loss,-214.975741,-54.180457,-160.795284\n",
    "profit_factor,1.355790,3.155997,0.749205\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter( res['pred'], res['result'] +1)\n",
    "ax.set_yscale(\"log\");\n",
    "# ax.set_xscale(\"log\");\n",
    "# All above 1 is winners\n",
    "plt.axhline(y=1, color='r', linestyle='-')\n",
    "# All above threshold is predicted to be winners\n",
    "plt.axvline(x=threshold, color='r', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0: 21', '10: 58', '20: 250', '30: 606', '40: 664', '50: 389', '60: 250', '70: 192', '80: 122', '90: 17']\n",
      "[0.3881372660175167, 0.7755111342273958, 0.6798787285021944, 0.7766147325958876, 0.8919591227882597, 1.5878893994858756, 3.123273338735743, 5.053368037794946, 10.397424076860737, 2057.322297275792]\n"
     ]
    }
   ],
   "source": [
    "def divide(a, b):\n",
    "    return b and a / b or 0\n",
    "\n",
    "def getStats(inrange):\n",
    "    winners = inrange[inrange['result'] >= 0]\n",
    "    losers = inrange[inrange['result'] <= 0]\n",
    "        \n",
    "    d = {\n",
    "        'winrate': divide(len(winners),len(inrange)),\n",
    "        'profit_factor': divide(winners['result'].sum(), -losers['result'].sum())\n",
    "    }\n",
    "    return d\n",
    "\n",
    "percentiles = []\n",
    "percentiles_print = []\n",
    "winrates_above = []\n",
    "profitfactors_above = []\n",
    "winrates_below = []\n",
    "profitfactors_below = []\n",
    "\n",
    "for i in range(10):\n",
    "    in_range =res[res['pred'].between(i/10, (i+1)/10)]\n",
    "    percentiles.append(i*10)\n",
    "    percentiles_print.append(f'{i*10}: {len(in_range)}')\n",
    "\n",
    "    # Probability Above i/10\n",
    "    above = getStats(in_range)\n",
    "    winrates_above.append(above['winrate'])\n",
    "    profitfactors_above.append(above['profit_factor'])\n",
    "\n",
    "    # Probability below i/10\n",
    "    below = getStats(res[res['pred'] < i/10])\n",
    "    winrates_below.append(below['winrate'])\n",
    "    profitfactors_below.append(below['profit_factor'])\n",
    "\n",
    "print(percentiles_print)\n",
    "print(profitfactors_above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.bar( percentiles, winrates_above)\n",
    "plt.axhline(y=0.5, color='r', linestyle=':')\n",
    "plt.xlabel('Predicted Probability Above')\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.bar( percentiles, winrates_below)\n",
    "plt.axhline(y=0.5, color='r', linestyle=':')\n",
    "plt.xlabel('Predicted Probability Below')\n",
    "plt.ylabel('Winrate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar( percentiles, profitfactors_above)\n",
    "plt.axhline(y=1, color='r', linestyle=':')\n",
    "plt.xlabel('Predicted Probability above')\n",
    "plt.ylabel('Profit factor')\n",
    "\n",
    "fig, ax2 = plt.subplots()\n",
    "ax2.bar( percentiles, profitfactors_below)\n",
    "plt.axhline(y=1, color='r', linestyle=':')\n",
    "plt.xlabel('Predicted Probability below')\n",
    "plt.ylabel('Profit factor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6688f5bd7a5c2379fdfde91b010ab5a3ba8033d4f740240b607cd397292295b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
