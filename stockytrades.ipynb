{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "### Operations\n",
    "- Figure out the least amount of data that needs to be provided\n",
    "- Save + load model and compare results\n",
    "- Build API around it\n",
    "\n",
    "### Features\n",
    "- Add feature for days since last signal\n",
    "- Add feature for number of signals last year\n",
    "- Add feature based on output on news model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:\\dev\\stocky-ml\\credentials.json\"\n",
    "\n",
    "# Data:\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dateutil.parser\n",
    "import dateutil.tz as tz\n",
    "from datetime import datetime, timedelta\n",
    "import talib   \n",
    "from talib import MA_Type\n",
    "\n",
    "\n",
    "# Visualization:\n",
    "import seaborn as sns\n",
    "\n",
    "# Database:\n",
    "from google.cloud import firestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create db instance: \n",
    "db = firestore.Client()\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price_data(id):\n",
    "    doc = db.collection('prices').document(id).get().to_dict()\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(doc['priceData'])\n",
    "    df = add_calculated_columns(df)\n",
    "    df = convert_dates(df)\n",
    "\n",
    "    # Read the file to lazily make sure that the dates are strings etc.\n",
    "    # FIXME: Should probably be done some other way.\n",
    "    # output.drop(columns=['Unnamed: 0'], inplace = True)\n",
    "    return df\n",
    "\n",
    "def get_trade_data(doc):\n",
    "    doc = doc.to_dict()\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(doc['trades'])\n",
    "    df['result'] = (df['exitPrice'] / df['entryPrice']) -1\n",
    "    df = convert_trade_dates(df)\n",
    "\n",
    "    return df[['date', 'result']]\n",
    "\n",
    "def convert_date(date, fmt = \"%Y-%m-%d\", target_tz = tz.gettz('CET')):\n",
    "    return date.replace(tzinfo=tz.gettz(\"UTC\")).astimezone(target_tz).strftime(fmt)\n",
    "    \n",
    "def convert_dates(df):\n",
    "    cet = tz.gettz('CET')\n",
    "    for i, row in df.iterrows():\n",
    "        d = convert_date(dateutil.parser.isoparse(row['date']))\n",
    "        df.at[i,'date'] = d\n",
    "    return df\n",
    "\n",
    "def convert_trade_dates(df):\n",
    "    cet = tz.gettz('CET')\n",
    "    for i, row in df.iterrows():\n",
    "        d = convert_date(row['entryDate'] - timedelta(days=1)) # Want one day earlier so that we don't have look ahead\n",
    "        df.at[i,'date'] = d\n",
    "    return df\n",
    "\n",
    "def add_calculated_columns(price):\n",
    "    lookbacks = [20, 50, 100, 200]\n",
    "    values = ['close', 'volume']\n",
    "        \n",
    "    for value in values:\n",
    "        for lookback in lookbacks:\n",
    "            # Get the rolling average and std:\n",
    "            price['average'] = price[value].rolling(lookback).mean()\n",
    "            price['std'] = price[value].rolling(lookback).std()\n",
    "            high = price['high'].rolling(lookback).max()\n",
    "            low = price['low'].rolling(lookback).min()\n",
    "            \n",
    "\n",
    "            # Normalize distance to mean. This could be done with the data above but dont know how.\n",
    "            price[f'zs-{lookback}-{value}'] = (price[value] - price['average']) / price['std']\n",
    "\n",
    "            # Get slope of rolling average and std\n",
    "            price[f'ma-slope-{lookback}-{value}'] = price['average'] / price['average'].shift(1)\n",
    "            price[f'std-slope-{lookback}-{value}'] = price['std'] / price['std'].shift(1)\n",
    "\n",
    "            # Get range\n",
    "            price[f'rng-{lookback}'] = high / low\n",
    "            price[f'percent-rng-{lookback}-{value}'] = (high / low) / price[value]\n",
    "            price[f'percent-std-{lookback}-{value}'] = price['std'] / price[value]\n",
    "\n",
    "            if value == 'volume':\n",
    "                price['temp_volume'] = round(price['volume'] * (price['close'] * 2 + price['open'] * 2 + price['low'] + price['high'])/6)\n",
    "                price[f'avg-log-volume-{lookback}'] = np.log10(price['temp_volume'])  \n",
    "                price.drop(columns=['temp_volume'], inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "            # Drop the actual values since they carry no interest:\n",
    "            price.drop(columns=['average', 'std'], inplace = True)\n",
    "            \n",
    "        # TODO: Add calculations for volume\n",
    "        # TODO: Add calculations for owners\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_index_stock_df(omxdf, stockdf):\n",
    "  df = stockdf.copy()\n",
    "  df['omx_close'] = omxdf['close']\n",
    "  df['stock_close'] = stockdf['close']\n",
    "\n",
    "  df = df[df['stock_close'].notna()]\n",
    "  df['volume'] = stockdf['volume']\n",
    "\n",
    "\n",
    "  df['stock_zs-50-close'] = stockdf['zs-50-close']\n",
    "  df['stock_zs-100-close'] = stockdf['zs-100-close']\n",
    "  df['stock_zs-200-close'] = stockdf['zs-200-close']\n",
    "  df['stock_quota'] = df['stock_close']/df['omx_close']\n",
    "\n",
    "\n",
    "  df['stock_hist_relative_perf20'] = df['stock_quota'].shift(20) / df['stock_quota']\n",
    "  df['stock_hist_relative_perf50'] = df['stock_quota'].shift(50) / df['stock_quota']\n",
    "  df['stock_hist_relative_perf100'] = df['stock_quota'].shift(100) / df['stock_quota']\n",
    "\n",
    "  df['stock_hist_perf20'] = df['stock_close'].shift(20) / df['stock_close']\n",
    "  df['stock_hist_perf50'] = df['stock_close'].shift(50) / df['stock_close']\n",
    "  df['stock_hist_perf100'] = df['stock_close'].shift(100) / df['stock_close']\n",
    "\n",
    "  for p in [8, 20, 34, 50, 100]:\n",
    "    df[f'stock_relative_rsi_{p}'] = talib.RSI(df['stock_quota'], timeperiod=p) /100\n",
    "    df[f'stock_rsi_{p}'] = talib.RSI(df['stock_close'], timeperiod=p) / 100\n",
    "\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the omx price data\n",
    "omxdf = get_price_data('19002')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>volume_stock</th>\n",
       "      <th>high_stock</th>\n",
       "      <th>low_stock</th>\n",
       "      <th>owners_stock</th>\n",
       "      <th>close_stock</th>\n",
       "      <th>open_stock</th>\n",
       "      <th>zs-20-close_stock</th>\n",
       "      <th>ma-slope-20-close_stock</th>\n",
       "      <th>std-slope-20-close_stock</th>\n",
       "      <th>rng-20_stock</th>\n",
       "      <th>percent-rng-20-close_stock</th>\n",
       "      <th>percent-std-20-close_stock</th>\n",
       "      <th>zs-50-close_stock</th>\n",
       "      <th>ma-slope-50-close_stock</th>\n",
       "      <th>std-slope-50-close_stock</th>\n",
       "      <th>rng-50_stock</th>\n",
       "      <th>percent-rng-50-close_stock</th>\n",
       "      <th>percent-std-50-close_stock</th>\n",
       "      <th>zs-100-close_stock</th>\n",
       "      <th>ma-slope-100-close_stock</th>\n",
       "      <th>std-slope-100-close_stock</th>\n",
       "      <th>rng-100_stock</th>\n",
       "      <th>percent-rng-100-close_stock</th>\n",
       "      <th>...</th>\n",
       "      <th>zs-20-volume_omx</th>\n",
       "      <th>ma-slope-20-volume_omx</th>\n",
       "      <th>std-slope-20-volume_omx</th>\n",
       "      <th>percent-rng-20-volume_omx</th>\n",
       "      <th>percent-std-20-volume_omx</th>\n",
       "      <th>avg-log-volume-20_omx</th>\n",
       "      <th>zs-50-volume_omx</th>\n",
       "      <th>ma-slope-50-volume_omx</th>\n",
       "      <th>std-slope-50-volume_omx</th>\n",
       "      <th>percent-rng-50-volume_omx</th>\n",
       "      <th>percent-std-50-volume_omx</th>\n",
       "      <th>avg-log-volume-50_omx</th>\n",
       "      <th>zs-100-volume_omx</th>\n",
       "      <th>ma-slope-100-volume_omx</th>\n",
       "      <th>std-slope-100-volume_omx</th>\n",
       "      <th>percent-rng-100-volume_omx</th>\n",
       "      <th>percent-std-100-volume_omx</th>\n",
       "      <th>avg-log-volume-100_omx</th>\n",
       "      <th>zs-200-volume_omx</th>\n",
       "      <th>ma-slope-200-volume_omx</th>\n",
       "      <th>std-slope-200-volume_omx</th>\n",
       "      <th>percent-rng-200-volume_omx</th>\n",
       "      <th>percent-std-200-volume_omx</th>\n",
       "      <th>avg-log-volume-200_omx</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>937763.0</td>\n",
       "      <td>105.45</td>\n",
       "      <td>100.05</td>\n",
       "      <td>16786.0</td>\n",
       "      <td>104.80</td>\n",
       "      <td>100.55</td>\n",
       "      <td>2.406278</td>\n",
       "      <td>1.008411</td>\n",
       "      <td>1.206170</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>0.011768</td>\n",
       "      <td>0.049977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.334595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-04</td>\n",
       "      <td>2167584.0</td>\n",
       "      <td>170.55</td>\n",
       "      <td>160.45</td>\n",
       "      <td>18567.0</td>\n",
       "      <td>169.90</td>\n",
       "      <td>160.75</td>\n",
       "      <td>2.570386</td>\n",
       "      <td>1.012324</td>\n",
       "      <td>1.023911</td>\n",
       "      <td>1.277528</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.041544</td>\n",
       "      <td>2.138433</td>\n",
       "      <td>1.010411</td>\n",
       "      <td>1.007476</td>\n",
       "      <td>1.849783</td>\n",
       "      <td>0.010887</td>\n",
       "      <td>0.097403</td>\n",
       "      <td>2.087838</td>\n",
       "      <td>1.003921</td>\n",
       "      <td>1.019671</td>\n",
       "      <td>2.210915</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156139</td>\n",
       "      <td>1.009669</td>\n",
       "      <td>0.995888</td>\n",
       "      <td>1.114351e-08</td>\n",
       "      <td>0.302561</td>\n",
       "      <td>11.251926</td>\n",
       "      <td>-0.240291</td>\n",
       "      <td>0.983645</td>\n",
       "      <td>0.935139</td>\n",
       "      <td>1.267716e-08</td>\n",
       "      <td>0.303904</td>\n",
       "      <td>11.251926</td>\n",
       "      <td>-0.256563</td>\n",
       "      <td>0.997336</td>\n",
       "      <td>0.999528</td>\n",
       "      <td>1.438877e-08</td>\n",
       "      <td>0.450774</td>\n",
       "      <td>11.251926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.438877e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.251926</td>\n",
       "      <td>-0.134604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-07-06</td>\n",
       "      <td>908268.0</td>\n",
       "      <td>184.65</td>\n",
       "      <td>179.55</td>\n",
       "      <td>18595.0</td>\n",
       "      <td>184.45</td>\n",
       "      <td>180.00</td>\n",
       "      <td>2.339982</td>\n",
       "      <td>1.000686</td>\n",
       "      <td>1.031583</td>\n",
       "      <td>1.262564</td>\n",
       "      <td>0.006845</td>\n",
       "      <td>0.047219</td>\n",
       "      <td>2.121880</td>\n",
       "      <td>1.007725</td>\n",
       "      <td>1.012728</td>\n",
       "      <td>1.526664</td>\n",
       "      <td>0.008277</td>\n",
       "      <td>0.079306</td>\n",
       "      <td>1.999881</td>\n",
       "      <td>1.003812</td>\n",
       "      <td>1.019996</td>\n",
       "      <td>2.393700</td>\n",
       "      <td>0.012977</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.021078</td>\n",
       "      <td>0.980999</td>\n",
       "      <td>1.023035</td>\n",
       "      <td>1.436401e-08</td>\n",
       "      <td>0.324498</td>\n",
       "      <td>11.121916</td>\n",
       "      <td>-0.961179</td>\n",
       "      <td>0.993908</td>\n",
       "      <td>1.009332</td>\n",
       "      <td>1.544756e-08</td>\n",
       "      <td>0.344210</td>\n",
       "      <td>11.121916</td>\n",
       "      <td>-0.920364</td>\n",
       "      <td>0.999753</td>\n",
       "      <td>1.000571</td>\n",
       "      <td>1.968961e-08</td>\n",
       "      <td>0.605698</td>\n",
       "      <td>11.121916</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.968961e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.121916</td>\n",
       "      <td>-0.067935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-10-14</td>\n",
       "      <td>1479449.0</td>\n",
       "      <td>191.50</td>\n",
       "      <td>178.10</td>\n",
       "      <td>20927.0</td>\n",
       "      <td>188.90</td>\n",
       "      <td>178.10</td>\n",
       "      <td>1.817743</td>\n",
       "      <td>1.009085</td>\n",
       "      <td>1.050660</td>\n",
       "      <td>1.272425</td>\n",
       "      <td>0.006736</td>\n",
       "      <td>0.050375</td>\n",
       "      <td>0.618434</td>\n",
       "      <td>0.997120</td>\n",
       "      <td>0.968908</td>\n",
       "      <td>1.505648</td>\n",
       "      <td>0.007971</td>\n",
       "      <td>0.105323</td>\n",
       "      <td>0.539262</td>\n",
       "      <td>1.002220</td>\n",
       "      <td>0.992524</td>\n",
       "      <td>1.586279</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.291895</td>\n",
       "      <td>0.990344</td>\n",
       "      <td>0.996820</td>\n",
       "      <td>1.444031e-08</td>\n",
       "      <td>0.270197</td>\n",
       "      <td>11.128150</td>\n",
       "      <td>0.064049</td>\n",
       "      <td>1.004745</td>\n",
       "      <td>0.991875</td>\n",
       "      <td>1.470981e-08</td>\n",
       "      <td>0.238545</td>\n",
       "      <td>11.128150</td>\n",
       "      <td>-0.350554</td>\n",
       "      <td>0.997029</td>\n",
       "      <td>0.999035</td>\n",
       "      <td>1.619484e-08</td>\n",
       "      <td>0.365987</td>\n",
       "      <td>11.128150</td>\n",
       "      <td>-0.602651</td>\n",
       "      <td>1.00146</td>\n",
       "      <td>0.996767</td>\n",
       "      <td>2.069980e-08</td>\n",
       "      <td>0.572475</td>\n",
       "      <td>11.128150</td>\n",
       "      <td>0.282463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>659684.0</td>\n",
       "      <td>290.60</td>\n",
       "      <td>274.90</td>\n",
       "      <td>28485.0</td>\n",
       "      <td>283.70</td>\n",
       "      <td>276.40</td>\n",
       "      <td>2.180677</td>\n",
       "      <td>1.006883</td>\n",
       "      <td>1.156869</td>\n",
       "      <td>1.243475</td>\n",
       "      <td>0.004383</td>\n",
       "      <td>0.044750</td>\n",
       "      <td>1.856338</td>\n",
       "      <td>1.005233</td>\n",
       "      <td>0.983717</td>\n",
       "      <td>1.338377</td>\n",
       "      <td>0.004718</td>\n",
       "      <td>0.055730</td>\n",
       "      <td>1.829468</td>\n",
       "      <td>1.004957</td>\n",
       "      <td>1.003570</td>\n",
       "      <td>1.709517</td>\n",
       "      <td>0.006026</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.927763</td>\n",
       "      <td>1.000411</td>\n",
       "      <td>0.996812</td>\n",
       "      <td>1.471851e-08</td>\n",
       "      <td>0.147023</td>\n",
       "      <td>11.207166</td>\n",
       "      <td>-0.554525</td>\n",
       "      <td>1.000018</td>\n",
       "      <td>0.999947</td>\n",
       "      <td>1.555449e-08</td>\n",
       "      <td>0.216722</td>\n",
       "      <td>11.207166</td>\n",
       "      <td>-0.387574</td>\n",
       "      <td>0.999153</td>\n",
       "      <td>1.000751</td>\n",
       "      <td>1.727156e-08</td>\n",
       "      <td>0.280320</td>\n",
       "      <td>11.207166</td>\n",
       "      <td>-0.307612</td>\n",
       "      <td>0.99760</td>\n",
       "      <td>0.994528</td>\n",
       "      <td>1.848767e-08</td>\n",
       "      <td>0.286702</td>\n",
       "      <td>11.207166</td>\n",
       "      <td>0.297843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        date  volume_stock  high_stock  low_stock  owners_stock  close_stock  open_stock  zs-20-close_stock  ma-slope-20-close_stock  std-slope-20-close_stock  rng-20_stock  percent-rng-20-close_stock  percent-std-20-close_stock  zs-50-close_stock  ma-slope-50-close_stock  std-slope-50-close_stock  rng-50_stock  percent-rng-50-close_stock  percent-std-50-close_stock  zs-100-close_stock  ma-slope-100-close_stock  std-slope-100-close_stock  rng-100_stock  percent-rng-100-close_stock  ...  zs-20-volume_omx  ma-slope-20-volume_omx  std-slope-20-volume_omx  percent-rng-20-volume_omx  percent-std-20-volume_omx  avg-log-volume-20_omx  zs-50-volume_omx  ma-slope-50-volume_omx  std-slope-50-volume_omx  percent-rng-50-volume_omx  percent-std-50-volume_omx  avg-log-volume-50_omx  zs-100-volume_omx  ma-slope-100-volume_omx  std-slope-100-volume_omx  percent-rng-100-volume_omx  percent-std-100-volume_omx  avg-log-volume-100_omx  zs-200-volume_omx  ma-slope-200-volume_omx  \\\n",
       "0           0  2019-11-11      937763.0      105.45     100.05       16786.0       104.80      100.55           2.406278                 1.008411                  1.206170      1.233333                    0.011768                    0.049977                NaN                      NaN                       NaN           NaN                         NaN                         NaN                 NaN                       NaN                        NaN            NaN                          NaN  ...               NaN                     NaN                      NaN                        NaN                        NaN                    NaN               NaN                     NaN                      NaN                        NaN                        NaN                    NaN                NaN                      NaN                       NaN                         NaN                         NaN                     NaN                NaN                      NaN   \n",
       "1           1  2020-06-04     2167584.0      170.55     160.45       18567.0       169.90      160.75           2.570386                 1.012324                  1.023911      1.277528                    0.007519                    0.041544           2.138433                 1.010411                  1.007476      1.849783                    0.010887                    0.097403            2.087838                  1.003921                   1.019671       2.210915                     0.013013  ...          0.156139                1.009669                 0.995888               1.114351e-08                   0.302561              11.251926         -0.240291                0.983645                 0.935139               1.267716e-08                   0.303904              11.251926          -0.256563                 0.997336                  0.999528                1.438877e-08                    0.450774               11.251926                NaN                      NaN   \n",
       "2           2  2020-07-06      908268.0      184.65     179.55       18595.0       184.45      180.00           2.339982                 1.000686                  1.031583      1.262564                    0.006845                    0.047219           2.121880                 1.007725                  1.012728      1.526664                    0.008277                    0.079306            1.999881                  1.003812                   1.019996       2.393700                     0.012977  ...         -1.021078                0.980999                 1.023035               1.436401e-08                   0.324498              11.121916         -0.961179                0.993908                 1.009332               1.544756e-08                   0.344210              11.121916          -0.920364                 0.999753                  1.000571                1.968961e-08                    0.605698               11.121916                NaN                      NaN   \n",
       "3           3  2020-10-14     1479449.0      191.50     178.10       20927.0       188.90      178.10           1.817743                 1.009085                  1.050660      1.272425                    0.006736                    0.050375           0.618434                 0.997120                  0.968908      1.505648                    0.007971                    0.105323            0.539262                  1.002220                   0.992524       1.586279                     0.008397  ...         -0.291895                0.990344                 0.996820               1.444031e-08                   0.270197              11.128150          0.064049                1.004745                 0.991875               1.470981e-08                   0.238545              11.128150          -0.350554                 0.997029                  0.999035                1.619484e-08                    0.365987               11.128150          -0.602651                  1.00146   \n",
       "4           4  2021-03-30      659684.0      290.60     274.90       28485.0       283.70      276.40           2.180677                 1.006883                  1.156869      1.243475                    0.004383                    0.044750           1.856338                 1.005233                  0.983717      1.338377                    0.004718                    0.055730            1.829468                  1.004957                   1.003570       1.709517                     0.006026  ...         -0.927763                1.000411                 0.996812               1.471851e-08                   0.147023              11.207166         -0.554525                1.000018                 0.999947               1.555449e-08                   0.216722              11.207166          -0.387574                 0.999153                  1.000751                1.727156e-08                    0.280320               11.207166          -0.307612                  0.99760   \n",
       "\n",
       "   std-slope-200-volume_omx  percent-rng-200-volume_omx  percent-std-200-volume_omx  avg-log-volume-200_omx    result  \n",
       "0                       NaN                         NaN                         NaN                     NaN  0.334595  \n",
       "1                       NaN                1.438877e-08                         NaN               11.251926 -0.134604  \n",
       "2                       NaN                1.968961e-08                         NaN               11.121916 -0.067935  \n",
       "3                  0.996767                2.069980e-08                    0.572475               11.128150  0.282463  \n",
       "4                  0.994528                1.848767e-08                    0.286702               11.207166  0.297843  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "try: \n",
    "  df = pd.read_csv('stockytrades.csv')\n",
    "except:\n",
    "  print('Failed to read file')\n",
    "  docs = db.collection('trades').stream()\n",
    "\n",
    "  for doc in docs:\n",
    "    print('starting', doc.id)\n",
    "    stockdf = get_price_data(doc.id)\n",
    "    tradedf =  get_trade_data(doc)\n",
    "    merged_df = pd.merge(stockdf, omxdf, on='date', suffixes=('_stock', '_omx'))\n",
    "\n",
    "    \n",
    "    merged_df = pd.merge(merged_df, tradedf, on='date', suffixes=('_1', '_2'))\n",
    "    merged_df.drop(columns=['owners'], inplace = True, errors='ignore')\n",
    "    \n",
    "    df = pd.concat([df, merged_df], ignore_index=True)\n",
    "  \n",
    "  # Save the file so we dont have to next time\n",
    "  df.to_csv('stockytrades.csv')\n",
    "\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20323"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop problematic columns.\n",
    "# TODO: See if you can improve the data quality to be able to use more of these\n",
    "df.drop(columns=[\n",
    "                'owners_stock', \n",
    "                'volume_omx',\n",
    "                'owners_omx',\n",
    "                'zs-20-volume_omx',\n",
    "                'ma-slope-20-volume_omx',\n",
    "                'std-slope-20-volume_omx',\n",
    "                'percent-rng-20-volume_omx',\n",
    "                'percent-std-20-volume_omx',\n",
    "                'avg-log-volume-20_omx',\n",
    "                'zs-50-volume_omx',\n",
    "                'ma-slope-50-volume_omx',\n",
    "                'std-slope-50-volume_omx',\n",
    "                'percent-rng-50-volume_omx',\n",
    "                'percent-std-50-volume_omx',\n",
    "                'avg-log-volume-50_omx',\n",
    "                'zs-100-volume_omx',\n",
    "                'ma-slope-100-volume_omx',\n",
    "                'std-slope-100-volume_omx',\n",
    "                'percent-rng-100-volume_omx',\n",
    "                'percent-std-100-volume_omx',\n",
    "                'avg-log-volume-100_omx',\n",
    "                'zs-200-volume_omx',\n",
    "                'ma-slope-200-volume_omx',\n",
    "                'std-slope-200-volume_omx',\n",
    "                'percent-rng-200-volume_omx',\n",
    "                'percent-std-200-volume_omx',\n",
    "                'avg-log-volume-200_omx',\n",
    "                'date',\n",
    "                'zs-200-volume_stock',            \n",
    "                'ma-slope-200-volume_stock',      \n",
    "                'std-slope-200-volume_stock',     \n",
    "                'percent-rng-200-volume_stock',  \n",
    "                'percent-std-200-volume_stock',   \n",
    "                'zs-100-volume_stock',            \n",
    "                'ma-slope-100-volume_stock',      \n",
    "                'std-slope-100-volume_stock',     \n",
    "                'percent-rng-100-volume_stock',   \n",
    "                'percent-std-100-volume_stock',   \n",
    "                ], inplace=True, errors='ignore')\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                        0\n",
       "volume_stock                     21\n",
       "high_stock                        0\n",
       "low_stock                         0\n",
       "close_stock                       0\n",
       "open_stock                        0\n",
       "zs-20-close_stock               351\n",
       "ma-slope-20-close_stock         361\n",
       "std-slope-20-close_stock        370\n",
       "rng-20_stock                    351\n",
       "percent-rng-20-close_stock      351\n",
       "percent-std-20-close_stock      351\n",
       "zs-50-close_stock               724\n",
       "ma-slope-50-close_stock         736\n",
       "std-slope-50-close_stock        736\n",
       "rng-50_stock                    724\n",
       "percent-rng-50-close_stock      724\n",
       "percent-std-50-close_stock      724\n",
       "zs-100-close_stock             1259\n",
       "ma-slope-100-close_stock       1271\n",
       "std-slope-100-close_stock      1271\n",
       "rng-100_stock                  1259\n",
       "percent-rng-100-close_stock    1259\n",
       "percent-std-100-close_stock    1259\n",
       "zs-200-close_stock             2321\n",
       "ma-slope-200-close_stock       2331\n",
       "std-slope-200-close_stock      2332\n",
       "rng-200_stock                  2321\n",
       "percent-rng-200-close_stock    2321\n",
       "percent-std-200-close_stock    2321\n",
       "zs-20-volume_stock              734\n",
       "ma-slope-20-volume_stock        749\n",
       "std-slope-20-volume_stock       750\n",
       "percent-rng-20-volume_stock     372\n",
       "percent-std-20-volume_stock     733\n",
       "avg-log-volume-20_stock          21\n",
       "zs-50-volume_stock             1343\n",
       "ma-slope-50-volume_stock       1366\n",
       "std-slope-50-volume_stock      1366\n",
       "percent-rng-50-volume_stock     745\n",
       "percent-std-50-volume_stock    1343\n",
       "avg-log-volume-50_stock          21\n",
       "avg-log-volume-100_stock         21\n",
       "avg-log-volume-200_stock         21\n",
       "high_omx                          0\n",
       "low_omx                           0\n",
       "close_omx                         0\n",
       "open_omx                          0\n",
       "zs-20-close_omx                   0\n",
       "ma-slope-20-close_omx             0\n",
       "std-slope-20-close_omx            0\n",
       "rng-20_omx                        0\n",
       "percent-rng-20-close_omx          0\n",
       "percent-std-20-close_omx          0\n",
       "zs-50-close_omx                   0\n",
       "ma-slope-50-close_omx             0\n",
       "std-slope-50-close_omx            0\n",
       "rng-50_omx                        0\n",
       "percent-rng-50-close_omx          0\n",
       "percent-std-50-close_omx          0\n",
       "zs-100-close_omx                  0\n",
       "ma-slope-100-close_omx            0\n",
       "std-slope-100-close_omx           0\n",
       "rng-100_omx                       0\n",
       "percent-rng-100-close_omx         0\n",
       "percent-std-100-close_omx         0\n",
       "zs-200-close_omx                  0\n",
       "ma-slope-200-close_omx            0\n",
       "std-slope-200-close_omx           0\n",
       "rng-200_omx                       0\n",
       "percent-rng-200-close_omx         0\n",
       "percent-std-200-close_omx         0\n",
       "result                            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trades: 17403\n"
     ]
    }
   ],
   "source": [
    "# Clean the data that cannot be used for training\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "print(\"Total number of trades:\", len(df))\n",
    "\n",
    "\n",
    "# Add the label we want to predict\n",
    "df['label'] = np.where(df['result'] > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.036230354368728006 89.56603773584905\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 3.6230354368728004%\n",
      "Best 8956.603773584906%\n",
      "Worst -98.48192771084338%\n",
      "std 78.43640288073676%\n"
     ]
    }
   ],
   "source": [
    "#plot distribution of points by team \n",
    "avg = df['result'].mean()\n",
    "best = df['result'].max()\n",
    "worst = df['result'].min()\n",
    "std = df['result'].std()\n",
    "\n",
    "print(avg, best)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "stats = ['Average', 'Best', 'Worst', 'std']\n",
    "columns = [avg, best, worst, std]\n",
    "ax.bar(stats, columns)\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(stats)):\n",
    "  print(stats[i], f'{columns[i]*100}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning part\n",
    "\n",
    "- Drop all `NaN` values first. \n",
    "- Then split the dataset for test and training (using K-fold?). \n",
    "- Train the model\n",
    "- Create a confusion matrix on the validation data. Compare results with reality\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "train_data, validation_data =  train_test_split(df, test_size=0.15, random_state=42) \n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = train_data.drop(columns=['result', 'label'], axis=1).values\n",
    "y = train_data['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold\n",
    "num_folds = 5\n",
    "# Define the k-fold cross-validator\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize input data\n",
    "scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define TensorFlow model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X.shape[1],)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    # tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bauhn\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0 1], y=[1 1 0 ... 0 0 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
    "\n",
    "# Define learning rate scheduler callback\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', patience=10, factor=0.5)\n",
    "\n",
    "# Calculate class weights based on occurrence since the base strategy loses more often we want to equalize the weights\n",
    "class_weights = compute_class_weight('balanced', np.unique(y), y)\n",
    "\n",
    "# Convert class weights to a dictionary\n",
    "class_weights_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "370/370 [==============================] - 2s 2ms/step - loss: 0.7298 - accuracy: 0.5393 - val_loss: 0.6832 - val_accuracy: 0.5772 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6827 - accuracy: 0.5735 - val_loss: 0.6699 - val_accuracy: 0.5945 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6767 - accuracy: 0.5857 - val_loss: 0.6640 - val_accuracy: 0.6134 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6701 - accuracy: 0.6019 - val_loss: 0.6689 - val_accuracy: 0.5992 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6670 - accuracy: 0.6091 - val_loss: 0.6612 - val_accuracy: 0.6171 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6627 - accuracy: 0.6130 - val_loss: 0.6667 - val_accuracy: 0.6103 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6585 - accuracy: 0.6238 - val_loss: 0.6561 - val_accuracy: 0.6228 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6587 - accuracy: 0.6263 - val_loss: 0.6534 - val_accuracy: 0.6239 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6589 - accuracy: 0.6208 - val_loss: 0.6481 - val_accuracy: 0.6448 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6560 - accuracy: 0.6344 - val_loss: 0.6597 - val_accuracy: 0.6330 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6547 - accuracy: 0.6301 - val_loss: 0.6525 - val_accuracy: 0.6326 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6548 - accuracy: 0.6337 - val_loss: 0.6551 - val_accuracy: 0.6347 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6520 - accuracy: 0.6391 - val_loss: 0.6547 - val_accuracy: 0.6266 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6525 - accuracy: 0.6345 - val_loss: 0.6545 - val_accuracy: 0.6357 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6489 - accuracy: 0.6393 - val_loss: 0.6549 - val_accuracy: 0.6320 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6516 - accuracy: 0.6388 - val_loss: 0.6484 - val_accuracy: 0.6326 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6465 - accuracy: 0.6451 - val_loss: 0.6490 - val_accuracy: 0.6428 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6453 - accuracy: 0.6484 - val_loss: 0.6485 - val_accuracy: 0.6377 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6440 - accuracy: 0.6500 - val_loss: 0.6579 - val_accuracy: 0.6232 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6407 - accuracy: 0.6457 - val_loss: 0.6415 - val_accuracy: 0.6455 - lr: 5.0000e-04\n",
      "Epoch 21/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6378 - accuracy: 0.6562 - val_loss: 0.6453 - val_accuracy: 0.6482 - lr: 5.0000e-04\n",
      "Epoch 22/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6360 - accuracy: 0.6556 - val_loss: 0.6402 - val_accuracy: 0.6563 - lr: 5.0000e-04\n",
      "Epoch 23/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6353 - accuracy: 0.6568 - val_loss: 0.6452 - val_accuracy: 0.6465 - lr: 5.0000e-04\n",
      "Epoch 24/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6315 - accuracy: 0.6555 - val_loss: 0.6414 - val_accuracy: 0.6516 - lr: 5.0000e-04\n",
      "Epoch 25/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6296 - accuracy: 0.6614 - val_loss: 0.6450 - val_accuracy: 0.6438 - lr: 5.0000e-04\n",
      "Epoch 26/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6311 - accuracy: 0.6596 - val_loss: 0.6439 - val_accuracy: 0.6418 - lr: 5.0000e-04\n",
      "Epoch 27/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6315 - accuracy: 0.6531 - val_loss: 0.6446 - val_accuracy: 0.6381 - lr: 5.0000e-04\n",
      "Epoch 28/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6286 - accuracy: 0.6566 - val_loss: 0.6429 - val_accuracy: 0.6472 - lr: 5.0000e-04\n",
      "Epoch 29/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6304 - accuracy: 0.6593 - val_loss: 0.6456 - val_accuracy: 0.6360 - lr: 5.0000e-04\n",
      "Epoch 30/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6260 - accuracy: 0.6636 - val_loss: 0.6417 - val_accuracy: 0.6353 - lr: 5.0000e-04\n",
      "Epoch 31/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6254 - accuracy: 0.6630 - val_loss: 0.6402 - val_accuracy: 0.6428 - lr: 5.0000e-04\n",
      "Epoch 32/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6234 - accuracy: 0.6653 - val_loss: 0.6408 - val_accuracy: 0.6350 - lr: 5.0000e-04\n",
      "Epoch 33/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6191 - accuracy: 0.6632 - val_loss: 0.6440 - val_accuracy: 0.6306 - lr: 2.5000e-04\n",
      "Epoch 34/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6168 - accuracy: 0.6636 - val_loss: 0.6462 - val_accuracy: 0.6269 - lr: 2.5000e-04\n",
      "Epoch 35/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6193 - accuracy: 0.6648 - val_loss: 0.6436 - val_accuracy: 0.6347 - lr: 2.5000e-04\n",
      "Epoch 36/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6167 - accuracy: 0.6706 - val_loss: 0.6417 - val_accuracy: 0.6381 - lr: 2.5000e-04\n",
      "Epoch 37/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6136 - accuracy: 0.6718 - val_loss: 0.6457 - val_accuracy: 0.6343 - lr: 2.5000e-04\n",
      "Epoch 38/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6170 - accuracy: 0.6616 - val_loss: 0.6453 - val_accuracy: 0.6310 - lr: 2.5000e-04\n",
      "Epoch 39/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6123 - accuracy: 0.6686 - val_loss: 0.6479 - val_accuracy: 0.6313 - lr: 2.5000e-04\n",
      "Epoch 40/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6135 - accuracy: 0.6680 - val_loss: 0.6490 - val_accuracy: 0.6249 - lr: 2.5000e-04\n",
      "Epoch 41/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6102 - accuracy: 0.6724 - val_loss: 0.6440 - val_accuracy: 0.6316 - lr: 2.5000e-04\n",
      "Epoch 42/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6094 - accuracy: 0.6758 - val_loss: 0.6468 - val_accuracy: 0.6326 - lr: 2.5000e-04\n",
      "Epoch 43/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6147 - accuracy: 0.6653 - val_loss: 0.6452 - val_accuracy: 0.6347 - lr: 1.2500e-04\n",
      "Epoch 44/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6101 - accuracy: 0.6760 - val_loss: 0.6459 - val_accuracy: 0.6330 - lr: 1.2500e-04\n",
      "Epoch 45/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6093 - accuracy: 0.6705 - val_loss: 0.6459 - val_accuracy: 0.6306 - lr: 1.2500e-04\n",
      "Epoch 46/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6059 - accuracy: 0.6773 - val_loss: 0.6457 - val_accuracy: 0.6337 - lr: 1.2500e-04\n",
      "Epoch 47/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6096 - accuracy: 0.6718 - val_loss: 0.6471 - val_accuracy: 0.6326 - lr: 1.2500e-04\n",
      "Epoch 48/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6064 - accuracy: 0.6789 - val_loss: 0.6453 - val_accuracy: 0.6333 - lr: 1.2500e-04\n",
      "Epoch 49/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6061 - accuracy: 0.6840 - val_loss: 0.6454 - val_accuracy: 0.6353 - lr: 1.2500e-04\n",
      "Epoch 50/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6065 - accuracy: 0.6793 - val_loss: 0.6455 - val_accuracy: 0.6353 - lr: 1.2500e-04\n",
      "Epoch 51/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6074 - accuracy: 0.6739 - val_loss: 0.6469 - val_accuracy: 0.6350 - lr: 1.2500e-04\n",
      "Epoch 52/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6041 - accuracy: 0.6761 - val_loss: 0.6465 - val_accuracy: 0.6337 - lr: 1.2500e-04\n",
      "Epoch 1/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6280 - accuracy: 0.6591 - val_loss: 0.5812 - val_accuracy: 0.7117 - lr: 6.2500e-05\n",
      "Epoch 2/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6199 - accuracy: 0.6666 - val_loss: 0.5856 - val_accuracy: 0.7077 - lr: 6.2500e-05\n",
      "Epoch 3/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6271 - accuracy: 0.6593 - val_loss: 0.5884 - val_accuracy: 0.7029 - lr: 6.2500e-05\n",
      "Epoch 4/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6190 - accuracy: 0.6658 - val_loss: 0.5883 - val_accuracy: 0.7094 - lr: 6.2500e-05\n",
      "Epoch 5/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6189 - accuracy: 0.6636 - val_loss: 0.5890 - val_accuracy: 0.6982 - lr: 6.2500e-05\n",
      "Epoch 6/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6211 - accuracy: 0.6654 - val_loss: 0.5929 - val_accuracy: 0.7029 - lr: 6.2500e-05\n",
      "Epoch 7/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6183 - accuracy: 0.6686 - val_loss: 0.5928 - val_accuracy: 0.7002 - lr: 6.2500e-05\n",
      "Epoch 8/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6207 - accuracy: 0.6690 - val_loss: 0.5892 - val_accuracy: 0.7023 - lr: 6.2500e-05\n",
      "Epoch 9/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6167 - accuracy: 0.6704 - val_loss: 0.5936 - val_accuracy: 0.6992 - lr: 6.2500e-05\n",
      "Epoch 10/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6200 - accuracy: 0.6653 - val_loss: 0.5923 - val_accuracy: 0.7080 - lr: 6.2500e-05\n",
      "Epoch 11/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6216 - accuracy: 0.6664 - val_loss: 0.5912 - val_accuracy: 0.7043 - lr: 6.2500e-05\n",
      "Epoch 12/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6190 - accuracy: 0.6628 - val_loss: 0.5921 - val_accuracy: 0.7090 - lr: 3.1250e-05\n",
      "Epoch 13/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6178 - accuracy: 0.6695 - val_loss: 0.5923 - val_accuracy: 0.7053 - lr: 3.1250e-05\n",
      "Epoch 14/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6185 - accuracy: 0.6685 - val_loss: 0.5934 - val_accuracy: 0.7073 - lr: 3.1250e-05\n",
      "Epoch 15/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6181 - accuracy: 0.6685 - val_loss: 0.5928 - val_accuracy: 0.7077 - lr: 3.1250e-05\n",
      "Epoch 16/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6210 - accuracy: 0.6643 - val_loss: 0.5922 - val_accuracy: 0.7036 - lr: 3.1250e-05\n",
      "Epoch 17/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6171 - accuracy: 0.6695 - val_loss: 0.5911 - val_accuracy: 0.7063 - lr: 3.1250e-05\n",
      "Epoch 18/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6185 - accuracy: 0.6724 - val_loss: 0.5938 - val_accuracy: 0.7029 - lr: 3.1250e-05\n",
      "Epoch 19/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6134 - accuracy: 0.6719 - val_loss: 0.5939 - val_accuracy: 0.7016 - lr: 3.1250e-05\n",
      "Epoch 20/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6147 - accuracy: 0.6661 - val_loss: 0.5910 - val_accuracy: 0.7040 - lr: 3.1250e-05\n",
      "Epoch 21/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6136 - accuracy: 0.6718 - val_loss: 0.5939 - val_accuracy: 0.7013 - lr: 3.1250e-05\n",
      "Epoch 22/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6139 - accuracy: 0.6766 - val_loss: 0.5947 - val_accuracy: 0.7016 - lr: 1.5625e-05\n",
      "Epoch 23/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6134 - accuracy: 0.6723 - val_loss: 0.5948 - val_accuracy: 0.7043 - lr: 1.5625e-05\n",
      "Epoch 24/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6121 - accuracy: 0.6757 - val_loss: 0.5962 - val_accuracy: 0.7016 - lr: 1.5625e-05\n",
      "Epoch 25/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6157 - accuracy: 0.6733 - val_loss: 0.5943 - val_accuracy: 0.6999 - lr: 1.5625e-05\n",
      "Epoch 26/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6157 - accuracy: 0.6686 - val_loss: 0.5926 - val_accuracy: 0.7080 - lr: 1.5625e-05\n",
      "Epoch 27/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6142 - accuracy: 0.6743 - val_loss: 0.5932 - val_accuracy: 0.7046 - lr: 1.5625e-05\n",
      "Epoch 28/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6164 - accuracy: 0.6692 - val_loss: 0.5937 - val_accuracy: 0.7002 - lr: 1.5625e-05\n",
      "Epoch 29/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6158 - accuracy: 0.6718 - val_loss: 0.5942 - val_accuracy: 0.6972 - lr: 1.5625e-05\n",
      "Epoch 30/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6137 - accuracy: 0.6724 - val_loss: 0.5954 - val_accuracy: 0.6979 - lr: 1.5625e-05\n",
      "Epoch 31/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6140 - accuracy: 0.6729 - val_loss: 0.5935 - val_accuracy: 0.6996 - lr: 1.5625e-05\n",
      "Epoch 1/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6197 - accuracy: 0.6656 - val_loss: 0.5773 - val_accuracy: 0.7312 - lr: 7.8125e-06\n",
      "Epoch 2/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6178 - accuracy: 0.6717 - val_loss: 0.5747 - val_accuracy: 0.7306 - lr: 7.8125e-06\n",
      "Epoch 3/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6187 - accuracy: 0.6651 - val_loss: 0.5768 - val_accuracy: 0.7323 - lr: 7.8125e-06\n",
      "Epoch 4/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6209 - accuracy: 0.6673 - val_loss: 0.5762 - val_accuracy: 0.7285 - lr: 7.8125e-06\n",
      "Epoch 5/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6160 - accuracy: 0.6714 - val_loss: 0.5811 - val_accuracy: 0.7238 - lr: 7.8125e-06\n",
      "Epoch 6/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6176 - accuracy: 0.6643 - val_loss: 0.5787 - val_accuracy: 0.7292 - lr: 7.8125e-06\n",
      "Epoch 7/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6201 - accuracy: 0.6628 - val_loss: 0.5780 - val_accuracy: 0.7309 - lr: 7.8125e-06\n",
      "Epoch 8/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6188 - accuracy: 0.6660 - val_loss: 0.5788 - val_accuracy: 0.7272 - lr: 7.8125e-06\n",
      "Epoch 9/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6197 - accuracy: 0.6666 - val_loss: 0.5773 - val_accuracy: 0.7309 - lr: 7.8125e-06\n",
      "Epoch 10/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6172 - accuracy: 0.6698 - val_loss: 0.5784 - val_accuracy: 0.7292 - lr: 7.8125e-06\n",
      "Epoch 11/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6190 - accuracy: 0.6645 - val_loss: 0.5773 - val_accuracy: 0.7292 - lr: 7.8125e-06\n",
      "Epoch 12/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6170 - accuracy: 0.6646 - val_loss: 0.5771 - val_accuracy: 0.7279 - lr: 7.8125e-06\n",
      "Epoch 13/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6202 - accuracy: 0.6633 - val_loss: 0.5777 - val_accuracy: 0.7306 - lr: 3.9063e-06\n",
      "Epoch 14/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6171 - accuracy: 0.6694 - val_loss: 0.5791 - val_accuracy: 0.7255 - lr: 3.9063e-06\n",
      "Epoch 15/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6130 - accuracy: 0.6717 - val_loss: 0.5780 - val_accuracy: 0.7302 - lr: 3.9063e-06\n",
      "Epoch 16/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6156 - accuracy: 0.6676 - val_loss: 0.5768 - val_accuracy: 0.7302 - lr: 3.9063e-06\n",
      "Epoch 17/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6171 - accuracy: 0.6677 - val_loss: 0.5790 - val_accuracy: 0.7302 - lr: 3.9063e-06\n",
      "Epoch 18/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6166 - accuracy: 0.6660 - val_loss: 0.5771 - val_accuracy: 0.7268 - lr: 3.9063e-06\n",
      "Epoch 19/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6149 - accuracy: 0.6702 - val_loss: 0.5810 - val_accuracy: 0.7252 - lr: 3.9063e-06\n",
      "Epoch 20/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6160 - accuracy: 0.6721 - val_loss: 0.5799 - val_accuracy: 0.7265 - lr: 3.9063e-06\n",
      "Epoch 21/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6175 - accuracy: 0.6700 - val_loss: 0.5784 - val_accuracy: 0.7316 - lr: 3.9063e-06\n",
      "Epoch 22/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6185 - accuracy: 0.6702 - val_loss: 0.5787 - val_accuracy: 0.7262 - lr: 3.9063e-06\n",
      "Epoch 23/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6169 - accuracy: 0.6671 - val_loss: 0.5785 - val_accuracy: 0.7306 - lr: 1.9531e-06\n",
      "Epoch 24/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6184 - accuracy: 0.6638 - val_loss: 0.5790 - val_accuracy: 0.7268 - lr: 1.9531e-06\n",
      "Epoch 25/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6194 - accuracy: 0.6651 - val_loss: 0.5812 - val_accuracy: 0.7299 - lr: 1.9531e-06\n",
      "Epoch 26/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6162 - accuracy: 0.6718 - val_loss: 0.5781 - val_accuracy: 0.7282 - lr: 1.9531e-06\n",
      "Epoch 27/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6183 - accuracy: 0.6671 - val_loss: 0.5802 - val_accuracy: 0.7268 - lr: 1.9531e-06\n",
      "Epoch 28/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6172 - accuracy: 0.6606 - val_loss: 0.5780 - val_accuracy: 0.7295 - lr: 1.9531e-06\n",
      "Epoch 29/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6203 - accuracy: 0.6648 - val_loss: 0.5782 - val_accuracy: 0.7248 - lr: 1.9531e-06\n",
      "Epoch 30/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6152 - accuracy: 0.6713 - val_loss: 0.5825 - val_accuracy: 0.7262 - lr: 1.9531e-06\n",
      "Epoch 31/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6193 - accuracy: 0.6643 - val_loss: 0.5787 - val_accuracy: 0.7268 - lr: 1.9531e-06\n",
      "Epoch 32/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6182 - accuracy: 0.6685 - val_loss: 0.5792 - val_accuracy: 0.7295 - lr: 1.9531e-06\n",
      "Epoch 1/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6121 - accuracy: 0.6759 - val_loss: 0.5734 - val_accuracy: 0.7248 - lr: 9.7656e-07\n",
      "Epoch 2/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6157 - accuracy: 0.6653 - val_loss: 0.5729 - val_accuracy: 0.7252 - lr: 9.7656e-07\n",
      "Epoch 3/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6175 - accuracy: 0.6674 - val_loss: 0.5727 - val_accuracy: 0.7245 - lr: 9.7656e-07\n",
      "Epoch 4/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6151 - accuracy: 0.6682 - val_loss: 0.5728 - val_accuracy: 0.7265 - lr: 9.7656e-07\n",
      "Epoch 5/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6156 - accuracy: 0.6669 - val_loss: 0.5741 - val_accuracy: 0.7248 - lr: 9.7656e-07\n",
      "Epoch 6/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6160 - accuracy: 0.6663 - val_loss: 0.5749 - val_accuracy: 0.7241 - lr: 9.7656e-07\n",
      "Epoch 7/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6129 - accuracy: 0.6741 - val_loss: 0.5740 - val_accuracy: 0.7221 - lr: 9.7656e-07\n",
      "Epoch 8/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6162 - accuracy: 0.6714 - val_loss: 0.5731 - val_accuracy: 0.7241 - lr: 9.7656e-07\n",
      "Epoch 9/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6180 - accuracy: 0.6712 - val_loss: 0.5738 - val_accuracy: 0.7238 - lr: 9.7656e-07\n",
      "Epoch 10/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6141 - accuracy: 0.6708 - val_loss: 0.5756 - val_accuracy: 0.7214 - lr: 9.7656e-07\n",
      "Epoch 11/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6176 - accuracy: 0.6644 - val_loss: 0.5783 - val_accuracy: 0.7231 - lr: 9.7656e-07\n",
      "Epoch 12/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6169 - accuracy: 0.6661 - val_loss: 0.5759 - val_accuracy: 0.7224 - lr: 9.7656e-07\n",
      "Epoch 13/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6171 - accuracy: 0.6700 - val_loss: 0.5767 - val_accuracy: 0.7187 - lr: 9.7656e-07\n",
      "Epoch 14/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6167 - accuracy: 0.6644 - val_loss: 0.5743 - val_accuracy: 0.7248 - lr: 4.8828e-07\n",
      "Epoch 15/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6148 - accuracy: 0.6682 - val_loss: 0.5735 - val_accuracy: 0.7197 - lr: 4.8828e-07\n",
      "Epoch 16/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6196 - accuracy: 0.6662 - val_loss: 0.5753 - val_accuracy: 0.7238 - lr: 4.8828e-07\n",
      "Epoch 17/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6163 - accuracy: 0.6695 - val_loss: 0.5759 - val_accuracy: 0.7231 - lr: 4.8828e-07\n",
      "Epoch 18/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6165 - accuracy: 0.6631 - val_loss: 0.5739 - val_accuracy: 0.7235 - lr: 4.8828e-07\n",
      "Epoch 19/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6196 - accuracy: 0.6595 - val_loss: 0.5754 - val_accuracy: 0.7204 - lr: 4.8828e-07\n",
      "Epoch 20/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6183 - accuracy: 0.6666 - val_loss: 0.5740 - val_accuracy: 0.7268 - lr: 4.8828e-07\n",
      "Epoch 21/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6132 - accuracy: 0.6715 - val_loss: 0.5709 - val_accuracy: 0.7255 - lr: 4.8828e-07\n",
      "Epoch 22/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6133 - accuracy: 0.6651 - val_loss: 0.5739 - val_accuracy: 0.7231 - lr: 4.8828e-07\n",
      "Epoch 23/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6124 - accuracy: 0.6723 - val_loss: 0.5759 - val_accuracy: 0.7224 - lr: 4.8828e-07\n",
      "Epoch 24/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6156 - accuracy: 0.6609 - val_loss: 0.5705 - val_accuracy: 0.7268 - lr: 4.8828e-07\n",
      "Epoch 25/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6190 - accuracy: 0.6667 - val_loss: 0.5728 - val_accuracy: 0.7228 - lr: 4.8828e-07\n",
      "Epoch 26/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6160 - accuracy: 0.6665 - val_loss: 0.5750 - val_accuracy: 0.7221 - lr: 4.8828e-07\n",
      "Epoch 27/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6170 - accuracy: 0.6673 - val_loss: 0.5729 - val_accuracy: 0.7235 - lr: 4.8828e-07\n",
      "Epoch 28/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6203 - accuracy: 0.6648 - val_loss: 0.5754 - val_accuracy: 0.7204 - lr: 4.8828e-07\n",
      "Epoch 29/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6157 - accuracy: 0.6654 - val_loss: 0.5747 - val_accuracy: 0.7265 - lr: 4.8828e-07\n",
      "Epoch 30/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6144 - accuracy: 0.6666 - val_loss: 0.5752 - val_accuracy: 0.7221 - lr: 4.8828e-07\n",
      "Epoch 31/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6165 - accuracy: 0.6664 - val_loss: 0.5719 - val_accuracy: 0.7241 - lr: 4.8828e-07\n",
      "Epoch 32/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6146 - accuracy: 0.6724 - val_loss: 0.5714 - val_accuracy: 0.7238 - lr: 4.8828e-07\n",
      "Epoch 33/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6157 - accuracy: 0.6628 - val_loss: 0.5736 - val_accuracy: 0.7228 - lr: 4.8828e-07\n",
      "Epoch 34/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6186 - accuracy: 0.6636 - val_loss: 0.5746 - val_accuracy: 0.7218 - lr: 4.8828e-07\n",
      "Epoch 35/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6165 - accuracy: 0.6698 - val_loss: 0.5705 - val_accuracy: 0.7275 - lr: 2.4414e-07\n",
      "Epoch 36/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6176 - accuracy: 0.6647 - val_loss: 0.5731 - val_accuracy: 0.7258 - lr: 2.4414e-07\n",
      "Epoch 37/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6177 - accuracy: 0.6667 - val_loss: 0.5731 - val_accuracy: 0.7248 - lr: 2.4414e-07\n",
      "Epoch 38/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6165 - accuracy: 0.6680 - val_loss: 0.5734 - val_accuracy: 0.7241 - lr: 2.4414e-07\n",
      "Epoch 39/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6163 - accuracy: 0.6670 - val_loss: 0.5770 - val_accuracy: 0.7194 - lr: 2.4414e-07\n",
      "Epoch 40/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6166 - accuracy: 0.6683 - val_loss: 0.5766 - val_accuracy: 0.7194 - lr: 2.4414e-07\n",
      "Epoch 41/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6149 - accuracy: 0.6690 - val_loss: 0.5736 - val_accuracy: 0.7208 - lr: 2.4414e-07\n",
      "Epoch 42/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6141 - accuracy: 0.6695 - val_loss: 0.5756 - val_accuracy: 0.7221 - lr: 2.4414e-07\n",
      "Epoch 43/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6156 - accuracy: 0.6663 - val_loss: 0.5743 - val_accuracy: 0.7292 - lr: 2.4414e-07\n",
      "Epoch 44/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6177 - accuracy: 0.6625 - val_loss: 0.5727 - val_accuracy: 0.7265 - lr: 2.4414e-07\n",
      "Epoch 45/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6189 - accuracy: 0.6648 - val_loss: 0.5709 - val_accuracy: 0.7238 - lr: 1.2207e-07\n",
      "Epoch 46/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6122 - accuracy: 0.6752 - val_loss: 0.5769 - val_accuracy: 0.7221 - lr: 1.2207e-07\n",
      "Epoch 47/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6144 - accuracy: 0.6671 - val_loss: 0.5752 - val_accuracy: 0.7204 - lr: 1.2207e-07\n",
      "Epoch 48/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6158 - accuracy: 0.6668 - val_loss: 0.5770 - val_accuracy: 0.7231 - lr: 1.2207e-07\n",
      "Epoch 49/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6170 - accuracy: 0.6654 - val_loss: 0.5759 - val_accuracy: 0.7201 - lr: 1.2207e-07\n",
      "Epoch 50/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6189 - accuracy: 0.6650 - val_loss: 0.5731 - val_accuracy: 0.7221 - lr: 1.2207e-07\n",
      "Epoch 51/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6205 - accuracy: 0.6633 - val_loss: 0.5752 - val_accuracy: 0.7201 - lr: 1.2207e-07\n",
      "Epoch 52/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6123 - accuracy: 0.6724 - val_loss: 0.5746 - val_accuracy: 0.7228 - lr: 1.2207e-07\n",
      "Epoch 53/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6151 - accuracy: 0.6666 - val_loss: 0.5753 - val_accuracy: 0.7241 - lr: 1.2207e-07\n",
      "Epoch 54/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6165 - accuracy: 0.6733 - val_loss: 0.5748 - val_accuracy: 0.7204 - lr: 1.2207e-07\n",
      "Epoch 55/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6155 - accuracy: 0.6659 - val_loss: 0.5722 - val_accuracy: 0.7221 - lr: 6.1035e-08\n",
      "Epoch 56/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6176 - accuracy: 0.6704 - val_loss: 0.5751 - val_accuracy: 0.7204 - lr: 6.1035e-08\n",
      "Epoch 57/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6163 - accuracy: 0.6703 - val_loss: 0.5731 - val_accuracy: 0.7245 - lr: 6.1035e-08\n",
      "Epoch 58/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6137 - accuracy: 0.6704 - val_loss: 0.5755 - val_accuracy: 0.7201 - lr: 6.1035e-08\n",
      "Epoch 59/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6145 - accuracy: 0.6682 - val_loss: 0.5733 - val_accuracy: 0.7279 - lr: 6.1035e-08\n",
      "Epoch 60/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6167 - accuracy: 0.6661 - val_loss: 0.5738 - val_accuracy: 0.7238 - lr: 6.1035e-08\n",
      "Epoch 61/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6156 - accuracy: 0.6694 - val_loss: 0.5749 - val_accuracy: 0.7224 - lr: 6.1035e-08\n",
      "Epoch 62/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6198 - accuracy: 0.6583 - val_loss: 0.5719 - val_accuracy: 0.7245 - lr: 6.1035e-08\n",
      "Epoch 63/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6174 - accuracy: 0.6666 - val_loss: 0.5759 - val_accuracy: 0.7191 - lr: 6.1035e-08\n",
      "Epoch 64/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6183 - accuracy: 0.6626 - val_loss: 0.5750 - val_accuracy: 0.7224 - lr: 6.1035e-08\n",
      "Epoch 65/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6181 - accuracy: 0.6630 - val_loss: 0.5742 - val_accuracy: 0.7245 - lr: 3.0518e-08\n",
      "Epoch 1/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6205 - accuracy: 0.6638 - val_loss: 0.5764 - val_accuracy: 0.7167 - lr: 3.0518e-08\n",
      "Epoch 2/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6241 - accuracy: 0.6601 - val_loss: 0.5768 - val_accuracy: 0.7133 - lr: 3.0518e-08\n",
      "Epoch 3/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6286 - accuracy: 0.6561 - val_loss: 0.5777 - val_accuracy: 0.7147 - lr: 3.0518e-08\n",
      "Epoch 4/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6251 - accuracy: 0.6571 - val_loss: 0.5767 - val_accuracy: 0.7126 - lr: 3.0518e-08\n",
      "Epoch 5/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6230 - accuracy: 0.6594 - val_loss: 0.5750 - val_accuracy: 0.7174 - lr: 3.0518e-08\n",
      "Epoch 6/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6221 - accuracy: 0.6573 - val_loss: 0.5774 - val_accuracy: 0.7120 - lr: 3.0518e-08\n",
      "Epoch 7/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6220 - accuracy: 0.6668 - val_loss: 0.5769 - val_accuracy: 0.7143 - lr: 3.0518e-08\n",
      "Epoch 8/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6199 - accuracy: 0.6672 - val_loss: 0.5753 - val_accuracy: 0.7150 - lr: 3.0518e-08\n",
      "Epoch 9/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6195 - accuracy: 0.6644 - val_loss: 0.5752 - val_accuracy: 0.7153 - lr: 3.0518e-08\n",
      "Epoch 10/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6234 - accuracy: 0.6621 - val_loss: 0.5771 - val_accuracy: 0.7157 - lr: 3.0518e-08\n",
      "Epoch 11/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6198 - accuracy: 0.6628 - val_loss: 0.5758 - val_accuracy: 0.7126 - lr: 3.0518e-08\n",
      "Epoch 12/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6218 - accuracy: 0.6645 - val_loss: 0.5763 - val_accuracy: 0.7184 - lr: 3.0518e-08\n",
      "Epoch 13/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6203 - accuracy: 0.6642 - val_loss: 0.5765 - val_accuracy: 0.7133 - lr: 3.0518e-08\n",
      "Epoch 14/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6197 - accuracy: 0.6646 - val_loss: 0.5756 - val_accuracy: 0.7140 - lr: 3.0518e-08\n",
      "Epoch 15/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6193 - accuracy: 0.6639 - val_loss: 0.5745 - val_accuracy: 0.7153 - lr: 3.0518e-08\n",
      "Epoch 16/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6204 - accuracy: 0.6619 - val_loss: 0.5767 - val_accuracy: 0.7170 - lr: 3.0518e-08\n",
      "Epoch 17/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6221 - accuracy: 0.6593 - val_loss: 0.5763 - val_accuracy: 0.7164 - lr: 3.0518e-08\n",
      "Epoch 18/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6252 - accuracy: 0.6606 - val_loss: 0.5769 - val_accuracy: 0.7187 - lr: 3.0518e-08\n",
      "Epoch 19/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6222 - accuracy: 0.6630 - val_loss: 0.5750 - val_accuracy: 0.7137 - lr: 3.0518e-08\n",
      "Epoch 20/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6215 - accuracy: 0.6600 - val_loss: 0.5772 - val_accuracy: 0.7143 - lr: 3.0518e-08\n",
      "Epoch 21/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6195 - accuracy: 0.6660 - val_loss: 0.5747 - val_accuracy: 0.7167 - lr: 3.0518e-08\n",
      "Epoch 22/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6245 - accuracy: 0.6584 - val_loss: 0.5757 - val_accuracy: 0.7130 - lr: 3.0518e-08\n",
      "Epoch 23/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6240 - accuracy: 0.6606 - val_loss: 0.5775 - val_accuracy: 0.7133 - lr: 3.0518e-08\n",
      "Epoch 24/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6211 - accuracy: 0.6611 - val_loss: 0.5764 - val_accuracy: 0.7147 - lr: 3.0518e-08\n",
      "Epoch 25/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6223 - accuracy: 0.6633 - val_loss: 0.5740 - val_accuracy: 0.7167 - lr: 3.0518e-08\n",
      "Epoch 26/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6239 - accuracy: 0.6603 - val_loss: 0.5755 - val_accuracy: 0.7140 - lr: 3.0518e-08\n",
      "Epoch 27/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6221 - accuracy: 0.6603 - val_loss: 0.5783 - val_accuracy: 0.7153 - lr: 3.0518e-08\n",
      "Epoch 28/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6233 - accuracy: 0.6619 - val_loss: 0.5741 - val_accuracy: 0.7147 - lr: 3.0518e-08\n",
      "Epoch 29/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6218 - accuracy: 0.6616 - val_loss: 0.5769 - val_accuracy: 0.7130 - lr: 3.0518e-08\n",
      "Epoch 30/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6222 - accuracy: 0.6637 - val_loss: 0.5762 - val_accuracy: 0.7116 - lr: 3.0518e-08\n",
      "Epoch 31/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6251 - accuracy: 0.6581 - val_loss: 0.5760 - val_accuracy: 0.7164 - lr: 3.0518e-08\n",
      "Epoch 32/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6232 - accuracy: 0.6616 - val_loss: 0.5768 - val_accuracy: 0.7157 - lr: 3.0518e-08\n",
      "Epoch 33/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6220 - accuracy: 0.6649 - val_loss: 0.5753 - val_accuracy: 0.7164 - lr: 3.0518e-08\n",
      "Epoch 34/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6223 - accuracy: 0.6614 - val_loss: 0.5772 - val_accuracy: 0.7174 - lr: 3.0518e-08\n",
      "Epoch 35/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6227 - accuracy: 0.6611 - val_loss: 0.5769 - val_accuracy: 0.7120 - lr: 3.0518e-08\n",
      "Epoch 36/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6219 - accuracy: 0.6591 - val_loss: 0.5762 - val_accuracy: 0.7130 - lr: 1.5259e-08\n",
      "Epoch 37/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6190 - accuracy: 0.6627 - val_loss: 0.5758 - val_accuracy: 0.7174 - lr: 1.5259e-08\n",
      "Epoch 38/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6196 - accuracy: 0.6667 - val_loss: 0.5763 - val_accuracy: 0.7143 - lr: 1.5259e-08\n",
      "Epoch 39/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6186 - accuracy: 0.6659 - val_loss: 0.5743 - val_accuracy: 0.7157 - lr: 1.5259e-08\n",
      "Epoch 40/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6207 - accuracy: 0.6638 - val_loss: 0.5758 - val_accuracy: 0.7137 - lr: 1.5259e-08\n",
      "Epoch 41/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6200 - accuracy: 0.6666 - val_loss: 0.5757 - val_accuracy: 0.7164 - lr: 1.5259e-08\n",
      "Epoch 42/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6246 - accuracy: 0.6572 - val_loss: 0.5747 - val_accuracy: 0.7133 - lr: 1.5259e-08\n",
      "Epoch 43/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6217 - accuracy: 0.6622 - val_loss: 0.5765 - val_accuracy: 0.7194 - lr: 1.5259e-08\n",
      "Epoch 44/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6235 - accuracy: 0.6595 - val_loss: 0.5766 - val_accuracy: 0.7181 - lr: 1.5259e-08\n",
      "Epoch 45/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6203 - accuracy: 0.6632 - val_loss: 0.5759 - val_accuracy: 0.7167 - lr: 1.5259e-08\n",
      "Epoch 46/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6197 - accuracy: 0.6678 - val_loss: 0.5764 - val_accuracy: 0.7187 - lr: 7.6294e-09\n",
      "Epoch 47/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6226 - accuracy: 0.6571 - val_loss: 0.5758 - val_accuracy: 0.7143 - lr: 7.6294e-09\n",
      "Epoch 48/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6229 - accuracy: 0.6604 - val_loss: 0.5754 - val_accuracy: 0.7123 - lr: 7.6294e-09\n",
      "Epoch 49/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6203 - accuracy: 0.6656 - val_loss: 0.5765 - val_accuracy: 0.7130 - lr: 7.6294e-09\n",
      "Epoch 50/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6231 - accuracy: 0.6609 - val_loss: 0.5747 - val_accuracy: 0.7177 - lr: 7.6294e-09\n",
      "Epoch 51/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6221 - accuracy: 0.6611 - val_loss: 0.5764 - val_accuracy: 0.7123 - lr: 7.6294e-09\n",
      "Epoch 52/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6205 - accuracy: 0.6604 - val_loss: 0.5741 - val_accuracy: 0.7224 - lr: 7.6294e-09\n",
      "Epoch 53/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6207 - accuracy: 0.6616 - val_loss: 0.5762 - val_accuracy: 0.7150 - lr: 7.6294e-09\n",
      "Epoch 54/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6242 - accuracy: 0.6611 - val_loss: 0.5755 - val_accuracy: 0.7153 - lr: 7.6294e-09\n",
      "Epoch 55/1000\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6182 - accuracy: 0.6649 - val_loss: 0.5758 - val_accuracy: 0.7191 - lr: 7.6294e-09\n"
     ]
    }
   ],
   "source": [
    "# Train the model using k-fold cross-validation\n",
    "for train_index, val_index in kfold.split(X):\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    # Scale the data\n",
    "    X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n",
    "    X_val_fold_scaled = scaler.transform(X_val_fold)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_fold_scaled, \n",
    "              y_train_fold, \n",
    "              epochs=1000, \n",
    "              batch_size=32, \n",
    "              validation_data=(X_val_fold_scaled, y_val_fold), \n",
    "              callbacks=[lr_scheduler, early_stopping],  \n",
    "              class_weight=class_weights_dict \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scale the validation data with the same scaler used for the training data\n",
    "validation_x = scaler.fit_transform(validation_data.drop(columns=['result', 'label'], axis=1).values)\n",
    "\n",
    "# Run predictions on the validation dataset\n",
    "y_pred = model.predict(validation_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6238988893144389 - precision: 0.4930232558139535 - recall: 0.5480868665977249 - f1 0.5190989226248776\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(validation_data['label'], np.where(y_pred > 0.5, 1, 0).flatten()).ravel()\n",
    "\n",
    "accuracy = (tp+tn) / (tp+tn+fn+fp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2 * (precision*recall)/(precision+recall)\n",
    "\n",
    "print(f'accuracy: {accuracy} - precision: {precision} - recall: {recall} - f1 {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(df, name):\n",
    "    winners = df[df['result'] >= 0]\n",
    "    losers = df[df['result'] < 0]\n",
    "    \n",
    "    return pd.DataFrame.from_dict({\n",
    "        'count': len(df),\n",
    "        'avg': df['result'].mean(),\n",
    "        'winrate': len(winners) / len(df),\n",
    "        'avg_winner': winners['result'].mean(),\n",
    "        'avg_loser': losers['result'].mean(),\n",
    "        'total_win': winners['result'].sum(),\n",
    "        'total_loss':  losers['result'].sum(),\n",
    "        'profit_factor': winners['result'].sum() / -losers['result'].sum()\n",
    "    }, orient='index', columns=[name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>predicted winners</th>\n",
       "      <th>predicted losers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2611.000000</td>\n",
       "      <td>1075.000000</td>\n",
       "      <td>1536.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>0.048369</td>\n",
       "      <td>0.130555</td>\n",
       "      <td>-0.009150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winrate</th>\n",
       "      <td>0.387591</td>\n",
       "      <td>0.505116</td>\n",
       "      <td>0.305339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_winner</th>\n",
       "      <td>0.328885</td>\n",
       "      <td>0.370692</td>\n",
       "      <td>0.280481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_loser</th>\n",
       "      <td>-0.129168</td>\n",
       "      <td>-0.114547</td>\n",
       "      <td>-0.136457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_win</th>\n",
       "      <td>332.831466</td>\n",
       "      <td>201.285968</td>\n",
       "      <td>131.545498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_loss</th>\n",
       "      <td>-206.538871</td>\n",
       "      <td>-60.939099</td>\n",
       "      <td>-145.599772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>profit_factor</th>\n",
       "      <td>1.611471</td>\n",
       "      <td>3.303068</td>\n",
       "      <td>0.903473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       all  predicted winners  predicted losers\n",
       "count          2611.000000        1075.000000       1536.000000\n",
       "avg               0.048369           0.130555         -0.009150\n",
       "winrate           0.387591           0.505116          0.305339\n",
       "avg_winner        0.328885           0.370692          0.280481\n",
       "avg_loser        -0.129168          -0.114547         -0.136457\n",
       "total_win       332.831466         201.285968        131.545498\n",
       "total_loss     -206.538871         -60.939099       -145.599772\n",
       "profit_factor     1.611471           3.303068          0.903473"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = validation_data[['result', 'label']].copy()\n",
    "res['pred'] = y_pred\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "all_trades = summarize(res, \"all\")\n",
    "pred_winners = summarize(res[res['pred'] >= threshold], \"predicted winners\")\n",
    "pred_losers = summarize(res[res['pred'] < threshold], \"predicted losers\")\n",
    "\n",
    "\n",
    "pd.concat([all_trades, pred_winners, pred_losers,], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter( res['pred'], res['result'] +1)\n",
    "ax.set_yscale(\"log\");\n",
    "# ax.set_xscale(\"log\");\n",
    "# All above 1 is winners\n",
    "plt.axhline(y=1, color='r', linestyle='-')\n",
    "# All above threshold is predicted to be winners\n",
    "plt.axvline(x=threshold, color='r', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide(a, b):\n",
    "    return b and a / b or 0\n",
    "\n",
    "def getStats(inrange):\n",
    "    winners = inrange[inrange['result'] >= 0]\n",
    "    losers = inrange[inrange['result'] <= 0]\n",
    "        \n",
    "    d = {\n",
    "        'winrate': divide(len(winners),len(inrange)),\n",
    "        'profit_factor': divide(winners['result'].sum(), -losers['result'].sum())\n",
    "    }\n",
    "    return d\n",
    "\n",
    "percentiles = []\n",
    "winrates_above = []\n",
    "profitfactors_above = []\n",
    "winrates_below = []\n",
    "profitfactors_below = []\n",
    "\n",
    "for i in range(10):\n",
    "    percentiles.append(i*10)\n",
    "\n",
    "    # Probability Above i/10\n",
    "    above = getStats(res[res['pred'] >= i/10])\n",
    "    winrates_above.append(above['winrate'])\n",
    "    profitfactors_above.append(above['profit_factor'])\n",
    "\n",
    "    # Probability below i/10\n",
    "    below = getStats(res[res['pred'] < i/10])\n",
    "    winrates_below.append(below['winrate'])\n",
    "    profitfactors_below.append(below['profit_factor'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.bar( percentiles, winrates_above)\n",
    "plt.axhline(y=0.5, color='r', linestyle=':')\n",
    "plt.xlabel('Predicted Probability Above')\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.bar( percentiles, winrates_below)\n",
    "plt.axhline(y=0.5, color='r', linestyle=':')\n",
    "plt.xlabel('Predicted Probability Below')\n",
    "plt.ylabel('Winrate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar( percentiles, profitfactors_above)\n",
    "plt.axhline(y=1, color='r', linestyle=':')\n",
    "plt.xlabel('Predicted Probability above')\n",
    "plt.ylabel('Profit factor')\n",
    "\n",
    "fig, ax2 = plt.subplots()\n",
    "ax2.bar( percentiles, profitfactors_below)\n",
    "plt.axhline(y=1, color='r', linestyle=':')\n",
    "plt.xlabel('Predicted Probability below')\n",
    "plt.ylabel('Profit factor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6688f5bd7a5c2379fdfde91b010ab5a3ba8033d4f740240b607cd397292295b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
