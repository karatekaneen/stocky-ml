{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "### Operations\n",
    "- Figure out the least amount of data that needs to be provided\n",
    "- Save + load model and compare results\n",
    "- Build API around it\n",
    "\n",
    "### Features\n",
    "- ~~Add feature for days since last signal~~\n",
    "- ~~Add feature for number of signals last year~~\n",
    "- ~~Kaufmanns efficiency ratio~~\n",
    "- Insider buys/sells\n",
    "- VIX\n",
    "- Interest rates\n",
    "- Squeeze DIX/GEX\n",
    "- Add feature based on output on news model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:\\dev\\stocky-ml\\credentials.json\"\n",
    "\n",
    "# Data:\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dateutil.parser\n",
    "import dateutil.tz as tz\n",
    "from datetime import datetime, timedelta\n",
    "import talib   \n",
    "from talib import MA_Type\n",
    "\n",
    "\n",
    "# Visualization:\n",
    "import seaborn as sns\n",
    "\n",
    "# Database:\n",
    "from google.cloud import firestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create db instance: \n",
    "db = firestore.Client()\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price_data(id):\n",
    "    doc = db.collection('prices').document(id).get().to_dict()\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(doc['priceData'])\n",
    "    df = add_calculated_columns(df)\n",
    "    df = convert_dates(df)\n",
    "\n",
    "    # Read the file to lazily make sure that the dates are strings etc.\n",
    "    # FIXME: Should probably be done some other way.\n",
    "    # output.drop(columns=['Unnamed: 0'], inplace = True)\n",
    "    return df\n",
    "\n",
    "def get_trade_data(doc):\n",
    "    doc = doc.to_dict()\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(doc['trades'])\n",
    "    df['result'] = (df['exitPrice'] / df['entryPrice']) -1\n",
    "    df['trades_last_year'] =  count_trades_last_year(df, df['entryDate'])\n",
    "    shifted = df['exitDate'].shift()\n",
    "    df['days_since_last_signal'] = (df['entryDate'] - shifted).dt.days\n",
    "    # df['days_since_last_signal'] =df['days_since_last_signal'].days\n",
    "\n",
    "    df = convert_trade_dates(df)\n",
    "\n",
    "    return df[['date', 'result', 'trades_last_year',  'days_since_last_signal']]\n",
    "\n",
    "def convert_date(date, fmt = \"%Y-%m-%d\", target_tz = tz.gettz('CET')):\n",
    "    return date.replace(tzinfo=tz.gettz(\"UTC\")).astimezone(target_tz).strftime(fmt)\n",
    "\n",
    "def count_trades_last_year(df, d):\n",
    "    # print(d)\n",
    "    out = []\n",
    "\n",
    "    for x in d:\n",
    "        trades_last_year = df[(df['entryDate'] < x - timedelta(days=2)) & (df['entryDate'] >= x - timedelta(days=365))]\n",
    "        out.append(len(trades_last_year))\n",
    "    return out\n",
    "\n",
    "    \n",
    "def convert_dates(df):\n",
    "    cet = tz.gettz('CET')\n",
    "    for i, row in df.iterrows():\n",
    "        d = convert_date(dateutil.parser.isoparse(row['date']))\n",
    "        df.at[i,'date'] = d\n",
    "    return df\n",
    "\n",
    "def kaufmanns_efficiency_ratio(prices, n):\n",
    "    \"\"\"\n",
    "    Calculates Kaufmann's Efficiency Ratio over a lookback of n.\n",
    "    :param prices: list of prices\n",
    "    :param n: lookback period\n",
    "    :return: Kaufmann's Efficiency Ratio\n",
    "    \"\"\"\n",
    "    change = abs(prices[len(prices) -1] - prices[0])\n",
    "    volatility = sum(abs(prices[i] - prices[i-1]) for i in range(1, n))\n",
    "    return change / volatility if volatility != 0 else 0\n",
    "\n",
    "\n",
    "def convert_trade_dates(df):\n",
    "    cet = tz.gettz('CET')\n",
    "    for i, row in df.iterrows():\n",
    "        d = convert_date(row['entryDate'] - timedelta(days=1)) # Want one day earlier so that we don't have look ahead\n",
    "        df.at[i,'date'] = d\n",
    "    return df\n",
    "\n",
    "def add_calculated_columns(price):\n",
    "    lookbacks = [20, 50, 100, 200]\n",
    "    values = ['close', 'volume']\n",
    "        \n",
    "    price['volume_cash'] = round(price['volume'] * (price['close'] * 2 + price['open'] * 2 + price['low'] + price['high'])/6)\n",
    "    for value in values:\n",
    "        for lookback in lookbacks:\n",
    "            # Get the rolling average and std:\n",
    "            price['average'] = price[value].rolling(lookback).mean()\n",
    "            price['std'] = price[value].rolling(lookback).std()\n",
    "            high = price['high'].rolling(lookback).max()\n",
    "            low = price['low'].rolling(lookback).min()\n",
    "            \n",
    "\n",
    "            # Normalize distance to mean. This could be done with the data above but dont know how.\n",
    "            price[f'zs-{lookback}-{value}'] = (price[value] - price['average']) / price['std']\n",
    "\n",
    "            # Get slope of rolling average and std\n",
    "            price[f'ma-slope-{lookback}-{value}'] = price['average'] / price['average'].shift(1)\n",
    "            price[f'std-slope-{lookback}-{value}'] = price['std'] / price['std'].shift(1)\n",
    "\n",
    "            # Get range\n",
    "            price[f'rng-{lookback}'] = high / low\n",
    "            price[f'percent-rng-{lookback}-{value}'] = (high / low) / price[value]\n",
    "            price[f'percent-std-{lookback}-{value}'] = price['std'] / price[value]\n",
    "\n",
    "            if value == 'volume':\n",
    "                price['temp_volume'] = round(price['volume'] * (price['close'] * 2 + price['open'] * 2 + price['low'] + price['high'])/6)\n",
    "                # price[f'avg-log-volume-{lookback}'] = np.log10(price['temp_volume'])  \n",
    "                price.drop(columns=['temp_volume'], inplace = True)\n",
    "            else:\n",
    "                # Hehe, so bad code\n",
    "                # apply the kaufmanns_efficiency_ratio function to a rolling window of the close column\n",
    "                price[f'kaufmanns_efficiency_ratio-{lookback}'] = price['close'].rolling(window=lookback).apply(lambda x: kaufmanns_efficiency_ratio(x.tolist(), lookback))\n",
    "\n",
    "\n",
    "            # Drop the actual values since they carry no interest:\n",
    "            price.drop(columns=['average', 'std'], inplace = True)\n",
    "            \n",
    "        # TODO: Add calculations for volume\n",
    "        # TODO: Add calculations for owners\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_index_stock_df(omxdf, stockdf):\n",
    "  df = pd.merge(stockdf[['date', 'close']], omxdf[['date', 'close']], on='date', suffixes=('_stock', '_omx'))\n",
    "  # df['date'] = df['date_stock']\n",
    "  df['stock_quota'] = df['close_stock']/df['close_omx']\n",
    "\n",
    "\n",
    "  df['stock_hist_relative_perf20'] = df['stock_quota'].shift(20) / df['stock_quota']\n",
    "  df['stock_hist_relative_perf50'] = df['stock_quota'].shift(50) / df['stock_quota']\n",
    "  df['stock_hist_relative_perf100'] = df['stock_quota'].shift(100) / df['stock_quota']\n",
    "\n",
    "  df['stock_hist_perf20'] = df['close_stock'].shift(20) / df['close_stock']\n",
    "  df['stock_hist_perf50'] = df['close_stock'].shift(50) / df['close_stock']\n",
    "  df['stock_hist_perf100'] = df['close_stock'].shift(100) / df['close_stock']\n",
    "\n",
    "  for p in [3, 10, 34, 100]:\n",
    "    df[f'stock_relative_rsi_{p}'] = talib.RSI(df['stock_quota'], timeperiod=p) /100\n",
    "    df[f'stock_rsi_{p}'] = talib.RSI(df['close_stock'], timeperiod=p) / 100\n",
    "\n",
    "  df.drop(columns=['close_stock', 'close_omx'], inplace = True)\n",
    "\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the omx price data\n",
    "omxdf = get_price_data('19002')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_columns = [\n",
    "                'Unnamed: 0',\n",
    "                # These values does not carry much importance\n",
    "                'volume_omx',\n",
    "                'owners_omx',\n",
    "                'date',\n",
    "                'high_stock',\n",
    "                'low_stock',\n",
    "                'owners_stock',\n",
    "                # 'close_stock',\n",
    "                'open_stock',\n",
    "                'high_omx',\n",
    "                'low_omx',\n",
    "                'owners_omx',\n",
    "                'close_omx',\n",
    "                'open_omx',\n",
    "                # Theses values are missing a lot of the time and would result in a lot of rows being dropped.\n",
    "                # TODO: See if you can improve the data quality to be able to use more of these\n",
    "                'owners_stock', \n",
    "                'zs-20-volume_omx',\n",
    "                'ma-slope-20-volume_omx',\n",
    "                'std-slope-20-volume_omx',\n",
    "                'percent-rng-20-volume_omx',\n",
    "                'percent-std-20-volume_omx',\n",
    "                'avg-log-volume-20_omx',\n",
    "                'zs-50-volume_omx',\n",
    "                'ma-slope-50-volume_omx',\n",
    "                'std-slope-50-volume_omx',\n",
    "                'percent-rng-50-volume_omx',\n",
    "                'percent-std-50-volume_omx',\n",
    "                'avg-log-volume-50_omx',\n",
    "                'zs-100-volume_omx',\n",
    "                'ma-slope-100-volume_omx',\n",
    "                'std-slope-100-volume_omx',\n",
    "                'percent-rng-100-volume_omx',\n",
    "                'percent-std-100-volume_omx',\n",
    "                'avg-log-volume-100_omx',\n",
    "                'zs-200-volume_omx',\n",
    "                'ma-slope-200-volume_omx',\n",
    "                'std-slope-200-volume_omx',\n",
    "                'percent-rng-200-volume_omx',\n",
    "                'percent-std-200-volume_omx',\n",
    "                'avg-log-volume-200_omx',\n",
    "                'zs-200-volume_stock',            \n",
    "                'ma-slope-200-volume_stock',      \n",
    "                'std-slope-200-volume_stock',     \n",
    "                'percent-rng-200-volume_stock',  \n",
    "                'percent-std-200-volume_stock',   \n",
    "                'zs-100-volume_stock',            \n",
    "                'ma-slope-100-volume_stock',      \n",
    "                'std-slope-100-volume_stock',     \n",
    "                'percent-rng-100-volume_stock',   \n",
    "                'percent-std-100-volume_stock',\n",
    "                'volume_stock',\n",
    "                'volume_cash_omx'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volume_stock: 3060.0 not equal to 3060.0. \n",
      "high_stock: 13.8 not equal to 13.8. \n",
      "low_stock: 12.0 not equal to 12.0. \n",
      "owners_stock: 392.0 not equal to 392.0. \n",
      "close_stock: 13.8 not equal to 13.8. \n",
      "open_stock: 12.6 not equal to 12.6. \n",
      "volume_cash_stock: 40086.0 not equal to 40086.0. \n",
      "rng-20_stock: 1.407079646017699 not equal to 1.407079646017699. \n",
      "rng-50_stock: 1.6460176991150444 not equal to 1.6460176991150444. \n",
      "rng-100_stock: 1.823008849557522 not equal to 1.823008849557522. \n",
      "rng-200_stock: 2.389380530973451 not equal to 2.389380530973451. \n",
      "percent-rng-200-close_stock: 0.1731435167372066 not equal to 0.1731435167372066. \n",
      "ma-slope-50-volume_stock: 1.0255056161404772 not equal to 1.0255056161404772. \n",
      "ma-slope-100-volume_stock: 1.0167263516464322 not equal to 1.0167263516464322. \n",
      "std-slope-100-volume_stock: 1.0004628596377207 not equal to 1.0004628596377207. \n",
      "ma-slope-200-volume_stock: 1.0083272278908526 not equal to 1.0083272278908526. \n",
      "volume_omx: 72259328.0 not equal to 72259328.0. \n",
      "high_omx: 2075.76 not equal to 2075.76. \n",
      "low_omx: 2054.96 not equal to 2054.96. \n",
      "close_omx: 2075.76 not equal to 2075.76. \n",
      "open_omx: 2056.93 not equal to 2056.93. \n",
      "volume_cash_omx: 149288975970.0 not equal to 149288975970.0. \n",
      "rng-20_omx: 1.068451454463699 not equal to 1.068451454463699. \n",
      "ma-slope-50-close_omx: 1.00169280796087 not equal to 1.00169280796087. \n",
      "rng-50_omx: 1.1346963193081518 not equal to 1.1346963193081518. \n",
      "ma-slope-100-close_omx: 1.0002178783026006 not equal to 1.0002178783026006. \n",
      "rng-100_omx: 1.2135004237121678 not equal to 1.2135004237121678. \n",
      "rng-200_omx: 1.2312233776874857 not equal to 1.2312233776874857. \n",
      "ma-slope-20-volume_omx: 0.9818683071010798 not equal to 0.9818683071010798. \n",
      "percent-rng-20-volume_omx: 1.4786346400338776e-08 not equal to 1.4786346400338776e-08. \n",
      "ma-slope-50-volume_omx: 0.969153952513124 not equal to 0.969153952513124. \n",
      "ma-slope-100-volume_omx: 0.997554653237363 not equal to 0.997554653237363. \n",
      "std-slope-100-volume_omx: 1.0062733703034008 not equal to 1.0062733703034008. \n",
      "percent-rng-100-volume_omx: 1.6793685428574256e-08 not equal to 1.6793685428574256e-08. \n",
      "percent-rng-200-volume_omx: 1.7038954163640793e-08 not equal to 1.7038954163640793e-08. \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from dateutil import parser\n",
    "\n",
    "vstockdf = pd.read_json('data/stock.json')\n",
    "vomxdf = pd.read_json('data/omx.json')\n",
    "original = pd.read_csv('stockytrades20230410-2.csv')\n",
    "\n",
    "# load the JSON data from a file\n",
    "with open('data/trades.json', 'r') as f:\n",
    "    v_trade_data = json.load(f)\n",
    "\n",
    "# convert the \"triggerDate\" field to a datetime object\n",
    "for item in v_trade_data:\n",
    "    item[\"triggerDate\"] = parser.parse(item[\"triggerDate\"])\n",
    "\n",
    "\n",
    "stockindex = vstockdf.index[(vstockdf['date'] == '2023-01-01T23:00:00+00:00')].item()\n",
    "omxindex = vomxdf.index[(vomxdf['date'] == '2023-01-01T23:00:00+00:00')].item()\n",
    "# TODO: trades_last_year,days_since_last_signal\n",
    "# This is what will be the input\n",
    "vstockres = vstockdf.loc[(stockindex-200):stockindex]\n",
    "v_omx_res = vomxdf.loc[(omxindex-200):omxindex]\n",
    "\n",
    "# All calculated fields to the input\n",
    "vstockres =  add_calculated_columns(vstockres.copy())\n",
    "v_omx_res =  add_calculated_columns(v_omx_res.copy())\n",
    "\n",
    "v_merged = pd.merge(vstockres, v_omx_res, on='date', suffixes=('_stock', '_omx'))\n",
    "\n",
    "\n",
    "\n",
    "validation_row = v_merged.iloc[-1]\n",
    "# condition =\n",
    "\n",
    "# select the rows based on the condition\n",
    "original_row = original.loc[ (original['date'] == '2023-01-02') & (original['close_stock'] == 13.8)].iloc[-1]\n",
    "original_row\n",
    "\n",
    "v_merged.drop(columns=unwanted_columns, inplace=True, errors='ignore')\n",
    "original.drop(columns=unwanted_columns, inplace=True, errors='ignore')\n",
    "\n",
    "for col_name in validation_row.index:\n",
    "    if validation_row[col_name] == original_row[col_name]:\n",
    "        print(f'{col_name}: {validation_row[col_name]} not equal to {original_row[col_name]}. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have 20449\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close_stock</th>\n",
       "      <th>volume_cash_stock</th>\n",
       "      <th>zs-20-close_stock</th>\n",
       "      <th>ma-slope-20-close_stock</th>\n",
       "      <th>std-slope-20-close_stock</th>\n",
       "      <th>rng-20_stock</th>\n",
       "      <th>percent-rng-20-close_stock</th>\n",
       "      <th>percent-std-20-close_stock</th>\n",
       "      <th>kaufmanns_efficiency_ratio-20_stock</th>\n",
       "      <th>zs-50-close_stock</th>\n",
       "      <th>ma-slope-50-close_stock</th>\n",
       "      <th>std-slope-50-close_stock</th>\n",
       "      <th>rng-50_stock</th>\n",
       "      <th>percent-rng-50-close_stock</th>\n",
       "      <th>percent-std-50-close_stock</th>\n",
       "      <th>kaufmanns_efficiency_ratio-50_stock</th>\n",
       "      <th>zs-100-close_stock</th>\n",
       "      <th>ma-slope-100-close_stock</th>\n",
       "      <th>std-slope-100-close_stock</th>\n",
       "      <th>rng-100_stock</th>\n",
       "      <th>percent-rng-100-close_stock</th>\n",
       "      <th>percent-std-100-close_stock</th>\n",
       "      <th>kaufmanns_efficiency_ratio-100_stock</th>\n",
       "      <th>zs-200-close_stock</th>\n",
       "      <th>ma-slope-200-close_stock</th>\n",
       "      <th>...</th>\n",
       "      <th>zs-200-close_omx</th>\n",
       "      <th>ma-slope-200-close_omx</th>\n",
       "      <th>std-slope-200-close_omx</th>\n",
       "      <th>rng-200_omx</th>\n",
       "      <th>percent-rng-200-close_omx</th>\n",
       "      <th>percent-std-200-close_omx</th>\n",
       "      <th>kaufmanns_efficiency_ratio-200_omx</th>\n",
       "      <th>stock_quota</th>\n",
       "      <th>stock_hist_relative_perf20</th>\n",
       "      <th>stock_hist_relative_perf50</th>\n",
       "      <th>stock_hist_relative_perf100</th>\n",
       "      <th>stock_hist_perf20</th>\n",
       "      <th>stock_hist_perf50</th>\n",
       "      <th>stock_hist_perf100</th>\n",
       "      <th>stock_relative_rsi_3</th>\n",
       "      <th>stock_rsi_3</th>\n",
       "      <th>stock_relative_rsi_10</th>\n",
       "      <th>stock_rsi_10</th>\n",
       "      <th>stock_relative_rsi_34</th>\n",
       "      <th>stock_rsi_34</th>\n",
       "      <th>stock_relative_rsi_100</th>\n",
       "      <th>stock_rsi_100</th>\n",
       "      <th>result</th>\n",
       "      <th>trades_last_year</th>\n",
       "      <th>days_since_last_signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104.800</td>\n",
       "      <td>96308260.0</td>\n",
       "      <td>2.406278</td>\n",
       "      <td>1.008411</td>\n",
       "      <td>1.206170</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>0.011768</td>\n",
       "      <td>0.049977</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.550730</td>\n",
       "      <td>1.000816</td>\n",
       "      <td>1.008874</td>\n",
       "      <td>1.205161</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.034478</td>\n",
       "      <td>0.121085</td>\n",
       "      <td>0.059427</td>\n",
       "      <td>0.910515</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.853244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.972445</td>\n",
       "      <td>0.982223</td>\n",
       "      <td>0.811284</td>\n",
       "      <td>0.867440</td>\n",
       "      <td>0.574894</td>\n",
       "      <td>0.654295</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.334595</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>169.900</td>\n",
       "      <td>358482267.0</td>\n",
       "      <td>2.570386</td>\n",
       "      <td>1.012324</td>\n",
       "      <td>1.023911</td>\n",
       "      <td>1.277528</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.041544</td>\n",
       "      <td>0.263646</td>\n",
       "      <td>2.138433</td>\n",
       "      <td>1.010411</td>\n",
       "      <td>1.007476</td>\n",
       "      <td>1.849783</td>\n",
       "      <td>0.010887</td>\n",
       "      <td>0.097403</td>\n",
       "      <td>0.320304</td>\n",
       "      <td>2.087838</td>\n",
       "      <td>1.003921</td>\n",
       "      <td>1.019671</td>\n",
       "      <td>2.210915</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.105117</td>\n",
       "      <td>0.110061</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316113</td>\n",
       "      <td>1.000618</td>\n",
       "      <td>0.996687</td>\n",
       "      <td>1.510986</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.079502</td>\n",
       "      <td>0.047553</td>\n",
       "      <td>0.099913</td>\n",
       "      <td>0.874627</td>\n",
       "      <td>0.742786</td>\n",
       "      <td>0.662212</td>\n",
       "      <td>0.782519</td>\n",
       "      <td>0.592113</td>\n",
       "      <td>0.695115</td>\n",
       "      <td>0.844962</td>\n",
       "      <td>0.938508</td>\n",
       "      <td>0.630043</td>\n",
       "      <td>0.713423</td>\n",
       "      <td>0.582931</td>\n",
       "      <td>0.604268</td>\n",
       "      <td>0.576904</td>\n",
       "      <td>0.579589</td>\n",
       "      <td>-0.134604</td>\n",
       "      <td>1</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>184.450</td>\n",
       "      <td>165471292.0</td>\n",
       "      <td>2.339982</td>\n",
       "      <td>1.000686</td>\n",
       "      <td>1.031583</td>\n",
       "      <td>1.262564</td>\n",
       "      <td>0.006845</td>\n",
       "      <td>0.047219</td>\n",
       "      <td>0.083603</td>\n",
       "      <td>2.121880</td>\n",
       "      <td>1.007725</td>\n",
       "      <td>1.012728</td>\n",
       "      <td>1.526664</td>\n",
       "      <td>0.008277</td>\n",
       "      <td>0.079306</td>\n",
       "      <td>0.245033</td>\n",
       "      <td>1.999881</td>\n",
       "      <td>1.003812</td>\n",
       "      <td>1.019996</td>\n",
       "      <td>2.393700</td>\n",
       "      <td>0.012977</td>\n",
       "      <td>0.117752</td>\n",
       "      <td>0.101908</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475301</td>\n",
       "      <td>1.000196</td>\n",
       "      <td>1.000570</td>\n",
       "      <td>1.510986</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.076064</td>\n",
       "      <td>0.018926</td>\n",
       "      <td>0.106564</td>\n",
       "      <td>0.988013</td>\n",
       "      <td>0.771849</td>\n",
       "      <td>0.661123</td>\n",
       "      <td>0.987802</td>\n",
       "      <td>0.681214</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.908826</td>\n",
       "      <td>0.958653</td>\n",
       "      <td>0.704871</td>\n",
       "      <td>0.750717</td>\n",
       "      <td>0.596260</td>\n",
       "      <td>0.613973</td>\n",
       "      <td>0.579304</td>\n",
       "      <td>0.582695</td>\n",
       "      <td>-0.067935</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>188.900</td>\n",
       "      <td>272119986.0</td>\n",
       "      <td>1.817743</td>\n",
       "      <td>1.009085</td>\n",
       "      <td>1.050660</td>\n",
       "      <td>1.272425</td>\n",
       "      <td>0.006736</td>\n",
       "      <td>0.050375</td>\n",
       "      <td>0.445190</td>\n",
       "      <td>0.618434</td>\n",
       "      <td>0.997120</td>\n",
       "      <td>0.968908</td>\n",
       "      <td>1.505648</td>\n",
       "      <td>0.007971</td>\n",
       "      <td>0.105323</td>\n",
       "      <td>0.144158</td>\n",
       "      <td>0.539262</td>\n",
       "      <td>1.002220</td>\n",
       "      <td>0.992524</td>\n",
       "      <td>1.586279</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>0.110237</td>\n",
       "      <td>0.102990</td>\n",
       "      <td>1.146368</td>\n",
       "      <td>1.002639</td>\n",
       "      <td>...</td>\n",
       "      <td>1.045681</td>\n",
       "      <td>1.000126</td>\n",
       "      <td>1.001366</td>\n",
       "      <td>1.510986</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.076733</td>\n",
       "      <td>0.016113</td>\n",
       "      <td>0.102616</td>\n",
       "      <td>0.837600</td>\n",
       "      <td>1.196999</td>\n",
       "      <td>0.907093</td>\n",
       "      <td>0.836421</td>\n",
       "      <td>1.134992</td>\n",
       "      <td>0.791689</td>\n",
       "      <td>0.833995</td>\n",
       "      <td>0.862504</td>\n",
       "      <td>0.681871</td>\n",
       "      <td>0.702627</td>\n",
       "      <td>0.535038</td>\n",
       "      <td>0.554836</td>\n",
       "      <td>0.532713</td>\n",
       "      <td>0.545055</td>\n",
       "      <td>0.282463</td>\n",
       "      <td>3</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>283.700</td>\n",
       "      <td>185338220.0</td>\n",
       "      <td>2.180677</td>\n",
       "      <td>1.006883</td>\n",
       "      <td>1.156869</td>\n",
       "      <td>1.243475</td>\n",
       "      <td>0.004383</td>\n",
       "      <td>0.044750</td>\n",
       "      <td>0.438554</td>\n",
       "      <td>1.856338</td>\n",
       "      <td>1.005233</td>\n",
       "      <td>0.983717</td>\n",
       "      <td>1.338377</td>\n",
       "      <td>0.004718</td>\n",
       "      <td>0.055730</td>\n",
       "      <td>0.257166</td>\n",
       "      <td>1.829468</td>\n",
       "      <td>1.004957</td>\n",
       "      <td>1.003570</td>\n",
       "      <td>1.709517</td>\n",
       "      <td>0.006026</td>\n",
       "      <td>0.112228</td>\n",
       "      <td>0.251390</td>\n",
       "      <td>2.328634</td>\n",
       "      <td>1.003156</td>\n",
       "      <td>...</td>\n",
       "      <td>2.419453</td>\n",
       "      <td>1.001572</td>\n",
       "      <td>1.005738</td>\n",
       "      <td>1.356484</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.061583</td>\n",
       "      <td>0.189438</td>\n",
       "      <td>0.128638</td>\n",
       "      <td>0.935286</td>\n",
       "      <td>0.863594</td>\n",
       "      <td>0.756640</td>\n",
       "      <td>0.876630</td>\n",
       "      <td>0.766655</td>\n",
       "      <td>0.608037</td>\n",
       "      <td>0.824917</td>\n",
       "      <td>0.921578</td>\n",
       "      <td>0.683500</td>\n",
       "      <td>0.772665</td>\n",
       "      <td>0.577465</td>\n",
       "      <td>0.641126</td>\n",
       "      <td>0.554928</td>\n",
       "      <td>0.594481</td>\n",
       "      <td>0.297843</td>\n",
       "      <td>3</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20444</th>\n",
       "      <td>3.220</td>\n",
       "      <td>201007.0</td>\n",
       "      <td>1.885562</td>\n",
       "      <td>1.005542</td>\n",
       "      <td>1.115694</td>\n",
       "      <td>1.189781</td>\n",
       "      <td>0.369497</td>\n",
       "      <td>0.052211</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>2.379878</td>\n",
       "      <td>1.001652</td>\n",
       "      <td>1.060642</td>\n",
       "      <td>1.216418</td>\n",
       "      <td>0.377770</td>\n",
       "      <td>0.040453</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.481943</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.980255</td>\n",
       "      <td>1.305970</td>\n",
       "      <td>0.405581</td>\n",
       "      <td>0.048954</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>-0.113575</td>\n",
       "      <td>1.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678951</td>\n",
       "      <td>1.000078</td>\n",
       "      <td>1.000572</td>\n",
       "      <td>1.510986</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.075577</td>\n",
       "      <td>0.003225</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>0.910364</td>\n",
       "      <td>0.990203</td>\n",
       "      <td>1.157031</td>\n",
       "      <td>0.900621</td>\n",
       "      <td>0.925466</td>\n",
       "      <td>1.111801</td>\n",
       "      <td>0.957251</td>\n",
       "      <td>0.792624</td>\n",
       "      <td>0.733114</td>\n",
       "      <td>0.734970</td>\n",
       "      <td>0.525619</td>\n",
       "      <td>0.576481</td>\n",
       "      <td>0.492673</td>\n",
       "      <td>0.512981</td>\n",
       "      <td>0.751497</td>\n",
       "      <td>1</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20445</th>\n",
       "      <td>8.150</td>\n",
       "      <td>353777.0</td>\n",
       "      <td>3.633179</td>\n",
       "      <td>1.011299</td>\n",
       "      <td>1.928957</td>\n",
       "      <td>1.739130</td>\n",
       "      <td>0.213390</td>\n",
       "      <td>0.048547</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>3.958643</td>\n",
       "      <td>1.006857</td>\n",
       "      <td>1.211992</td>\n",
       "      <td>1.834862</td>\n",
       "      <td>0.225136</td>\n",
       "      <td>0.056907</td>\n",
       "      <td>0.220513</td>\n",
       "      <td>3.416976</td>\n",
       "      <td>1.005887</td>\n",
       "      <td>1.029586</td>\n",
       "      <td>2.202643</td>\n",
       "      <td>0.270263</td>\n",
       "      <td>0.073606</td>\n",
       "      <td>0.202199</td>\n",
       "      <td>3.086596</td>\n",
       "      <td>1.003762</td>\n",
       "      <td>...</td>\n",
       "      <td>2.503607</td>\n",
       "      <td>1.000612</td>\n",
       "      <td>1.005188</td>\n",
       "      <td>1.147137</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.025011</td>\n",
       "      <td>0.086095</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>0.885522</td>\n",
       "      <td>0.794143</td>\n",
       "      <td>0.579914</td>\n",
       "      <td>0.815951</td>\n",
       "      <td>0.736196</td>\n",
       "      <td>0.561963</td>\n",
       "      <td>0.912015</td>\n",
       "      <td>0.924778</td>\n",
       "      <td>0.737462</td>\n",
       "      <td>0.773966</td>\n",
       "      <td>0.614021</td>\n",
       "      <td>0.643285</td>\n",
       "      <td>0.574271</td>\n",
       "      <td>0.600901</td>\n",
       "      <td>0.075949</td>\n",
       "      <td>0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20446</th>\n",
       "      <td>7.100</td>\n",
       "      <td>22308.0</td>\n",
       "      <td>3.822402</td>\n",
       "      <td>1.005084</td>\n",
       "      <td>2.256391</td>\n",
       "      <td>1.153226</td>\n",
       "      <td>0.162426</td>\n",
       "      <td>0.024872</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>4.088019</td>\n",
       "      <td>1.002818</td>\n",
       "      <td>1.215204</td>\n",
       "      <td>1.162602</td>\n",
       "      <td>0.163747</td>\n",
       "      <td>0.023945</td>\n",
       "      <td>0.145161</td>\n",
       "      <td>3.799314</td>\n",
       "      <td>1.001113</td>\n",
       "      <td>1.081142</td>\n",
       "      <td>1.288288</td>\n",
       "      <td>0.181449</td>\n",
       "      <td>0.029805</td>\n",
       "      <td>0.078652</td>\n",
       "      <td>0.906580</td>\n",
       "      <td>0.999659</td>\n",
       "      <td>...</td>\n",
       "      <td>1.934217</td>\n",
       "      <td>1.000267</td>\n",
       "      <td>1.007592</td>\n",
       "      <td>1.254132</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.045850</td>\n",
       "      <td>0.025410</td>\n",
       "      <td>0.003227</td>\n",
       "      <td>0.978107</td>\n",
       "      <td>0.937329</td>\n",
       "      <td>0.939241</td>\n",
       "      <td>0.908451</td>\n",
       "      <td>0.866197</td>\n",
       "      <td>0.852113</td>\n",
       "      <td>0.924929</td>\n",
       "      <td>0.916377</td>\n",
       "      <td>0.727160</td>\n",
       "      <td>0.762488</td>\n",
       "      <td>0.540029</td>\n",
       "      <td>0.584495</td>\n",
       "      <td>0.506421</td>\n",
       "      <td>0.520755</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0</td>\n",
       "      <td>378.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20447</th>\n",
       "      <td>1.398</td>\n",
       "      <td>35958231.0</td>\n",
       "      <td>3.230458</td>\n",
       "      <td>1.010494</td>\n",
       "      <td>1.514962</td>\n",
       "      <td>1.349057</td>\n",
       "      <td>0.964990</td>\n",
       "      <td>0.045171</td>\n",
       "      <td>0.259494</td>\n",
       "      <td>2.502462</td>\n",
       "      <td>1.003584</td>\n",
       "      <td>1.065199</td>\n",
       "      <td>1.349057</td>\n",
       "      <td>0.964990</td>\n",
       "      <td>0.047404</td>\n",
       "      <td>0.139785</td>\n",
       "      <td>2.174580</td>\n",
       "      <td>1.000383</td>\n",
       "      <td>1.013434</td>\n",
       "      <td>1.349057</td>\n",
       "      <td>0.964990</td>\n",
       "      <td>0.047933</td>\n",
       "      <td>0.021311</td>\n",
       "      <td>1.072893</td>\n",
       "      <td>1.001655</td>\n",
       "      <td>...</td>\n",
       "      <td>2.229635</td>\n",
       "      <td>1.001465</td>\n",
       "      <td>1.006487</td>\n",
       "      <td>1.361214</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.065627</td>\n",
       "      <td>0.201038</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.845056</td>\n",
       "      <td>0.963428</td>\n",
       "      <td>1.117262</td>\n",
       "      <td>0.822604</td>\n",
       "      <td>0.842632</td>\n",
       "      <td>0.951359</td>\n",
       "      <td>0.924829</td>\n",
       "      <td>0.935651</td>\n",
       "      <td>0.701462</td>\n",
       "      <td>0.733584</td>\n",
       "      <td>0.533868</td>\n",
       "      <td>0.581280</td>\n",
       "      <td>0.500654</td>\n",
       "      <td>0.536594</td>\n",
       "      <td>-0.097222</td>\n",
       "      <td>0</td>\n",
       "      <td>518.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20448</th>\n",
       "      <td>0.976</td>\n",
       "      <td>7746202.0</td>\n",
       "      <td>0.376892</td>\n",
       "      <td>0.994564</td>\n",
       "      <td>0.964171</td>\n",
       "      <td>1.585915</td>\n",
       "      <td>1.624913</td>\n",
       "      <td>0.116625</td>\n",
       "      <td>0.247350</td>\n",
       "      <td>-0.681756</td>\n",
       "      <td>0.996602</td>\n",
       "      <td>1.000483</td>\n",
       "      <td>1.723944</td>\n",
       "      <td>1.766336</td>\n",
       "      <td>0.137543</td>\n",
       "      <td>0.199580</td>\n",
       "      <td>-1.262439</td>\n",
       "      <td>0.998189</td>\n",
       "      <td>1.007152</td>\n",
       "      <td>1.859155</td>\n",
       "      <td>1.904872</td>\n",
       "      <td>0.120506</td>\n",
       "      <td>0.115132</td>\n",
       "      <td>-1.725790</td>\n",
       "      <td>0.999250</td>\n",
       "      <td>...</td>\n",
       "      <td>2.261777</td>\n",
       "      <td>1.000597</td>\n",
       "      <td>1.000879</td>\n",
       "      <td>1.146077</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.025169</td>\n",
       "      <td>0.077248</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.180516</td>\n",
       "      <td>1.243851</td>\n",
       "      <td>1.244295</td>\n",
       "      <td>1.104508</td>\n",
       "      <td>1.186475</td>\n",
       "      <td>1.209016</td>\n",
       "      <td>0.924377</td>\n",
       "      <td>0.943839</td>\n",
       "      <td>0.534052</td>\n",
       "      <td>0.588732</td>\n",
       "      <td>0.411903</td>\n",
       "      <td>0.441595</td>\n",
       "      <td>0.441272</td>\n",
       "      <td>0.458183</td>\n",
       "      <td>-0.075805</td>\n",
       "      <td>1</td>\n",
       "      <td>239.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20449 rows Ã— 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       close_stock  volume_cash_stock  zs-20-close_stock  ma-slope-20-close_stock  std-slope-20-close_stock  rng-20_stock  percent-rng-20-close_stock  percent-std-20-close_stock  kaufmanns_efficiency_ratio-20_stock  zs-50-close_stock  ma-slope-50-close_stock  std-slope-50-close_stock  rng-50_stock  percent-rng-50-close_stock  percent-std-50-close_stock  kaufmanns_efficiency_ratio-50_stock  zs-100-close_stock  ma-slope-100-close_stock  std-slope-100-close_stock  rng-100_stock  percent-rng-100-close_stock  percent-std-100-close_stock  kaufmanns_efficiency_ratio-100_stock  zs-200-close_stock  ma-slope-200-close_stock  ...  zs-200-close_omx  ma-slope-200-close_omx  std-slope-200-close_omx  rng-200_omx  percent-rng-200-close_omx  percent-std-200-close_omx  kaufmanns_efficiency_ratio-200_omx  stock_quota  stock_hist_relative_perf20  stock_hist_relative_perf50  stock_hist_relative_perf100  stock_hist_perf20  stock_hist_perf50  stock_hist_perf100  stock_relative_rsi_3  stock_rsi_3  \\\n",
       "0          104.800         96308260.0           2.406278                 1.008411                  1.206170      1.233333                    0.011768                    0.049977                             0.666667                NaN                      NaN                       NaN           NaN                         NaN                         NaN                                  NaN                 NaN                       NaN                        NaN            NaN                          NaN                          NaN                                   NaN                 NaN                       NaN  ...          2.550730                1.000816                 1.008874     1.205161                   0.000683                   0.034478                            0.121085     0.059427                    0.910515                         NaN                          NaN           0.853244                NaN                 NaN              0.972445     0.982223   \n",
       "1          169.900        358482267.0           2.570386                 1.012324                  1.023911      1.277528                    0.007519                    0.041544                             0.263646           2.138433                 1.010411                  1.007476      1.849783                    0.010887                    0.097403                             0.320304            2.087838                  1.003921                   1.019671       2.210915                     0.013013                     0.105117                              0.110061                 NaN                       NaN  ...          0.316113                1.000618                 0.996687     1.510986                   0.000889                   0.079502                            0.047553     0.099913                    0.874627                    0.742786                     0.662212           0.782519           0.592113            0.695115              0.844962     0.938508   \n",
       "2          184.450        165471292.0           2.339982                 1.000686                  1.031583      1.262564                    0.006845                    0.047219                             0.083603           2.121880                 1.007725                  1.012728      1.526664                    0.008277                    0.079306                             0.245033            1.999881                  1.003812                   1.019996       2.393700                     0.012977                     0.117752                              0.101908                 NaN                       NaN  ...          0.475301                1.000196                 1.000570     1.510986                   0.000873                   0.076064                            0.018926     0.106564                    0.988013                    0.771849                     0.661123           0.987802           0.681214            0.709677              0.908826     0.958653   \n",
       "3          188.900        272119986.0           1.817743                 1.009085                  1.050660      1.272425                    0.006736                    0.050375                             0.445190           0.618434                 0.997120                  0.968908      1.505648                    0.007971                    0.105323                             0.144158            0.539262                  1.002220                   0.992524       1.586279                     0.008397                     0.110237                              0.102990            1.146368                  1.002639  ...          1.045681                1.000126                 1.001366     1.510986                   0.000821                   0.076733                            0.016113     0.102616                    0.837600                    1.196999                     0.907093           0.836421           1.134992            0.791689              0.833995     0.862504   \n",
       "4          283.700        185338220.0           2.180677                 1.006883                  1.156869      1.243475                    0.004383                    0.044750                             0.438554           1.856338                 1.005233                  0.983717      1.338377                    0.004718                    0.055730                             0.257166            1.829468                  1.004957                   1.003570       1.709517                     0.006026                     0.112228                              0.251390            2.328634                  1.003156  ...          2.419453                1.001572                 1.005738     1.356484                   0.000615                   0.061583                            0.189438     0.128638                    0.935286                    0.863594                     0.756640           0.876630           0.766655            0.608037              0.824917     0.921578   \n",
       "...            ...                ...                ...                      ...                       ...           ...                         ...                         ...                                  ...                ...                      ...                       ...           ...                         ...                         ...                                  ...                 ...                       ...                        ...            ...                          ...                          ...                                   ...                 ...                       ...  ...               ...                     ...                      ...          ...                        ...                        ...                                 ...          ...                         ...                         ...                          ...                ...                ...                 ...                   ...          ...   \n",
       "20444        3.220           201007.0           1.885562                 1.005542                  1.115694      1.189781                    0.369497                    0.052211                             0.396226           2.379878                 1.001652                  1.060642      1.216418                    0.377770                    0.040453                             0.100000            1.481943                  0.999465                   0.980255       1.305970                     0.405581                     0.048954                              0.056000           -0.113575                  1.000215  ...          0.678951                1.000078                 1.000572     1.510986                   0.000852                   0.075577                            0.003225     0.001816                    0.910364                    0.990203                     1.157031           0.900621           0.925466            1.111801              0.957251     0.792624   \n",
       "20445        8.150           353777.0           3.633179                 1.011299                  1.928957      1.739130                    0.213390                    0.048547                             0.404762           3.958643                 1.006857                  1.211992      1.834862                    0.225136                    0.056907                             0.220513            3.416976                  1.005887                   1.029586       2.202643                     0.270263                     0.073606                              0.202199            3.086596                  1.003762  ...          2.503607                1.000612                 1.005188     1.147137                   0.000467                   0.025011                            0.086095     0.003318                    0.885522                    0.794143                     0.579914           0.815951           0.736196            0.561963              0.912015     0.924778   \n",
       "20446        7.100            22308.0           3.822402                 1.005084                  2.256391      1.153226                    0.162426                    0.024872                             0.333333           4.088019                 1.002818                  1.215204      1.162602                    0.163747                    0.023945                             0.145161            3.799314                  1.001113                   1.081142       1.288288                     0.181449                     0.029805                              0.078652            0.906580                  0.999659  ...          1.934217                1.000267                 1.007592     1.254132                   0.000570                   0.045850                            0.025410     0.003227                    0.978107                    0.937329                     0.939241           0.908451           0.866197            0.852113              0.924929     0.916377   \n",
       "20447        1.398         35958231.0           3.230458                 1.010494                  1.514962      1.349057                    0.964990                    0.045171                             0.259494           2.502462                 1.003584                  1.065199      1.349057                    0.964990                    0.047404                             0.139785            2.174580                  1.000383                   1.013434       1.349057                     0.964990                     0.047933                              0.021311            1.072893                  1.001655  ...          2.229635                1.001465                 1.006487     1.361214                   0.000611                   0.065627                            0.201038     0.000627                    0.845056                    0.963428                     1.117262           0.822604           0.842632            0.951359              0.924829     0.935651   \n",
       "20448        0.976          7746202.0           0.376892                 0.994564                  0.964171      1.585915                    1.624913                    0.116625                             0.247350          -0.681756                 0.996602                  1.000483      1.723944                    1.766336                    0.137543                             0.199580           -1.262439                  0.998189                   1.007152       1.859155                     1.904872                     0.120506                              0.115132           -1.725790                  0.999250  ...          2.261777                1.000597                 1.000879     1.146077                   0.000469                   0.025169                            0.077248     0.000400                    1.180516                    1.243851                     1.244295           1.104508           1.186475            1.209016              0.924377     0.943839   \n",
       "\n",
       "       stock_relative_rsi_10  stock_rsi_10  stock_relative_rsi_34  stock_rsi_34  stock_relative_rsi_100  stock_rsi_100    result  trades_last_year  days_since_last_signal  \n",
       "0                   0.811284      0.867440               0.574894      0.654295                     NaN            NaN  0.334595                 0                     NaN  \n",
       "1                   0.630043      0.713423               0.582931      0.604268                0.576904       0.579589 -0.134604                 1                    94.0  \n",
       "2                   0.704871      0.750717               0.596260      0.613973                0.579304       0.582695 -0.067935                 2                    25.0  \n",
       "3                   0.681871      0.702627               0.535038      0.554836                0.532713       0.545055  0.282463                 3                    55.0  \n",
       "4                   0.683500      0.772665               0.577465      0.641126                0.554928       0.594481  0.297843                 3                    33.0  \n",
       "...                      ...           ...                    ...           ...                     ...            ...       ...               ...                     ...  \n",
       "20444               0.733114      0.734970               0.525619      0.576481                0.492673       0.512981  0.751497                 1                   160.0  \n",
       "20445               0.737462      0.773966               0.614021      0.643285                0.574271       0.600901  0.075949                 0                    37.0  \n",
       "20446               0.727160      0.762488               0.540029      0.584495                0.506421       0.520755  0.035714                 0                   378.0  \n",
       "20447               0.701462      0.733584               0.533868      0.581280                0.500654       0.536594 -0.097222                 0                   518.0  \n",
       "20448               0.534052      0.588732               0.411903      0.441595                0.441272       0.458183 -0.075805                 1                   239.0  \n",
       "\n",
       "[20449 rows x 86 columns]"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "tradedf = pd.DataFrame()\n",
    "stockdf = pd.DataFrame()\n",
    "\n",
    "try: \n",
    "  df = pd.read_csv('stockytrades.csv')\n",
    "except:\n",
    "  print('Failed to read file')\n",
    "  docs = db.collection('trades').stream()\n",
    "\n",
    "  for doc in docs:\n",
    "    print('starting', doc.id)\n",
    "    stockdf = get_price_data(doc.id)\n",
    "    tradedf =  get_trade_data(doc)\n",
    "    merged_df = pd.merge(stockdf, omxdf, on='date', suffixes=('_stock', '_omx'))\n",
    "    merged_df = pd.merge(merged_df, merge_index_stock_df(omxdf, stockdf), on='date')\n",
    "\n",
    "    \n",
    "    # merged_df = pd.merge(merged_df, tradedf, on='date', suffixes=('_1', '_2'))\n",
    "    merged_df = pd.merge(merged_df, tradedf, on='date')\n",
    "    merged_df.drop(columns=['owners'], inplace = True, errors='ignore')\n",
    "    \n",
    "    df = pd.concat([df, merged_df], ignore_index=True)\n",
    "  \n",
    "  # Save the file so we dont have to next time\n",
    "  # Drop problematic columns.\n",
    "  df.to_csv('stockytrades.csv')\n",
    "\n",
    "df.drop(columns=unwanted_columns, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "print(\"have\", len(df))\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "std-slope-200-close_stock               2335\n",
       "ma-slope-200-close_stock                2334\n",
       "percent-std-200-close_stock             2324\n",
       "kaufmanns_efficiency_ratio-200_stock    2324\n",
       "rng-200_stock                           2324\n",
       "percent-rng-200-close_stock             2324\n",
       "zs-200-close_stock                      2324\n",
       "std-slope-50-volume_stock               1367\n",
       "ma-slope-50-volume_stock                1367\n",
       "percent-std-50-volume_stock             1344\n",
       "zs-50-volume_stock                      1344\n",
       "days_since_last_signal                  1288\n",
       "stock_rsi_100                           1274\n",
       "stock_relative_rsi_100                  1274\n",
       "stock_hist_perf100                      1274\n",
       "stock_hist_relative_perf100             1274\n",
       "ma-slope-100-close_stock                1272\n",
       "std-slope-100-close_stock               1272\n",
       "zs-100-close_stock                      1260\n",
       "rng-100_stock                           1260\n",
       "percent-rng-100-close_stock             1260\n",
       "percent-std-100-close_stock             1260\n",
       "kaufmanns_efficiency_ratio-100_stock    1260\n",
       "std-slope-20-volume_stock                750\n",
       "ma-slope-20-volume_stock                 749\n",
       "percent-rng-50-volume_stock              746\n",
       "stock_hist_perf50                        738\n",
       "stock_hist_relative_perf50               738\n",
       "std-slope-50-close_stock                 737\n",
       "ma-slope-50-close_stock                  737\n",
       "zs-20-volume_stock                       734\n",
       "percent-std-20-volume_stock              733\n",
       "percent-std-50-close_stock               725\n",
       "zs-50-close_stock                        725\n",
       "kaufmanns_efficiency_ratio-50_stock      725\n",
       "rng-50_stock                             725\n",
       "percent-rng-50-close_stock               725\n",
       "stock_relative_rsi_34                    543\n",
       "stock_rsi_34                             543\n",
       "percent-rng-20-volume_stock              372\n",
       "std-slope-20-close_stock                 370\n",
       "stock_hist_perf20                        362\n",
       "stock_hist_relative_perf20               362\n",
       "ma-slope-20-close_stock                  361\n",
       "zs-20-close_stock                        351\n",
       "rng-20_stock                             351\n",
       "kaufmanns_efficiency_ratio-20_stock      351\n",
       "percent-std-20-close_stock               351\n",
       "percent-rng-20-close_stock               351\n",
       "stock_rsi_10                             212\n",
       "stock_relative_rsi_10                    212\n",
       "stock_rsi_3                               80\n",
       "stock_relative_rsi_3                      80\n",
       "volume_cash_stock                         21\n",
       "result                                     0\n",
       "trades_last_year                           0\n",
       "std-slope-200-close_omx                    0\n",
       "stock_quota                                0\n",
       "kaufmanns_efficiency_ratio-200_omx         0\n",
       "percent-std-200-close_omx                  0\n",
       "percent-rng-200-close_omx                  0\n",
       "rng-200_omx                                0\n",
       "close_stock                                0\n",
       "ma-slope-200-close_omx                     0\n",
       "rng-50_omx                                 0\n",
       "zs-20-close_omx                            0\n",
       "ma-slope-20-close_omx                      0\n",
       "std-slope-20-close_omx                     0\n",
       "percent-rng-20-close_omx                   0\n",
       "percent-std-20-close_omx                   0\n",
       "kaufmanns_efficiency_ratio-20_omx          0\n",
       "zs-50-close_omx                            0\n",
       "ma-slope-50-close_omx                      0\n",
       "std-slope-50-close_omx                     0\n",
       "percent-rng-50-close_omx                   0\n",
       "zs-200-close_omx                           0\n",
       "percent-std-50-close_omx                   0\n",
       "kaufmanns_efficiency_ratio-50_omx          0\n",
       "zs-100-close_omx                           0\n",
       "ma-slope-100-close_omx                     0\n",
       "std-slope-100-close_omx                    0\n",
       "rng-100_omx                                0\n",
       "percent-rng-100-close_omx                  0\n",
       "percent-std-100-close_omx                  0\n",
       "kaufmanns_efficiency_ratio-100_omx         0\n",
       "rng-20_omx                                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trades: 17179\n"
     ]
    }
   ],
   "source": [
    "# Clean the data that cannot be used for training\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "print(\"Total number of trades:\", len(df))\n",
    "\n",
    "\n",
    "# Add the label we want to predict\n",
    "df['label'] = np.where(df['result'] > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.034287729605589 89.56603773584905\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdIAAAE/CAYAAADyukJqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQJElEQVR4nO3de6xlB1XH8d+yIyKgLdArKS0wjaBYJSpOEKgiUiVo1TYRtI2ago2NRsWKUeorqCER4gPxEbWCWhUpUkyKYEBSiy9KZfrQQqvQ8JBCkSvQCmjU4vKPc0ZupzOd21n3zj0z8/kkzT3vs87Zs+939t5nTqu7AwAcns/Y6QEA4GgmpAAwIKQAMCCkADAgpAAwIKQAMLDrSD7ZySef3Lt37z6STwkAY9ddd92/dffaga47oiHdvXt39u7deySfEgDGqup9B7vOrl0AGBBSABgQUgAYEFIAGBBSABgQUgAYEFIAGBBSABgQUgAYEFIAGBBSABg4ot+1y/Fj9yWv3+kRjgnvfdHZOz0CcAi2SAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYGBTIa2qH66qd1TV26vqlVV1/6o6vaqurapbq+pVVXW/7R4WAFbNIUNaVacmeW6SPd39JUlOSHJekhcneUl3PzrJx5JcuJ2DAsAq2uyu3V1JPruqdiV5QJLbkzwtyRXL6y9Lcu6WTwcAK+6QIe3uDyT5xST/kkVA70xyXZI7uvuu5c1uS3Lqdg0JAKtqM7t2H5zknCSnJ3l4kgcmecZmn6CqLqqqvVW1d319/bAHBYBVtJldu1+X5D3dvd7d/5PkT5OcmeSk5a7eJDktyQcOdOfuvrS793T3nrW1tS0ZGgBWxWZC+i9JnlhVD6iqSnJWkpuTXJ3kmcvbXJDkyu0ZEQBW12aOkV6bxYeKrk9y0/I+lyZ5fpLnVdWtSR6a5OXbOCcArKRdh75J0t0vSPKC/S5+d5InbPlEAHAU8c1GADAgpAAwIKQAMCCkADAgpAAwIKQAMCCkADAgpAAwIKQAMCCkADAgpAAwIKQAMCCkADAgpAAwIKQAMCCkADAgpAAwIKQAMCCkADAgpAAwIKQAMCCkADAgpAAwIKQAMCCkADAgpAAwIKQAMCCkADAgpAAwIKQAMCCkADAgpAAwIKQAMCCkADAgpAAwIKQAMCCkADAgpAAwIKQAMCCkADAgpAAwIKQAMCCkADAgpAAwIKQAMCCkADAgpAAwIKQAMCCkADCwqZBW1UlVdUVV/VNV3VJVT6qqh1TVm6rqXcufD97uYQFg1Wx2i/SlSd7Q3Y9N8qVJbklySZKruvsxSa5angeA48ohQ1pVJyZ5SpKXJ0l3/3d335HknCSXLW92WZJzt2dEAFhdm9kiPT3JepLfq6obquplVfXAJA/r7tuXt/lQkocd6M5VdVFV7a2qvevr61szNQCsiM2EdFeSxyf5ze7+8iSfzH67cbu7k/SB7tzdl3b3nu7es7a2Np0XAFbKZkJ6W5Lbuvva5fkrsgjrv1bVKUmy/Pnh7RkRAFbXIUPa3R9K8v6q+sLlRWcluTnJa5NcsLzsgiRXbsuEALDCdm3ydj+Y5BVVdb8k707ynCwi/CdVdWGS9yX5tu0ZEQBW16ZC2t03JtlzgKvO2tJpAOAo45uNAGBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBASAFgQEgBYEBIAWBg0yGtqhOq6oaqet3y/OlVdW1V3VpVr6qq+23fmACwmu7LFukPJbllw/kXJ3lJdz86yceSXLiVgwHA0WBTIa2q05KcneRly/OV5GlJrlje5LIk527DfACw0ja7RforSX4syf8uzz80yR3dfdfy/G1JTj3QHavqoqraW1V719fXJ7MCwMo5ZEir6puSfLi7rzucJ+juS7t7T3fvWVtbO5yHAICVtWsTtzkzybdU1TcmuX+Sz03y0iQnVdWu5VbpaUk+sH1jAsBqOuQWaXf/eHef1t27k5yX5C+7+zuSXJ3kmcubXZDkym2bEgBW1OTfkT4/yfOq6tYsjpm+fGtGAoCjx2Z27f6/7n5zkjcvT787yRO2fiQAOHr4ZiMAGBBSABgQUgAYEFIAGBBSABgQUgAYEFIAGBBSABgQUgAYEFIAGBBSABgQUgAYEFIAGBBSABgQUgAYEFIAGBBSABgQUgAYEFIAGBBSABgQUgAYEFIAGBBSABgQUgAYEFIAGBBSABgQUgAYEFIAGBBSABgQUgAYEFIAGBBSABgQUgAYEFIAGBBSABgQUgAYEFIAGBBSABgQUgAYEFIAGBBSABgQUgAYEFIAGBBSABgQUgAYEFIAGBBSABgQUgAYOGRIq+oRVXV1Vd1cVe+oqh9aXv6QqnpTVb1r+fPB2z8uAKyWzWyR3pXkR7r7jCRPTPL9VXVGkkuSXNXdj0ly1fI8ABxXDhnS7r69u69fnv54kluSnJrknCSXLW92WZJzt2lGAFhZ9+kYaVXtTvLlSa5N8rDuvn151YeSPGxrRwOA1bfpkFbVg5K8JsnF3f3vG6/r7k7SB7nfRVW1t6r2rq+vj4YFgFWzqZBW1WdmEdFXdPefLi/+16o6ZXn9KUk+fKD7dvel3b2nu/esra1txcwAsDI286ndSvLyJLd09y9vuOq1SS5Ynr4gyZVbPx4ArLZdm7jNmUm+K8lNVXXj8rKfSPKiJH9SVRcmeV+Sb9uWCQFghR0ypN39t0nqIFeftbXjAMDRxTcbAcCAkALAgJACwICQAsCAkALAgJACwICQAsCAkALAgJACwICQAsCAkALAgJACwICQAsCAkALAgJACwICQAsCAkALAgJACwICQAsCAkALAgJACwICQAsCAkALAgJACwICQAsCAkALAgJACwICQAsCAkALAgJACwICQAsCAkALAgJACwICQAsCAkALAgJACwICQAsCAkALAgJACwICQAsCAkALAgJACwICQAsCAkALAgJACwICQAsCAkALAgJACwICQAsDAKKRV9Yyq+uequrWqLtmqoQDgaHHYIa2qE5L8RpJvSHJGkvOr6oytGgwAjga7Bvd9QpJbu/vdSVJVlyc5J8nNWzEYwPFo9yWv3+kRjgnvfdHZR+y5Jrt2T03y/g3nb1teBgDHjckW6aZU1UVJLkqSRz7ykdv9dKyII/m3Qe4bWzxbZzv+nFt3jj6TLdIPJHnEhvOnLS+7m+6+tLv3dPeetbW1wdMBwOqZhPRtSR5TVadX1f2SnJfktVszFgAcHQ57125331VVP5DkjUlOSPK73f2OLZsM2BZ2HcLWGh0j7e4/T/LnWzQLABx1fLMRAAwIKQAMCCkADAgpAAwIKQAMCCkADAgpAAwIKQAMCCkADAgpAAwIKQAMVHcfuSerWk/yviP2hKvh5CT/ttNDcECWzeqybFbb8bh8HtXdB/x/gR7RkB6Pqmpvd+/Z6Tm4J8tmdVk2q83yuTu7dgFgQEgBYEBIt9+lOz0AB2XZrC7LZrVZPhs4RgoAA7ZIAWBASA+gqs6tqq6qx+70LNx3VfWpqrqxqv6hqq6vqicf5uNcXFUP2Or5jjdV9ZKqunjD+TdW1cs2nP+lqnre4PGferjLmPvm3taJqnp2Vf36kZ5pFQjpgZ2f5G+XP0eq6oT5ONxH/9ndX9bdX5rkx5P8/GE+zsVJhHTu75I8OUmq6jOy+DeIX7zh+icnecuhHuRe1qWn7nt8tt3FsU7cg5Dup6oelOSrklyY5LyqekZVvXrD9U+tqtctTz+9qq5ZbvW8ennfVNV7q+rFVXV9kmdV1fdU1duWW0iv2fc3uqr6/Kp6a1XdVFUvrKpPbHieH13e5x+r6meP5HtwjPncJB/bd+ZA72tVPbCqXr9cPm+vqm+vqucmeXiSq6vq6h2a/VjxliRPWp7+4iRvT/LxqnpwVX1Wki9KcmJV3bBcF353efmB1qXnVtXNy+V3eVXtTvK9SX54uRfiq4/8yzs2HWC9eEH2Wyeq6jlV9c6q+vskZ+7owDto104PsILOSfKG7n5nVX0ki1/CX1lVD+zuTyb59iSXV9XJSX4qydd19yer6vlJnpfk55aP85HufnySVNVDu/t3lqdfmEWkfy3JS5O8tLtfWVXfu2+Aqnp6ksckeUKSSvLaqnpKd//19r/8Y8JnV9WNSe6f5JQkT0sO/r4mWUvywe4+e3m7E7v7zuXuxq/t7uPtG1y2VHd/sKruqqpHZrHleE2SU7OI651J3pXkZUnOWq53f5Dk+5L8yvIhNq5LH0xyenf/V1Wd1N13VNVvJflEd//ikX1lx7xnZL/1IslzslwnquqUJD+b5CuyWI5XJ7lhp4bdSbZI7+n8JJcvT1+e5FlJ3pDkm6tqV5Kzk1yZ5IlJzkjyd8tf2hckedSGx3nVhtNfUlV/U1U3JfmOfHq31pOS7Nva/eMNt3/68r8bklyf5LFZBIDN2bdr97FZ/DL4g6qqHPx9vSnJ1y+3fL66u+/cqcGPYW/JIqL7QnrNhvO3JXlPd79zedvLkjxlw303rkv/mOQVVfWdSe7a7qGPc4daL74yyZu7e727/zt3X07HFVukG1TVQ7LYenlcVXWSE5J0Fn8L+/4kH02yt7s/vvzF/KbuPthx1E9uOP37Sc7t7n+oqmdncUznXkdJ8vPd/duH+1pY6O5rlnsP1nIv72tVPT7JNyZ5YVVd1d0/t/9tGNl3nPRxWezafX+SH0ny70nenORb7+W+G9els7OI7Dcn+cmqetx2DEuy3Dtwt/Vip2daVbZI7+6ZSf6wux/V3bu7+xFJ3pPF33wfn+R78umt1bcmObOqHp38//GELzjI435Oktur6jOz2CLd56359C+Q8zZc/sYk373hmOupVfV585d3/KnFJ69PSPKRHOR9raqHJ/mP7v6jJL+QxbJOko9nseyYe0uSb0ry0e7+VHd/NMlJWeyVeU2S3fvWpSTfleSv9n+AWnxQ6RHdfXWS5yc5McmDYjlti4OsFxvf62uTfE1VPXT5u+1ZOzPpzrNFenfnJ3nxfpe9JovIvS7Js7PYhZvuXl9uXb5y3wcjsjhm+s7c009n8Ydufflz3x/Ei5P8UVX9ZBa7j+9cPvZfVNUXJblmseGbTyT5ziQfnr7A48S+Y6TJYiv0gu7+VJKDva+PTvILVfW/Sf4ni+NzyeLbW95QVR/s7q89ki/gGHRTFp/W/eP9LntQd99WVc9J8url4ZO3JfmtAzzGCVmsLydmsVx/dXmM9M+SXFFV5yT5we7+m219JcePx+We68WTsmGdqKqfyWI3/R1JbtyhOXecbzbaQbX49O5/dndX1XlJzu/uc3Z6LgA2zxbpzvqKJL++PN56R5Lv3tlxALivbJECwIAPGwHAgJACwICQAsCAkALAgJACwICQAsDA/wFBjVWvVRGiUgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 3.4287729605589%\n",
      "Best 8956.603773584906%\n",
      "Worst -98.48192771084338%\n",
      "std 78.62588432068087%\n"
     ]
    }
   ],
   "source": [
    "#plot distribution of points by team \n",
    "avg = df['result'].mean()\n",
    "best = df['result'].max()\n",
    "worst = df['result'].min()\n",
    "std = df['result'].std()\n",
    "\n",
    "print(avg, best)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "stats = ['Average', 'Best', 'Worst', 'std']\n",
    "columns = [avg, best, worst, std]\n",
    "ax.bar(stats, columns)\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(stats)):\n",
    "  print(stats[i], f'{columns[i]*100}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning part\n",
    "\n",
    "- Drop all `NaN` values first. \n",
    "- Then split the dataset for test and training (using K-fold?). \n",
    "- Train the model\n",
    "- Create a confusion matrix on the validation data. Compare results with reality\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "tf.autograph.set_verbosity(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "train_data, validation_data =  train_test_split(df, test_size=0.15, random_state=3456) \n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = train_data.drop(columns=['result', 'label'], axis=1).values\n",
    "y = train_data['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['close_stock', 'volume_cash_stock', 'zs-20-close_stock', 'ma-slope-20-close_stock', 'std-slope-20-close_stock', 'rng-20_stock', 'percent-rng-20-close_stock', 'percent-std-20-close_stock', 'kaufmanns_efficiency_ratio-20_stock', 'zs-50-close_stock', 'ma-slope-50-close_stock', 'std-slope-50-close_stock', 'rng-50_stock', 'percent-rng-50-close_stock', 'percent-std-50-close_stock', 'kaufmanns_efficiency_ratio-50_stock', 'zs-100-close_stock', 'ma-slope-100-close_stock', 'std-slope-100-close_stock', 'rng-100_stock', 'percent-rng-100-close_stock', 'percent-std-100-close_stock', 'kaufmanns_efficiency_ratio-100_stock', 'zs-200-close_stock', 'ma-slope-200-close_stock', 'std-slope-200-close_stock', 'rng-200_stock', 'percent-rng-200-close_stock', 'percent-std-200-close_stock', 'kaufmanns_efficiency_ratio-200_stock', 'zs-20-volume_stock', 'ma-slope-20-volume_stock', 'std-slope-20-volume_stock', 'percent-rng-20-volume_stock', 'percent-std-20-volume_stock', 'zs-50-volume_stock', 'ma-slope-50-volume_stock', 'std-slope-50-volume_stock', 'percent-rng-50-volume_stock', 'percent-std-50-volume_stock', 'zs-20-close_omx', 'ma-slope-20-close_omx', 'std-slope-20-close_omx', 'rng-20_omx', 'percent-rng-20-close_omx', 'percent-std-20-close_omx', 'kaufmanns_efficiency_ratio-20_omx', 'zs-50-close_omx', 'ma-slope-50-close_omx', 'std-slope-50-close_omx', 'rng-50_omx', 'percent-rng-50-close_omx', 'percent-std-50-close_omx', 'kaufmanns_efficiency_ratio-50_omx', 'zs-100-close_omx', 'ma-slope-100-close_omx', 'std-slope-100-close_omx', 'rng-100_omx', 'percent-rng-100-close_omx', 'percent-std-100-close_omx', 'kaufmanns_efficiency_ratio-100_omx', 'zs-200-close_omx', 'ma-slope-200-close_omx', 'std-slope-200-close_omx', 'rng-200_omx', 'percent-rng-200-close_omx', 'percent-std-200-close_omx', 'kaufmanns_efficiency_ratio-200_omx', 'stock_quota', 'stock_hist_relative_perf20', 'stock_hist_relative_perf50', 'stock_hist_relative_perf100', 'stock_hist_perf20', 'stock_hist_perf50', 'stock_hist_perf100', 'stock_relative_rsi_3', 'stock_rsi_3', 'stock_relative_rsi_10', 'stock_rsi_10', 'stock_relative_rsi_34', 'stock_rsi_34', 'stock_relative_rsi_100', 'stock_rsi_100', 'result', 'trades_last_year', 'days_since_last_signal', 'label']\n"
     ]
    }
   ],
   "source": [
    "# K-fold\n",
    "num_folds = 7\n",
    "# Define the k-fold cross-validator\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "\n",
    "print(df.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize input data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define TensorFlow model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X.shape[1],)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    # tf.keras.layers.BatchNormalization(),\n",
    "    # tf.keras.layers.BatchNormalization(),\n",
    "    # tf.keras.layers.Dropout(0.5),\n",
    "    # tf.keras.layers.Dense(32, activation='relu'),\n",
    "    # tf.keras.layers.BatchNormalization(),\n",
    "    # tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bauhn\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15)\n",
    "\n",
    "# Define learning rate scheduler callback\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.25)\n",
    "\n",
    "# Calculate class weights based on occurrence since the base strategy loses more often we want to equalize the weights\n",
    "class_weights = compute_class_weight('balanced', np.unique(y), y)\n",
    "\n",
    "# Convert class weights to a dictionary\n",
    "class_weights_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "783/783 [==============================] - 2s 2ms/step - loss: 0.7186 - accuracy: 0.5205 - val_loss: 0.6914 - val_accuracy: 0.5149 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6883 - accuracy: 0.5498 - val_loss: 0.6864 - val_accuracy: 0.5431 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6817 - accuracy: 0.5758 - val_loss: 0.6884 - val_accuracy: 0.5532 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6787 - accuracy: 0.5831 - val_loss: 0.6828 - val_accuracy: 0.5762 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6737 - accuracy: 0.5940 - val_loss: 0.6745 - val_accuracy: 0.5949 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "783/783 [==============================] - 1s 2ms/step - loss: 0.6720 - accuracy: 0.6059 - val_loss: 0.6726 - val_accuracy: 0.5983 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "783/783 [==============================] - 1s 2ms/step - loss: 0.6683 - accuracy: 0.6106 - val_loss: 0.6720 - val_accuracy: 0.6064 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6697 - accuracy: 0.6119 - val_loss: 0.6645 - val_accuracy: 0.6222 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "783/783 [==============================] - 1s 2ms/step - loss: 0.6641 - accuracy: 0.6237 - val_loss: 0.6724 - val_accuracy: 0.5978 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "783/783 [==============================] - 1s 2ms/step - loss: 0.6636 - accuracy: 0.6225 - val_loss: 0.6696 - val_accuracy: 0.6107 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6614 - accuracy: 0.6307 - val_loss: 0.6644 - val_accuracy: 0.6093 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6593 - accuracy: 0.6315 - val_loss: 0.6645 - val_accuracy: 0.6198 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6603 - accuracy: 0.6310 - val_loss: 0.6618 - val_accuracy: 0.6251 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6580 - accuracy: 0.6417 - val_loss: 0.6667 - val_accuracy: 0.6227 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6566 - accuracy: 0.6373 - val_loss: 0.6635 - val_accuracy: 0.6318 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6563 - accuracy: 0.6415 - val_loss: 0.6630 - val_accuracy: 0.6275 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6566 - accuracy: 0.6396 - val_loss: 0.6603 - val_accuracy: 0.6270 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6562 - accuracy: 0.6397 - val_loss: 0.6588 - val_accuracy: 0.6232 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6556 - accuracy: 0.6398 - val_loss: 0.6614 - val_accuracy: 0.6357 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6540 - accuracy: 0.6467 - val_loss: 0.6627 - val_accuracy: 0.6261 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6551 - accuracy: 0.6380 - val_loss: 0.6653 - val_accuracy: 0.6256 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6543 - accuracy: 0.6435 - val_loss: 0.6682 - val_accuracy: 0.6184 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6549 - accuracy: 0.6413 - val_loss: 0.6667 - val_accuracy: 0.6280 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6472 - accuracy: 0.6475 - val_loss: 0.6582 - val_accuracy: 0.6352 - lr: 2.5000e-04\n",
      "Epoch 25/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6450 - accuracy: 0.6540 - val_loss: 0.6598 - val_accuracy: 0.6309 - lr: 2.5000e-04\n",
      "Epoch 26/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6454 - accuracy: 0.6536 - val_loss: 0.6566 - val_accuracy: 0.6352 - lr: 2.5000e-04\n",
      "Epoch 27/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6475 - accuracy: 0.6470 - val_loss: 0.6577 - val_accuracy: 0.6337 - lr: 2.5000e-04\n",
      "Epoch 28/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6448 - accuracy: 0.6521 - val_loss: 0.6568 - val_accuracy: 0.6361 - lr: 2.5000e-04\n",
      "Epoch 29/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6429 - accuracy: 0.6580 - val_loss: 0.6557 - val_accuracy: 0.6357 - lr: 2.5000e-04\n",
      "Epoch 30/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6404 - accuracy: 0.6596 - val_loss: 0.6596 - val_accuracy: 0.6400 - lr: 2.5000e-04\n",
      "Epoch 31/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6406 - accuracy: 0.6556 - val_loss: 0.6534 - val_accuracy: 0.6424 - lr: 2.5000e-04\n",
      "Epoch 32/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6446 - accuracy: 0.6517 - val_loss: 0.6546 - val_accuracy: 0.6290 - lr: 2.5000e-04\n",
      "Epoch 33/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6405 - accuracy: 0.6580 - val_loss: 0.6559 - val_accuracy: 0.6409 - lr: 2.5000e-04\n",
      "Epoch 34/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6381 - accuracy: 0.6605 - val_loss: 0.6529 - val_accuracy: 0.6352 - lr: 2.5000e-04\n",
      "Epoch 35/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6412 - accuracy: 0.6593 - val_loss: 0.6562 - val_accuracy: 0.6342 - lr: 2.5000e-04\n",
      "Epoch 36/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6378 - accuracy: 0.6605 - val_loss: 0.6547 - val_accuracy: 0.6419 - lr: 2.5000e-04\n",
      "Epoch 37/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6394 - accuracy: 0.6595 - val_loss: 0.6551 - val_accuracy: 0.6357 - lr: 2.5000e-04\n",
      "Epoch 38/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6404 - accuracy: 0.6535 - val_loss: 0.6557 - val_accuracy: 0.6371 - lr: 2.5000e-04\n",
      "Epoch 39/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6371 - accuracy: 0.6584 - val_loss: 0.6571 - val_accuracy: 0.6361 - lr: 2.5000e-04\n",
      "Epoch 40/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6395 - accuracy: 0.6581 - val_loss: 0.6619 - val_accuracy: 0.6366 - lr: 6.2500e-05\n",
      "Epoch 41/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6353 - accuracy: 0.6585 - val_loss: 0.6599 - val_accuracy: 0.6381 - lr: 6.2500e-05\n",
      "Epoch 42/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6374 - accuracy: 0.6615 - val_loss: 0.6577 - val_accuracy: 0.6453 - lr: 6.2500e-05\n",
      "Epoch 43/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6362 - accuracy: 0.6564 - val_loss: 0.6564 - val_accuracy: 0.6347 - lr: 6.2500e-05\n",
      "Epoch 44/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6362 - accuracy: 0.6595 - val_loss: 0.6574 - val_accuracy: 0.6414 - lr: 6.2500e-05\n",
      "Epoch 45/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6385 - accuracy: 0.6580 - val_loss: 0.6620 - val_accuracy: 0.6347 - lr: 1.5625e-05\n",
      "Epoch 46/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6358 - accuracy: 0.6657 - val_loss: 0.6573 - val_accuracy: 0.6424 - lr: 1.5625e-05\n",
      "Epoch 47/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6387 - accuracy: 0.6588 - val_loss: 0.6534 - val_accuracy: 0.6424 - lr: 1.5625e-05\n",
      "Epoch 48/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6359 - accuracy: 0.6608 - val_loss: 0.6548 - val_accuracy: 0.6381 - lr: 1.5625e-05\n",
      "Epoch 49/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6387 - accuracy: 0.6554 - val_loss: 0.6561 - val_accuracy: 0.6429 - lr: 1.5625e-05\n",
      "hello\n",
      "Epoch 1/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6436 - accuracy: 0.6526 - val_loss: 0.6218 - val_accuracy: 0.6975 - lr: 3.9063e-06\n",
      "Epoch 2/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6431 - accuracy: 0.6542 - val_loss: 0.6257 - val_accuracy: 0.6951 - lr: 3.9063e-06\n",
      "Epoch 3/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6440 - accuracy: 0.6524 - val_loss: 0.6230 - val_accuracy: 0.6980 - lr: 3.9063e-06\n",
      "Epoch 4/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6436 - accuracy: 0.6572 - val_loss: 0.6202 - val_accuracy: 0.6965 - lr: 3.9063e-06\n",
      "Epoch 5/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6414 - accuracy: 0.6584 - val_loss: 0.6189 - val_accuracy: 0.6985 - lr: 3.9063e-06\n",
      "Epoch 6/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6445 - accuracy: 0.6536 - val_loss: 0.6219 - val_accuracy: 0.6970 - lr: 3.9063e-06\n",
      "Epoch 7/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6422 - accuracy: 0.6529 - val_loss: 0.6272 - val_accuracy: 0.6956 - lr: 3.9063e-06\n",
      "Epoch 8/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6417 - accuracy: 0.6565 - val_loss: 0.6225 - val_accuracy: 0.6975 - lr: 3.9063e-06\n",
      "Epoch 9/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6392 - accuracy: 0.6526 - val_loss: 0.6242 - val_accuracy: 0.6975 - lr: 3.9063e-06\n",
      "Epoch 10/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6413 - accuracy: 0.6544 - val_loss: 0.6193 - val_accuracy: 0.6946 - lr: 3.9063e-06\n",
      "Epoch 11/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6422 - accuracy: 0.6556 - val_loss: 0.6167 - val_accuracy: 0.6999 - lr: 9.7656e-07\n",
      "Epoch 12/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6444 - accuracy: 0.6567 - val_loss: 0.6177 - val_accuracy: 0.6961 - lr: 9.7656e-07\n",
      "Epoch 13/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6417 - accuracy: 0.6548 - val_loss: 0.6206 - val_accuracy: 0.6965 - lr: 9.7656e-07\n",
      "Epoch 14/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6418 - accuracy: 0.6560 - val_loss: 0.6190 - val_accuracy: 0.6970 - lr: 9.7656e-07\n",
      "Epoch 15/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6443 - accuracy: 0.6519 - val_loss: 0.6236 - val_accuracy: 0.6956 - lr: 9.7656e-07\n",
      "Epoch 16/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6421 - accuracy: 0.6547 - val_loss: 0.6248 - val_accuracy: 0.6932 - lr: 9.7656e-07\n",
      "Epoch 17/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6431 - accuracy: 0.6555 - val_loss: 0.6278 - val_accuracy: 0.6927 - lr: 2.4414e-07\n",
      "Epoch 18/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6456 - accuracy: 0.6504 - val_loss: 0.6236 - val_accuracy: 0.6956 - lr: 2.4414e-07\n",
      "Epoch 19/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6423 - accuracy: 0.6580 - val_loss: 0.6252 - val_accuracy: 0.6956 - lr: 2.4414e-07\n",
      "Epoch 20/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6438 - accuracy: 0.6533 - val_loss: 0.6236 - val_accuracy: 0.6989 - lr: 2.4414e-07\n",
      "Epoch 21/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6395 - accuracy: 0.6560 - val_loss: 0.6267 - val_accuracy: 0.6932 - lr: 2.4414e-07\n",
      "Epoch 22/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6456 - accuracy: 0.6540 - val_loss: 0.6215 - val_accuracy: 0.7004 - lr: 6.1035e-08\n",
      "Epoch 23/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6452 - accuracy: 0.6556 - val_loss: 0.6267 - val_accuracy: 0.6879 - lr: 6.1035e-08\n",
      "Epoch 24/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6410 - accuracy: 0.6564 - val_loss: 0.6251 - val_accuracy: 0.6956 - lr: 6.1035e-08\n",
      "Epoch 25/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6384 - accuracy: 0.6607 - val_loss: 0.6164 - val_accuracy: 0.6975 - lr: 6.1035e-08\n",
      "Epoch 26/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6412 - accuracy: 0.6558 - val_loss: 0.6288 - val_accuracy: 0.6898 - lr: 6.1035e-08\n",
      "Epoch 27/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6396 - accuracy: 0.6557 - val_loss: 0.6226 - val_accuracy: 0.6961 - lr: 6.1035e-08\n",
      "Epoch 28/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6404 - accuracy: 0.6581 - val_loss: 0.6253 - val_accuracy: 0.6922 - lr: 6.1035e-08\n",
      "Epoch 29/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6455 - accuracy: 0.6524 - val_loss: 0.6230 - val_accuracy: 0.6980 - lr: 6.1035e-08\n",
      "Epoch 30/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6428 - accuracy: 0.6573 - val_loss: 0.6234 - val_accuracy: 0.6932 - lr: 6.1035e-08\n",
      "Epoch 31/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6435 - accuracy: 0.6572 - val_loss: 0.6239 - val_accuracy: 0.6970 - lr: 1.5259e-08\n",
      "Epoch 32/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6402 - accuracy: 0.6577 - val_loss: 0.6210 - val_accuracy: 0.6956 - lr: 1.5259e-08\n",
      "Epoch 33/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6436 - accuracy: 0.6524 - val_loss: 0.6282 - val_accuracy: 0.6894 - lr: 1.5259e-08\n",
      "Epoch 34/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6407 - accuracy: 0.6526 - val_loss: 0.6237 - val_accuracy: 0.6970 - lr: 1.5259e-08\n",
      "Epoch 35/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6444 - accuracy: 0.6537 - val_loss: 0.6212 - val_accuracy: 0.6980 - lr: 1.5259e-08\n",
      "Epoch 36/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6461 - accuracy: 0.6497 - val_loss: 0.6258 - val_accuracy: 0.6937 - lr: 3.8147e-09\n",
      "Epoch 37/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6449 - accuracy: 0.6543 - val_loss: 0.6188 - val_accuracy: 0.6975 - lr: 3.8147e-09\n",
      "Epoch 38/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6429 - accuracy: 0.6586 - val_loss: 0.6241 - val_accuracy: 0.6985 - lr: 3.8147e-09\n",
      "Epoch 39/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6445 - accuracy: 0.6514 - val_loss: 0.6239 - val_accuracy: 0.6932 - lr: 3.8147e-09\n",
      "Epoch 40/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6441 - accuracy: 0.6531 - val_loss: 0.6274 - val_accuracy: 0.6879 - lr: 3.8147e-09\n",
      "hello\n",
      "Epoch 1/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6392 - accuracy: 0.6599 - val_loss: 0.6331 - val_accuracy: 0.6826 - lr: 9.5367e-10\n",
      "Epoch 2/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6449 - accuracy: 0.6504 - val_loss: 0.6291 - val_accuracy: 0.6846 - lr: 9.5367e-10\n",
      "Epoch 3/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6431 - accuracy: 0.6568 - val_loss: 0.6338 - val_accuracy: 0.6779 - lr: 9.5367e-10\n",
      "Epoch 4/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6406 - accuracy: 0.6579 - val_loss: 0.6350 - val_accuracy: 0.6807 - lr: 9.5367e-10\n",
      "Epoch 5/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6388 - accuracy: 0.6602 - val_loss: 0.6295 - val_accuracy: 0.6802 - lr: 9.5367e-10\n",
      "Epoch 6/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6400 - accuracy: 0.6597 - val_loss: 0.6336 - val_accuracy: 0.6802 - lr: 9.5367e-10\n",
      "Epoch 7/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6407 - accuracy: 0.6555 - val_loss: 0.6298 - val_accuracy: 0.6841 - lr: 9.5367e-10\n",
      "Epoch 8/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6388 - accuracy: 0.6583 - val_loss: 0.6306 - val_accuracy: 0.6759 - lr: 2.3842e-10\n",
      "Epoch 9/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6403 - accuracy: 0.6557 - val_loss: 0.6302 - val_accuracy: 0.6793 - lr: 2.3842e-10\n",
      "Epoch 10/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6410 - accuracy: 0.6618 - val_loss: 0.6288 - val_accuracy: 0.6860 - lr: 2.3842e-10\n",
      "Epoch 11/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6408 - accuracy: 0.6605 - val_loss: 0.6358 - val_accuracy: 0.6779 - lr: 2.3842e-10\n",
      "Epoch 12/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6398 - accuracy: 0.6608 - val_loss: 0.6309 - val_accuracy: 0.6860 - lr: 2.3842e-10\n",
      "Epoch 13/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6427 - accuracy: 0.6529 - val_loss: 0.6335 - val_accuracy: 0.6841 - lr: 2.3842e-10\n",
      "Epoch 14/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6394 - accuracy: 0.6588 - val_loss: 0.6305 - val_accuracy: 0.6802 - lr: 2.3842e-10\n",
      "Epoch 15/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6408 - accuracy: 0.6591 - val_loss: 0.6312 - val_accuracy: 0.6831 - lr: 2.3842e-10\n",
      "Epoch 16/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6400 - accuracy: 0.6551 - val_loss: 0.6329 - val_accuracy: 0.6841 - lr: 5.9605e-11\n",
      "Epoch 17/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6403 - accuracy: 0.6594 - val_loss: 0.6318 - val_accuracy: 0.6822 - lr: 5.9605e-11\n",
      "Epoch 18/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6379 - accuracy: 0.6640 - val_loss: 0.6291 - val_accuracy: 0.6802 - lr: 5.9605e-11\n",
      "Epoch 19/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6408 - accuracy: 0.6603 - val_loss: 0.6294 - val_accuracy: 0.6798 - lr: 5.9605e-11\n",
      "Epoch 20/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6366 - accuracy: 0.6652 - val_loss: 0.6303 - val_accuracy: 0.6807 - lr: 5.9605e-11\n",
      "Epoch 21/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6405 - accuracy: 0.6574 - val_loss: 0.6302 - val_accuracy: 0.6726 - lr: 1.4901e-11\n",
      "Epoch 22/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6421 - accuracy: 0.6544 - val_loss: 0.6334 - val_accuracy: 0.6846 - lr: 1.4901e-11\n",
      "Epoch 23/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6367 - accuracy: 0.6638 - val_loss: 0.6301 - val_accuracy: 0.6798 - lr: 1.4901e-11\n",
      "Epoch 24/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6394 - accuracy: 0.6588 - val_loss: 0.6325 - val_accuracy: 0.6850 - lr: 1.4901e-11\n",
      "Epoch 25/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6433 - accuracy: 0.6561 - val_loss: 0.6302 - val_accuracy: 0.6826 - lr: 1.4901e-11\n",
      "hello\n",
      "Epoch 1/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6380 - accuracy: 0.6604 - val_loss: 0.6452 - val_accuracy: 0.6764 - lr: 3.7253e-12\n",
      "Epoch 2/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6392 - accuracy: 0.6571 - val_loss: 0.6422 - val_accuracy: 0.6735 - lr: 3.7253e-12\n",
      "Epoch 3/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6398 - accuracy: 0.6560 - val_loss: 0.6417 - val_accuracy: 0.6774 - lr: 3.7253e-12\n",
      "Epoch 4/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6380 - accuracy: 0.6609 - val_loss: 0.6403 - val_accuracy: 0.6721 - lr: 3.7253e-12\n",
      "Epoch 5/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6404 - accuracy: 0.6587 - val_loss: 0.6441 - val_accuracy: 0.6740 - lr: 3.7253e-12\n",
      "Epoch 6/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6421 - accuracy: 0.6540 - val_loss: 0.6404 - val_accuracy: 0.6788 - lr: 3.7253e-12\n",
      "Epoch 7/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6417 - accuracy: 0.6532 - val_loss: 0.6429 - val_accuracy: 0.6831 - lr: 3.7253e-12\n",
      "Epoch 8/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6356 - accuracy: 0.6615 - val_loss: 0.6416 - val_accuracy: 0.6711 - lr: 3.7253e-12\n",
      "Epoch 9/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6428 - accuracy: 0.6568 - val_loss: 0.6469 - val_accuracy: 0.6740 - lr: 3.7253e-12\n",
      "Epoch 10/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6398 - accuracy: 0.6536 - val_loss: 0.6463 - val_accuracy: 0.6740 - lr: 9.3132e-13\n",
      "Epoch 11/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6398 - accuracy: 0.6601 - val_loss: 0.6471 - val_accuracy: 0.6750 - lr: 9.3132e-13\n",
      "Epoch 12/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6419 - accuracy: 0.6547 - val_loss: 0.6437 - val_accuracy: 0.6802 - lr: 9.3132e-13\n",
      "Epoch 13/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6450 - accuracy: 0.6501 - val_loss: 0.6475 - val_accuracy: 0.6726 - lr: 9.3132e-13\n",
      "Epoch 14/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6397 - accuracy: 0.6591 - val_loss: 0.6416 - val_accuracy: 0.6711 - lr: 9.3132e-13\n",
      "Epoch 15/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6407 - accuracy: 0.6580 - val_loss: 0.6422 - val_accuracy: 0.6726 - lr: 2.3283e-13\n",
      "Epoch 16/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6413 - accuracy: 0.6532 - val_loss: 0.6428 - val_accuracy: 0.6735 - lr: 2.3283e-13\n",
      "Epoch 17/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6406 - accuracy: 0.6556 - val_loss: 0.6475 - val_accuracy: 0.6721 - lr: 2.3283e-13\n",
      "Epoch 18/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6406 - accuracy: 0.6569 - val_loss: 0.6463 - val_accuracy: 0.6711 - lr: 2.3283e-13\n",
      "Epoch 19/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6412 - accuracy: 0.6556 - val_loss: 0.6470 - val_accuracy: 0.6731 - lr: 2.3283e-13\n",
      "hello\n",
      "Epoch 1/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6437 - accuracy: 0.6535 - val_loss: 0.6178 - val_accuracy: 0.6989 - lr: 5.8208e-14\n",
      "Epoch 2/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6412 - accuracy: 0.6574 - val_loss: 0.6143 - val_accuracy: 0.7033 - lr: 5.8208e-14\n",
      "Epoch 3/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6428 - accuracy: 0.6537 - val_loss: 0.6187 - val_accuracy: 0.7047 - lr: 5.8208e-14\n",
      "Epoch 4/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6427 - accuracy: 0.6546 - val_loss: 0.6194 - val_accuracy: 0.6999 - lr: 5.8208e-14\n",
      "Epoch 5/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6408 - accuracy: 0.6543 - val_loss: 0.6220 - val_accuracy: 0.6999 - lr: 5.8208e-14\n",
      "Epoch 6/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6436 - accuracy: 0.6530 - val_loss: 0.6244 - val_accuracy: 0.6994 - lr: 5.8208e-14\n",
      "Epoch 7/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6439 - accuracy: 0.6523 - val_loss: 0.6172 - val_accuracy: 0.7009 - lr: 5.8208e-14\n",
      "Epoch 8/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6411 - accuracy: 0.6595 - val_loss: 0.6218 - val_accuracy: 0.7028 - lr: 1.4552e-14\n",
      "Epoch 9/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6431 - accuracy: 0.6553 - val_loss: 0.6192 - val_accuracy: 0.7013 - lr: 1.4552e-14\n",
      "Epoch 10/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6439 - accuracy: 0.6504 - val_loss: 0.6197 - val_accuracy: 0.7013 - lr: 1.4552e-14\n",
      "Epoch 11/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6449 - accuracy: 0.6505 - val_loss: 0.6164 - val_accuracy: 0.7018 - lr: 1.4552e-14\n",
      "Epoch 12/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6419 - accuracy: 0.6553 - val_loss: 0.6202 - val_accuracy: 0.7047 - lr: 1.4552e-14\n",
      "Epoch 13/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6445 - accuracy: 0.6556 - val_loss: 0.6193 - val_accuracy: 0.7047 - lr: 3.6380e-15\n",
      "Epoch 14/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6422 - accuracy: 0.6583 - val_loss: 0.6183 - val_accuracy: 0.6994 - lr: 3.6380e-15\n",
      "Epoch 15/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6438 - accuracy: 0.6504 - val_loss: 0.6223 - val_accuracy: 0.7047 - lr: 3.6380e-15\n",
      "Epoch 16/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6406 - accuracy: 0.6597 - val_loss: 0.6196 - val_accuracy: 0.6994 - lr: 3.6380e-15\n",
      "Epoch 17/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6430 - accuracy: 0.6586 - val_loss: 0.6165 - val_accuracy: 0.7023 - lr: 3.6380e-15\n",
      "hello\n",
      "Epoch 1/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6448 - accuracy: 0.6584 - val_loss: 0.6196 - val_accuracy: 0.6946 - lr: 9.0949e-16\n",
      "Epoch 2/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6470 - accuracy: 0.6492 - val_loss: 0.6215 - val_accuracy: 0.6922 - lr: 9.0949e-16\n",
      "Epoch 3/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6438 - accuracy: 0.6560 - val_loss: 0.6293 - val_accuracy: 0.6865 - lr: 9.0949e-16\n",
      "Epoch 4/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6440 - accuracy: 0.6551 - val_loss: 0.6278 - val_accuracy: 0.6889 - lr: 9.0949e-16\n",
      "Epoch 5/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6397 - accuracy: 0.6602 - val_loss: 0.6218 - val_accuracy: 0.6908 - lr: 9.0949e-16\n",
      "Epoch 6/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6422 - accuracy: 0.6569 - val_loss: 0.6241 - val_accuracy: 0.6870 - lr: 9.0949e-16\n",
      "Epoch 7/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6442 - accuracy: 0.6506 - val_loss: 0.6227 - val_accuracy: 0.6879 - lr: 2.2737e-16\n",
      "Epoch 8/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6447 - accuracy: 0.6512 - val_loss: 0.6239 - val_accuracy: 0.6889 - lr: 2.2737e-16\n",
      "Epoch 9/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6457 - accuracy: 0.6528 - val_loss: 0.6294 - val_accuracy: 0.6850 - lr: 2.2737e-16\n",
      "Epoch 10/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6422 - accuracy: 0.6562 - val_loss: 0.6266 - val_accuracy: 0.6865 - lr: 2.2737e-16\n",
      "Epoch 11/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6425 - accuracy: 0.6562 - val_loss: 0.6241 - val_accuracy: 0.6913 - lr: 2.2737e-16\n",
      "Epoch 12/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6451 - accuracy: 0.6546 - val_loss: 0.6248 - val_accuracy: 0.6918 - lr: 5.6843e-17\n",
      "Epoch 13/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6426 - accuracy: 0.6592 - val_loss: 0.6258 - val_accuracy: 0.6894 - lr: 5.6843e-17\n",
      "Epoch 14/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6436 - accuracy: 0.6576 - val_loss: 0.6249 - val_accuracy: 0.6874 - lr: 5.6843e-17\n",
      "Epoch 15/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6441 - accuracy: 0.6540 - val_loss: 0.6266 - val_accuracy: 0.6898 - lr: 5.6843e-17\n",
      "Epoch 16/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6463 - accuracy: 0.6512 - val_loss: 0.6270 - val_accuracy: 0.6846 - lr: 5.6843e-17\n",
      "hello\n",
      "Epoch 1/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6396 - accuracy: 0.6582 - val_loss: 0.6260 - val_accuracy: 0.6807 - lr: 1.4211e-17\n",
      "Epoch 2/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6411 - accuracy: 0.6538 - val_loss: 0.6288 - val_accuracy: 0.6817 - lr: 1.4211e-17\n",
      "Epoch 3/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6426 - accuracy: 0.6544 - val_loss: 0.6239 - val_accuracy: 0.6874 - lr: 1.4211e-17\n",
      "Epoch 4/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6394 - accuracy: 0.6560 - val_loss: 0.6312 - val_accuracy: 0.6779 - lr: 1.4211e-17\n",
      "Epoch 5/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6407 - accuracy: 0.6620 - val_loss: 0.6234 - val_accuracy: 0.6918 - lr: 1.4211e-17\n",
      "Epoch 6/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6408 - accuracy: 0.6575 - val_loss: 0.6305 - val_accuracy: 0.6850 - lr: 1.4211e-17\n",
      "Epoch 7/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6451 - accuracy: 0.6492 - val_loss: 0.6268 - val_accuracy: 0.6826 - lr: 1.4211e-17\n",
      "Epoch 8/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6401 - accuracy: 0.6570 - val_loss: 0.6227 - val_accuracy: 0.6860 - lr: 1.4211e-17\n",
      "Epoch 9/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6417 - accuracy: 0.6547 - val_loss: 0.6276 - val_accuracy: 0.6812 - lr: 1.4211e-17\n",
      "Epoch 10/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6407 - accuracy: 0.6540 - val_loss: 0.6315 - val_accuracy: 0.6783 - lr: 1.4211e-17\n",
      "Epoch 11/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6419 - accuracy: 0.6545 - val_loss: 0.6280 - val_accuracy: 0.6884 - lr: 1.4211e-17\n",
      "Epoch 12/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6414 - accuracy: 0.6556 - val_loss: 0.6254 - val_accuracy: 0.6826 - lr: 1.4211e-17\n",
      "Epoch 13/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6424 - accuracy: 0.6555 - val_loss: 0.6248 - val_accuracy: 0.6817 - lr: 1.4211e-17\n",
      "Epoch 14/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6418 - accuracy: 0.6579 - val_loss: 0.6302 - val_accuracy: 0.6755 - lr: 3.5527e-18\n",
      "Epoch 15/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6427 - accuracy: 0.6517 - val_loss: 0.6320 - val_accuracy: 0.6812 - lr: 3.5527e-18\n",
      "Epoch 16/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6392 - accuracy: 0.6607 - val_loss: 0.6250 - val_accuracy: 0.6865 - lr: 3.5527e-18\n",
      "Epoch 17/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6387 - accuracy: 0.6603 - val_loss: 0.6219 - val_accuracy: 0.6850 - lr: 3.5527e-18\n",
      "Epoch 18/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6392 - accuracy: 0.6584 - val_loss: 0.6288 - val_accuracy: 0.6822 - lr: 3.5527e-18\n",
      "Epoch 19/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6429 - accuracy: 0.6518 - val_loss: 0.6270 - val_accuracy: 0.6822 - lr: 3.5527e-18\n",
      "Epoch 20/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6453 - accuracy: 0.6520 - val_loss: 0.6287 - val_accuracy: 0.6769 - lr: 3.5527e-18\n",
      "Epoch 21/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6429 - accuracy: 0.6559 - val_loss: 0.6285 - val_accuracy: 0.6783 - lr: 3.5527e-18\n",
      "Epoch 22/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6391 - accuracy: 0.6612 - val_loss: 0.6250 - val_accuracy: 0.6879 - lr: 3.5527e-18\n",
      "Epoch 23/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6393 - accuracy: 0.6662 - val_loss: 0.6314 - val_accuracy: 0.6913 - lr: 8.8818e-19\n",
      "Epoch 24/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6428 - accuracy: 0.6534 - val_loss: 0.6252 - val_accuracy: 0.6802 - lr: 8.8818e-19\n",
      "Epoch 25/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6407 - accuracy: 0.6575 - val_loss: 0.6273 - val_accuracy: 0.6841 - lr: 8.8818e-19\n",
      "Epoch 26/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6412 - accuracy: 0.6561 - val_loss: 0.6238 - val_accuracy: 0.6889 - lr: 8.8818e-19\n",
      "Epoch 27/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6410 - accuracy: 0.6526 - val_loss: 0.6286 - val_accuracy: 0.6802 - lr: 8.8818e-19\n",
      "Epoch 28/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6412 - accuracy: 0.6587 - val_loss: 0.6296 - val_accuracy: 0.6889 - lr: 2.2204e-19\n",
      "Epoch 29/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6405 - accuracy: 0.6600 - val_loss: 0.6304 - val_accuracy: 0.6798 - lr: 2.2204e-19\n",
      "Epoch 30/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6395 - accuracy: 0.6540 - val_loss: 0.6229 - val_accuracy: 0.6874 - lr: 2.2204e-19\n",
      "Epoch 31/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6415 - accuracy: 0.6584 - val_loss: 0.6303 - val_accuracy: 0.6798 - lr: 2.2204e-19\n",
      "Epoch 32/1000\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.6411 - accuracy: 0.6534 - val_loss: 0.6242 - val_accuracy: 0.6802 - lr: 2.2204e-19\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# Train the model using k-fold cross-validation\n",
    "for train_index, val_index in kfold.split(X):\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    # Scale the data\n",
    "    X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n",
    "    X_val_fold_scaled = scaler.transform(X_val_fold)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_fold_scaled, \n",
    "              y_train_fold, \n",
    "              epochs=1000, \n",
    "              batch_size=16, \n",
    "              validation_data=(X_val_fold_scaled, y_val_fold), \n",
    "              callbacks=[lr_scheduler, early_stopping],  \n",
    "              class_weight=class_weights_dict \n",
    "            )\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scale the validation data with the same scaler used for the training data\n",
    "validation_x = scaler.fit_transform(validation_data.drop(columns=['result', 'label'], axis=1).values)\n",
    "\n",
    "# Run predictions on the validation dataset\n",
    "y_pred = model.predict(validation_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6546371750097012 - precision: 0.5260115606936416 - recall: 0.48663101604278075 - f1 0.5055555555555555\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(validation_data['label'], np.where(y_pred > 0.5, 1, 0).flatten()).ravel()\n",
    "\n",
    "accuracy = (tp+tn) / (tp+tn+fn+fp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2 * (precision*recall)/(precision+recall)\n",
    "\n",
    "print(f'accuracy: {accuracy} - precision: {precision} - recall: {recall} - f1 {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(df, name):\n",
    "    winners = df[df['result'] >= 0]\n",
    "    losers = df[df['result'] < 0]\n",
    "    \n",
    "    return pd.DataFrame.from_dict({\n",
    "        'count': len(df),\n",
    "        'avg': df['result'].mean(),\n",
    "        'winrate': len(winners) / len(df),\n",
    "        'avg_winner': winners['result'].mean(),\n",
    "        'avg_loser': losers['result'].mean(),\n",
    "        'gain_loss_ratio': winners['result'].mean() /-losers['result'].mean(),\n",
    "        'total_win': winners['result'].sum(),\n",
    "        'total_loss':  losers['result'].sum(),\n",
    "        'profit_factor': winners['result'].sum() / -losers['result'].sum()\n",
    "    }, orient='index', columns=[name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>validation dataset</th>\n",
       "      <th>predicted winners</th>\n",
       "      <th>predicted losers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17179.000000</td>\n",
       "      <td>2577.000000</td>\n",
       "      <td>865.000000</td>\n",
       "      <td>1712.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>0.034288</td>\n",
       "      <td>0.025317</td>\n",
       "      <td>0.113907</td>\n",
       "      <td>-0.019444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winrate</th>\n",
       "      <td>0.374061</td>\n",
       "      <td>0.380287</td>\n",
       "      <td>0.535260</td>\n",
       "      <td>0.301986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_winner</th>\n",
       "      <td>0.310639</td>\n",
       "      <td>0.281702</td>\n",
       "      <td>0.314085</td>\n",
       "      <td>0.252701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_loser</th>\n",
       "      <td>-0.130860</td>\n",
       "      <td>-0.132013</td>\n",
       "      <td>-0.116645</td>\n",
       "      <td>-0.137183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gain_loss_ratio</th>\n",
       "      <td>2.373828</td>\n",
       "      <td>2.133890</td>\n",
       "      <td>2.692649</td>\n",
       "      <td>1.842072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_win</th>\n",
       "      <td>1996.165063</td>\n",
       "      <td>276.067884</td>\n",
       "      <td>145.421378</td>\n",
       "      <td>130.646506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_loss</th>\n",
       "      <td>-1407.136156</td>\n",
       "      <td>-210.825290</td>\n",
       "      <td>-46.891439</td>\n",
       "      <td>-163.933851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>profit_factor</th>\n",
       "      <td>1.418601</td>\n",
       "      <td>1.309463</td>\n",
       "      <td>3.101235</td>\n",
       "      <td>0.796946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          all  validation dataset  predicted winners  predicted losers\n",
       "count            17179.000000         2577.000000         865.000000       1712.000000\n",
       "avg                  0.034288            0.025317           0.113907         -0.019444\n",
       "winrate              0.374061            0.380287           0.535260          0.301986\n",
       "avg_winner           0.310639            0.281702           0.314085          0.252701\n",
       "avg_loser           -0.130860           -0.132013          -0.116645         -0.137183\n",
       "gain_loss_ratio      2.373828            2.133890           2.692649          1.842072\n",
       "total_win         1996.165063          276.067884         145.421378        130.646506\n",
       "total_loss       -1407.136156         -210.825290         -46.891439       -163.933851\n",
       "profit_factor        1.418601            1.309463           3.101235          0.796946"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = validation_data[['result', 'label']].copy()\n",
    "res['pred'] = y_pred\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "all_trades = summarize(df, \"all\")\n",
    "val_trades = summarize(res, \"validation dataset\")\n",
    "pred_winners = summarize(res[res['pred'] > threshold], \"predicted winners\")\n",
    "pred_losers = summarize(res[res['pred'] <= threshold], \"predicted losers\")\n",
    "\n",
    "\n",
    "pd.concat([all_trades, val_trades, pred_winners, pred_losers,], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3456 - Benchmark innan Kaufman + trade count features\n",
    "```csv\n",
    "stat,all,predicted winners,predicted losers\n",
    "count,2621.000000,1036.000000,1585.000000\n",
    "avg,0.029182,0.112754,-0.025443\n",
    "winrate,0.375048,0.519305,0.280757\n",
    "avg_winner,0.296503,0.317832,0.270716\n",
    "avg_loser,-0.131243,-0.108796,-0.141048\n",
    "total_win,291.461980,170.993371,120.468609\n",
    "total_loss,-214.975741,-54.180457,-160.795284\n",
    "profit_factor,1.355790,3.155997,0.749205\n",
    "```\n",
    "#### 3456 - Benchmark innan Kaufman + efter trade count features\n",
    "```csv\n",
    "\n",
    "stat,all,predicted winners,predicted losers\n",
    "count,2569.000000,970.000000,1599.000000\n",
    "avg,0.033677,0.120749,-0.019144\n",
    "winrate,0.384196,0.512371,0.306442\n",
    "avg_winner,0.295127,0.342731,0.246843\n",
    "avg_loser,-0.129441,-0.112497,-0.136667\n",
    "total_win,291.290120,170.337085,120.953035\n",
    "total_loss,-204.774914,-53.210891,-151.564022\n",
    "profit_factor,1.422489,3.201170,0.798033\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter( res['pred'], res['result'] +1)\n",
    "ax.set_yscale(\"log\");\n",
    "# ax.set_xscale(\"log\");\n",
    "# All above 1 is winners\n",
    "plt.axhline(y=1, color='r', linestyle='-')\n",
    "# All above threshold is predicted to be winners\n",
    "plt.axvline(x=threshold, color='r', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0: 8', '10: 12', '20: 40', '30: 596', '40: 1056', '50: 421', '60: 258', '70: 156', '80: 28', '90: 2']\n",
      "[0.5288033318345798, 0.7258705744526884, 0.38318057191016747, 0.49465829778017584, 1.018474290212893, 2.183265270099551, 3.2536247900313504, 7.362433187599715, 3.2998259058656343, 0]\n"
     ]
    }
   ],
   "source": [
    "def divide(a, b):\n",
    "    return b and a / b or 0\n",
    "\n",
    "def getStats(inrange):\n",
    "    winners = inrange[inrange['result'] >= 0]\n",
    "    losers = inrange[inrange['result'] <= 0]\n",
    "        \n",
    "    d = {\n",
    "        'winrate': divide(len(winners),len(inrange)),\n",
    "        'profit_factor': divide(winners['result'].sum(), -losers['result'].sum())\n",
    "    }\n",
    "    return d\n",
    "\n",
    "percentiles = []\n",
    "percentiles_print = []\n",
    "winrates_above = []\n",
    "profitfactors_above = []\n",
    "winrates_below = []\n",
    "profitfactors_below = []\n",
    "\n",
    "for i in range(10):\n",
    "    in_range =res[res['pred'].between(i/10, (i+1)/10)]\n",
    "    percentiles.append(i*10)\n",
    "    percentiles_print.append(f'{i*10}: {len(in_range)}')\n",
    "\n",
    "    # Probability Above i/10\n",
    "    above = getStats(in_range)\n",
    "    winrates_above.append(above['winrate'])\n",
    "    profitfactors_above.append(above['profit_factor'])\n",
    "\n",
    "    # Probability below i/10\n",
    "    below = getStats(res[res['pred'] < i/10])\n",
    "    winrates_below.append(below['winrate'])\n",
    "    profitfactors_below.append(below['profit_factor'])\n",
    "\n",
    "print(percentiles_print)\n",
    "print(profitfactors_above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.bar( percentiles, winrates_above)\n",
    "plt.axhline(y=0.5, color='r', linestyle=':')\n",
    "plt.xlabel('Predicted Probability Above')\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.bar( percentiles, winrates_below)\n",
    "plt.axhline(y=0.5, color='r', linestyle=':')\n",
    "plt.xlabel('Predicted Probability Below')\n",
    "plt.ylabel('Winrate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar( percentiles, profitfactors_above)\n",
    "plt.axhline(y=1, color='r', linestyle=':')\n",
    "plt.xlabel('Predicted Probability above')\n",
    "plt.ylabel('Profit factor')\n",
    "\n",
    "fig, ax2 = plt.subplots()\n",
    "ax2.bar( percentiles, profitfactors_below)\n",
    "plt.axhline(y=1, color='r', linestyle=':')\n",
    "plt.xlabel('Predicted Probability below')\n",
    "plt.ylabel('Profit factor')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the scaler to a file using joblib.dump\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "#scaler = joblib.load(\"scaler.joblib\")\n",
    "\n",
    "# Save the model to a file using model.save\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Dict\n",
    "\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "\n",
    "def convert_price_data_to_dataframe(json_data):\n",
    "    # parse the JSON data into a list of dictionaries\n",
    "    data = json.loads(json_data)\n",
    "    \n",
    "    # create a pandas DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # convert the \"Date\" column to datetime format\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# make an API call to retrieve the price data\n",
    "# url = \"http://example.com/api/price_data\"\n",
    "# response = requests.get(url)\n",
    "\n",
    "# # pass the response content to the function\n",
    "# df = convert_price_data_to_dataframe(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rows are equal.\n"
     ]
    }
   ],
   "source": [
    "# How to compare two rows, to be used to know that the api data is treated the same way as from db\n",
    "# create two example dataframes\n",
    "df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "df2 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "\n",
    "# compare rows with index 1 in both dataframes\n",
    "row1_df1 = df1.iloc[1]\n",
    "row1_df2 = df2.iloc[1]\n",
    "\n",
    "if row1_df1.equals(row1_df2):\n",
    "    print(\"The rows are equal.\")\n",
    "else:\n",
    "    print(\"The rows are not equal.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6688f5bd7a5c2379fdfde91b010ab5a3ba8033d4f740240b607cd397292295b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
